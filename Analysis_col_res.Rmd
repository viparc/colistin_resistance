---
title: "Relative contributions of antimicrobial use and external contaminations on colistin resistance in Mekong delta chicken farms"
csl: the-american-naturalist.csl
# runtime: shiny
output:
  html_document:
    theme: cerulean
    toc: yes
    toc_float: true
  pdf_document:
    toc: yes
<!-- bibliography: references.bib -->
---

<!--
IMAGES:
Insert them with: ![alt text](image.png)
You can also resize them if needed: convert image.png -resize 50% image.png
If you want to center the image, go through HTML code:
<div style="text-align:center"><img src ="image.png"/></div>

REFERENCES:
For references: Put all the bibTeX references in the file "references.bib"
in the current folder and cite the references as @key or [@key] in the text.
Uncomment the bibliography field in the above header and put a "References"
title wherever you want to display the reference list.
-->

<style type="text/css">
.main-container {
  max-width: 1370px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r general_options, include = FALSE}
knitr::knit_hooks$set(
  margin = function(before, options, envir) {
    if (before) par(mgp = c(1.5, .5, 0), bty = "n", plt = c(.105, .97, .13, .97))
    else NULL
  },
  prompt = function(before, options, envir) {
    options(prompt = if (options$engine %in% c("sh", "bash")) "$ " else "> ")
  })

knitr::opts_chunk$set(margin = TRUE, prompt = TRUE, comment = "",
                      collapse = TRUE, cache = FALSE, autodep = TRUE,
                      dev.args = list(pointsize = 11), fig.height = 3.5,
                      fig.width = 4.24725, fig.retina = 2, fig.align = "center")

options(width = 137)
```

The objective is to understand the relative contributions of different factors to the colistin resistance detected in samples from ViParc chicken farms:

* antimicrobial use (AMU)
* the carriage of colistin resistance in day-old chicks
* introductions of resistant strains from outside the flock (other flocks, environment, human-poultry transmission)

Available data is:

* Data on ViParc farms, including AMU and sampling dates: [ViParc data overview](https://rpubs.com/choisy/viparc_data).
* During each production cycle, 3 (or less) pooled chicken faeces samples are collected: at the beginning, in the middle and at the end of the cycle. Around 30 E.coli colonies are collected from each sample and pooled. The Optical Density of each of these pooled samples is measured twice to obtain their growth curve in presence of different colistin concentrations. For each sample, the MIC can be determined from these growth curves.

We first clean work environment and load needed packages.

```{r}
rm(list=ls(all=TRUE))

library(deSolve)
library(bbmle)
library(ggplot2)
library(grid)
library(reshape2)
library(readxl)
library(pracma)
library(agrmt)
library(shiny)
library(parallel)
library(plotly)

```

# Data

## Load data

We load data:

* `viparc_data_quali` and `viparc_data_quanti`: Includes production cycles, AMU (resp. qualitative and quantitative), sampling dates in ViParc farms.
* `gcur`: Optical density (growth curves) measured in the samples at different times, at different colistin concentrations
* `mic_data`: MIC for each sample

```{r}
# viparc_data_quali = read.csv("https://raw.githubusercontent.com/viparc/colistin_resistance/master/data/viparc_qualitative.csv")
# viparc_data_quanti = read.csv("https://raw.githubusercontent.com/viparc/colistin_resistance/master/data/viparc_quantitative.csv")

viparc_data_quali = read.csv("C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/viparc_qualitative.csv")
viparc_data_quanti = read.csv("C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/viparc_quantitative.csv")

mic_data = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/ColR_MIC.xlsx", sheet="MIC data"))
mic_data$MIC = as.numeric(mic_data$MIC)
mic_data = dcast(mic_data, ID + FarmID + Flockseq ~ SamplingPoint, value.var="MIC")
mic_data = mic_data[, c("ID", "FarmID", "Flockseq", "Start", "Mid", "End")]

gcur = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/ColR_MIC.xlsx", sheet="OD"))

id_matrix = mic_data[,c("ID", "FarmID", "Flockseq")]
```

## Choose type of resistance data: growth curves or MIC

Here, we decide which resistance data we analyze: "growth_curves" or "mic". The final object for resistance data will be `res_data`, with values ranging between 0 (lowest observed resistance) and 1 (highest observed resistance).

```{r}
type_res_data = "growth_curves"
```

```{r}
if(type_res_data == "mic"){
  res_data = mic_data[,c("Start", "Mid", "End")]
  rownames(res_data) = mic_data$ID
  colnames(res_data) = c("S", "M", "E")
  res_data = (res_data - min(res_data, na.rm=T))/(max(res_data, na.rm=T) - min(res_data, na.rm=T))
}

```

## Resistance data: determine which Optical Density dataset to use

This part is computed only in the case we chose growth curves as a measure of resistance in samples. We use `gcur`. Let us first plot the growth curves for each concentration. We remove concentrations 3 and 6, as too few OD were measured for these concentrations.

```{r, fig.width=6, fig.height=4}
if(type_res_data == "growth_curves"){
  
  gcur = melt(gcur, id.vars = c("SampleID", "Conc.", "FlockID", "SamplPoint", "Repeat"), variable.name = "Time")
  gcur$Time = as.numeric(gcur$Time)
  
  # Unique ID for each observed curve:
  gcur$obsID = paste0(gcur$SampleID, gcur$Repeat)
  
  # Remove concentrations 3 and 6:
  gcur = gcur[which(!gcur$Conc. %in% c(3, 6)),]
  
  p = ggplot(data = gcur, aes(x=Time, group=obsID))
  p = p + ggtitle("Optical Density in all samples for each concentration")
  p = p + xlab("Time (hours)") + ylab("OD (optical density)")
  p = p + geom_line(aes(y=value), col="grey")
  p = p + facet_wrap(~Conc.)
  plot(p)
  
  rm(p)
}
```

Our objective is to use a concentration that offers a distribution of areas under curve (AUC) as spread out as possible. We see that, in concentrations 0 and 0.5, no curve is totally flat, i.e. all samples show some growth. On the contrary, in concentrations 8, 12 and 16, most of the samples show no growth. Therefore, we need to choose between concentrations 1, 2 and 4, that all present a spread range of curves (and thus AUC).

For these concentrations (1, 2 and 4), we compute the area under curve (AUC), for each sample, between t=15 hours and t=50 hours. This corresponds to the moment curves start to grow.

To measure how much the distributions of AUC are spread out for each concentration, we use the ["van der Eijk's A" statistic](https://en.wikipedia.org/wiki/Multimodal_distribution#van_der_Eijk's_A).  It ranges between -1 (perfect bimodality of the distribution) to +1 (perfect unimodality). A value of 0 corresponds to a uniform distribution. We select the concentration for which this statistic is closest to 0, because we aim to have both a spread distribution, and intermediary values of AUC (between high AUC - resistant samples - and low AUC - not resistant samples). We use package `agrmt`.

```{r, label="Calculating van der Eijk's A"}
if(type_res_data == "growth_curves"){
  
  VdE_A_score = function(conc, t_start, t_end){
    
    all_auc = c()
    for(ts in unique(gcur$obsID)){
      if(length(seq(t_start,t_end)) == length(gcur$value[which((gcur$Time %in% seq(t_start,t_end)) & (gcur$Conc. == conc) & (gcur$obsID == ts))])){
        all_auc = c(all_auc, trapz(x = seq(t_start,t_end), y = gcur$value[which((gcur$Time %in% seq(t_start,t_end)) & (gcur$Conc. == conc) & (gcur$obsID == ts))]))
      }else{
        print(paste0("Problematic number of repetitions for obsID=", ts, " (concentration ", conc, ")."))
      }
    }
    
    hist(round(all_auc), main = paste0("Distribution of AUC values for concentration ", c, " (", t_start, "-", t_end, " hours)"), xlab="AUC values")
    VdE_A = agreement(table(factor(round(all_auc))))
  }
  
  for(c in c(1, 2, 4)){
    print(paste("van der Eijk's A for concentration", c, ":"))
    print(VdE_A_score(conc=c, t_start=15, t_end=50))
  }
  
  rm(c, VdE_A_score)
}
```

We therefore select concentration 1. We average the 2 repetitions of a same sample, and reshape the data to obtain `auc_ob`. We standardize the AUC values to range between 0 (lowest value) and 1 (highest value).

```{r}
if(type_res_data == "growth_curves"){
  
  gcur = gcur[which(gcur$Conc. == 1),]
  
  gcur = dcast(gcur, SampleID + Conc. + FlockID + SamplPoint + Time ~ Repeat, value.var="value")
  gcur$od = (gcur$`1` + gcur$`2`)/2
  gcur = gcur[,c("SampleID", "Conc.", "FlockID", "SamplPoint", "Time", "od")]
  
  auc_ob = as.data.frame(matrix(NA, nrow = length(unique(gcur$FlockID)), ncol = length(unique(gcur$SamplPoint))))
  dimnames(auc_ob) = list(unique(gcur$FlockID), c("S", "M", "E"))
  for(flo in unique(gcur$FlockID)){
    for(sp in unique(gcur$SamplPoint)){
      if(length(15:50) == length(gcur$od[which((gcur$Time %in% (15:50)) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))])){
        auc_ob[flo, sp] = trapz(x = 15:50, y = gcur$od[which((gcur$Time %in% (15:50)) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))])
      }else{
        print(paste0("Impossible to compute area under curve for flock ", flo, ", sampling point ", sp))
      }
    }
  }
  
  auc_ob = (auc_ob - min(auc_ob, na.rm=T))/(max(auc_ob, na.rm=T) - min(auc_ob, na.rm=T))
  
  # auc_ob = auc_ob[which(! rownames(auc_ob) %in% c("0351", "0381", "0391", "0765")),]
  # mic_data = mic_data[which(! mic_data$ID %in% c("0351", "0381", "0391", "0765")),]
  
  res_data = auc_ob

  rm(flo, sp, auc_ob)
}
```

No matters if the resistance metric we chose is "growth_curves" or "mic", the measure is between 0 (lowest resistance) and 1 (highest resistance). We print the first rows of the resistance dataset `res_data`:

```{r}
print(head(res_data))
rm(type_res_data, mic_data)
```

## Antimicrobial use and sampling times

AMU is observed on a weekly basis. The molecules used are recorded either in a qualitative (use / no use) or quantitave (mg used /kg of chicken on farm) format. We create matrixes containing AMU for each week (columns) of the production cycle and for each cycle (rows):

* `col_expo`: qualitative (yes/no) colistin use
* `col_expo_quanti`: quantitative (mg/kg) colistin use
* `allab_expo`: qualitative (yes/no) use of any antibiotic (including colistin)
* `allab_expo_quanti`: quantitative (mg/kg) use of all antibiotics (including colistin). We sum the quantity used for each antibiotic. An other option would be to take the maximum quantity used.

We specify that we include all antibiotics in the `all_ab` category:

```{r}
all_ab = colnames(viparc_data_quali)[10:54]
```

We define `n_cyc`, number of cycles included in the analysis, and n_samp, number of sampling times (3 in the study):

```{r}
n_cyc = nrow(res_data)
n_samp = 3
```

We also define `weeks_samp`, matrix `n_cyc`*`n_samp`, that indicates the week of samplings for each cycle. `n_weeks` is the maximum number of production weeks among cycles included in the study. `init_flock_size` is the initial flock size for each cycle included (not used for now).

```{r}
col_expo = allab_expo = matrix(NA, n_cyc, max(viparc_data_quali$week))
col_expo_quanti = allab_expo_quanti = matrix(NA, n_cyc, max(viparc_data_quanti$week))
weeks_samp = matrix(NA, n_cyc, n_samp)
init_flock_size = rep(NA, n_cyc)
rownames(col_expo) = rownames(col_expo_quanti) = rownames(allab_expo) = rownames(allab_expo_quanti) = rownames(weeks_samp) = names(init_flock_size) = rownames(res_data)

for(i in 1:n_cyc){
  farm_i = id_matrix$FarmID[i]
  flockseq_i = id_matrix$Flockseq[i]
  
  if(!(any(viparc_data_quali$completed[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i))]) == T)){
    print(paste0("For farm ",farm_i,", flock sequence ",flockseq_i,", the production cycle was not completed"))
  }
  
  # Colistin use (qualitative and quantitative):
  
  weeks_col_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,"colistin_use"] == T))]
  
  weeks_col_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,"colistin_use"] == F))]
  
  col_expo[i, weeks_col_use] = 1
  col_expo[i, weeks_col_no_use] = col_expo_quanti[i, weeks_col_no_use] = 0
  
  # Quantitative colistin use
  for(wk in weeks_col_use){
    col_expo_quanti[i, wk] = 1000 * viparc_data_quanti[which((viparc_data_quanti$farm == farm_i)&(viparc_data_quanti$flock == flockseq_i)&(viparc_data_quanti$week == wk)), which(colnames(viparc_data_quali) == "colistin_use")]
  }
  
  # All antibiotics use (qualitative and quantitative):
  
  weeks_allab_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(rowSums(viparc_data_quali[,all_ab], na.rm=T) != 0))]
  
  weeks_allab_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(rowSums(viparc_data_quali[,all_ab], na.rm=T) == 0))]
    
  allab_expo[i, weeks_allab_use] = 1
  allab_expo[i, weeks_allab_no_use] = allab_expo_quanti[i, weeks_allab_no_use] = 0
  
  # Quantitative [all antibiotics] use
  for(wk in weeks_allab_use){
    allab_expo_quanti[i, wk] = 1000 * sum(viparc_data_quanti[which((viparc_data_quanti$farm == farm_i)&(viparc_data_quanti$flock == flockseq_i)&(viparc_data_quanti$week == wk)), which(colnames(viparc_data_quali) %in% all_ab)])
  }
  
  # Weeks of sampling:
  
  weeks_samp[i,] = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali$sampling == T))]
  
  # Initial flock size:
  
  init_flock_size[i] = viparc_data_quali$nb_chicken[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali$week == 1))]
}

weeks_to_keep = which((colSums(!is.na(col_expo)) != 0) & (colSums(!is.na(allab_expo)) != 0))

col_expo = col_expo[,weeks_to_keep]
col_expo_quanti = col_expo_quanti[,weeks_to_keep]
allab_expo = allab_expo[,weeks_to_keep]
allab_expo_quanti = allab_expo_quanti[,weeks_to_keep]

n_weeks = max(weeks_to_keep)

if(any(col_expo != (col_expo_quanti !=0), na.rm=T) | any(allab_expo != (allab_expo_quanti !=0), na.rm=T)){print("ERROR: Qualitative and quantitative AMU data are not consistent.")}

rm(i, wk, weeks_col_use, weeks_col_no_use, weeks_allab_use, weeks_allab_no_use, farm_i, flockseq_i, weeks_to_keep, all_ab, viparc_data_quali, viparc_data_quanti, id_matrix, n_samp, init_flock_size)

```

# Visualization

## Production cycles, AMU and sampling dates

We plot the AMU and sampling dates for the `n_cyc` cycles. We also plot an example of the growth curves available for each sample, and the area under curve.

```{r, fig.width=8, fig.height=6, fig.retina=2}
plot(x=c(0,0.1), y=c(0,0), xlim=c(0,ncol(col_expo)+5), ylim=c(0,n_cyc+10), type="l", xlab="Weeks", ylab="", axes=F)
axis(side = 1)
title(ylab="Cycles", mgp=c(0,1,0))

for (i in 1:n_cyc){
  
  rect(xleft = 0.5, xright = 0.5+max(which(! is.na(col_expo[i,]))), ybottom = i-0.5, ytop = i+0.5, col = "bisque", border="bisque")
  
  for (j in 1:ncol(col_expo)){
    
    if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 1) & (allab_expo[i,j] == 1)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "coral3", border="coral3")
    }else if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 0) & (allab_expo[i,j] == 1)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "deepskyblue4", border="deepskyblue4")
    }

    if (any(weeks_samp[i,] == j, na.rm=T)){
      rect(xleft = j-0.1, xright = j+0.1, ybottom = i-0.5, ytop = i+0.5, col = "black", border="black")
    }
  }
  lines(x = c(0.5, 0.5+max(which(! is.na(col_expo[i,])))), y = c(i-0.5, i-0.5), col="grey")
  lines(x = c(0.5, 0.5+max(which(! is.na(col_expo[i,])))), y = c(i+0.5, i+0.5), col="grey")
}

legend(x=0, y=n_cyc+7, inset=0.5, legend=c("No antimicrobials used", "Antimicrobials used, including colistin", "Non-colistin antimicrobials used"), fill=c("bisque", "coral3", "deepskyblue4"), cex = 0.8, bty = "n")


# 18th line starting from above
p2 = ggplot(data = gcur[(gcur$SampleID == "0681E"),], aes(x=Time, y=od))
p2 = p2 + xlab("Time (hours)") + ylab("OD")
p2 = p2 + geom_ribbon(aes(x=ifelse((Time>15 & Time<=50), Time, 50), ymin=0, ymax=ifelse((Time>15 & Time<=50), od, od[Time==50])), alpha=1, fill="chartreuse4")
p2 = p2 + geom_line(col="black", size=1)

print(p2, vp=viewport(width = 0.205, height = 0.275, x = 0.94, y = 0.39, just = c("right", "bottom")))

rect(xleft=23, xright=31, ybottom=10, ytop=23, lwd=2)
lines(x=c(weeks_samp["0681",3], 23), y=c(18, 17), lwd=2)

rm(i, j, p2, gcur)
```

## Resistance and AMU over time

We plot the observed resistance (red dots) for each cycle.

AMU (only colistin and all antibotics) is represented as colored bars. We can plot the qualitative use (yes/no each week) (`amu_quanti=F`), or the quantitative use (amount per kg of chicken each week) (`amu_quanti=T`). In the latter case, we fix a ceiling, i.e. a maximum of the quantity plotted: this is to avoid one large AMU measure preventing to observe lower quantities on the graph. This ceiling is determined by `ratio_resmet_quanti_amu` which is the ratio between the maximum value of the resistance metric, and the maximum value of the AMU **on the plot** (no consequence on the values in the analysis).

```{r}
amu_quanti = F
ratio_resmet_quanti_amu = 50

```

```{r, warning=F, fig.width=8, fig.height=6}
obs_plot = rbind(data.frame(cyc=rep(NA, n_cyc*n_weeks), week=rep(NA, n_cyc*n_weeks), obser=rep(NA, n_cyc*n_weeks), exp_ab=rep(NA, n_cyc*n_weeks), type_ab=rep("Colistin", n_cyc*n_weeks)),
                 data.frame(cyc=rep(NA, n_cyc*n_weeks), week=rep(NA, n_cyc*n_weeks), obser=rep(NA, n_cyc*n_weeks), exp_ab=rep(NA, n_cyc*n_weeks), type_ab=rep("All antibiotics", n_cyc*n_weeks)))

obs_plot$cyc = as.vector(matrix(rep(rownames(res_data),n_weeks), n_weeks, n_cyc, byrow=T))
obs_plot$week = rep(seq(1,n_weeks),n_cyc)

for (f in rownames(res_data)){
  n_weeks_cyc = sum(!(is.na(col_expo[f,])))
  samp_cyc = which(!(is.na(res_data[f,])))
  
  if(length(samp_cyc) < 2){print(paste0("CAUTION: For cycle ", f,", the number of samples collected is strictly less than 2"))}

  obs_plot$obser[which((obs_plot$cyc==f) & (obs_plot$week %in% weeks_samp[f,samp_cyc]))] = as.numeric(res_data[f,samp_cyc])
  
  if(amu_quanti){
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "Colistin"))] = col_expo_quanti[f,]
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "All antibiotics"))] = allab_expo_quanti[f,]

  }else{
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "Colistin"))] = col_expo[f,]
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "All antibiotics"))] = allab_expo[f,]
  }
}

if(amu_quanti){
  obs_plot$exp_ab[which(obs_plot$exp_ab > (max(res_data, na.rm=T) * ratio_resmet_quanti_amu))] = max(res_data, na.rm=T) * ratio_resmet_quanti_amu
}

obs_plot$cyc = factor(obs_plot$cyc, levels=rownames(res_data))

# Plot:

p = ggplot(data=obs_plot[which(obs_plot$cyc %in% rownames(res_data)),], aes(x=week))
p = p + xlab("Time (weeks)")
p = p + ylab("Resistance metric")

if(amu_quanti){
  
  p = p + ggtitle("Quantitative AMU and colistin resistance in each cycle")
  p = p + geom_bar(aes(y=exp_ab/ratio_resmet_quanti_amu, fill=type_ab), stat="identity", position="dodge2", col=NA)
  
  p = p + scale_y_continuous(sec.axis = sec_axis(~.*ratio_resmet_quanti_amu, name = paste0("AMU (mg/kg) (ceiling ", round(max(res_data, na.rm=T) * ratio_resmet_quanti_amu), " mg/kg for each type)")))
  p = p + theme(axis.text.y.right = element_text(color="blue"), axis.title.y.right = element_text(color="blue"))

}else{
  
  p = p + ggtitle("Qualitative AMU and colistin resistance in each cycle")
  p = p + geom_bar(aes(y=exp_ab/2, fill=type_ab), stat="identity", position="stack", col=NA)
}

p = p +  scale_fill_discrete(name = "Antibiotics used:")
p = p + geom_point(aes(y = obser), size = 3, shape = 21,  fill = "red", color = "black")
p = p + facet_wrap(~ cyc)
p

rm(f, n_weeks_cyc, samp_cyc, far, iter_list, p, pl, ratio_resmet_quanti_amu, obs_plot, amu_quanti, n_weeks)
```

## Resistance plotted versus different AMU metrics in Shiny

This function is useful to:

* Show an interactive Shiny plot (`obj="plot"`) (not shown in the regular html)
* The further estimation of our model (`obj="est"`), as it returns the observed resistance and AMU, depending on the value of some parameters

```{r}

resamu = function(nwkseff, quantuse=T, tempo_amu="exp_decay", thresh_res, ignore.nul=F, lag_amu_init, obj="plot", expo="allab"){
  obtab = data.frame(cycle=NA, amu_init=NA, amu_t=NA, obsres=NA, age=NA, init=NA, prev=NA, end=NA)
  for(cyc_i in 1:n_cyc){
    for(time_i in 2:3){
      
      first_amu_week_consid = weeks_samp[cyc_i, time_i] - nwkseff
      last_amu_week_consid = weeks_samp[cyc_i, time_i] - 1
      
      if(expo == "allab"){
        use_quali = allab_expo
        use_quanti = allab_expo_quanti
      }else if(expo == "col"){
        use_quali = col_expo
        use_quanti = col_expo_quanti
      }
      
      if(tempo_amu %in% c("step", "linear")){
        inc_wks = max(1, first_amu_week_consid):last_amu_week_consid
      }else if(tempo_amu == "exp_decay"){
        inc_wks = 1:last_amu_week_consid
      }
      
      amu_t = rep(0, last_amu_week_consid)
      if(quantuse){
        amu_t[inc_wks] = use_quanti[cyc_i, inc_wks]
      }else{
        amu_t[inc_wks] = use_quali[cyc_i, inc_wks]
      }

      if(tempo_amu == "linear"){
        time_eff = seq(0, 1, length.out = nwkseff+1)[-1]
        amu_t[inc_wks] = amu_t[inc_wks] * time_eff
      }else if(tempo_amu == "exp_decay"){
        time_eff = (exp(-log(100)/nwkseff))^(rev(inc_wks)-1)
        amu_t = amu_t * time_eff
      }
      
      amu_t = sum(amu_t)
      
      if(lag_amu_init > last_amu_week_consid){
        lag_amu_init = last_amu_week_consid
      }
      amu_init = c(use_quanti[cyc_i, (1:floor(lag_amu_init))], (lag_amu_init - floor(lag_amu_init)) * use_quanti[cyc_i, ceiling(lag_amu_init)])
      
      amu_init = sum(amu_init)
      
      obtab = rbind(obtab, c(cyc_i, amu_init, amu_t, res_data[cyc_i, time_i], weeks_samp[cyc_i, time_i], res_data[cyc_i, 1], res_data[cyc_i, time_i-1], time_i==3))
    }
  }
  obtab = obtab[-1,]
  
  if(ignore.nul){
    obtab = obtab[which((obtab$amu_init + obtab$amu_t) != 0),]
  }
  obtab$cycle = as.factor(obtab$cycle)
  
  obtab$res = obtab$obsres >= thresh_res

  if(obj == "plot"){
    p = ggplot(data=obtab, aes(x=amu_t, y=obsres, col=init))
    p = p + xlab("Recent AMU") + ylab("Measure of resistance")
    p = p + ggtitle(paste0("Pearson correlation between recent AMU and the measure of resistance: ", round(cor(obtab$amu_t, obtab$obsres, use="complete.obs"), 3)))
    p = p + geom_point()
    p = p + geom_hline(yintercept = thresh_res, linetype="dashed", col="grey")
    p = p + scale_color_gradient(low = "red", high = "blue")
    # p = ggplotly(p)
    return(p)
  }else if(obj == "est"){
    return(obtab)
  }
}

```

Here, either we run an interactive Shiny app, or we plot a simple graph showing on the y-axis the measured resistance, and on the x-axis the AMU (qualitative, all classes) in the last 3 days before each sampling, with an exponential decay of the effect of antibiotics. The color is the value of resistance in the initial sample of the production cycle (week 1). This is to observe the graphical association between resistance and AMU, and the group of observations for which resistance is high but AMU is low. For the last ones, we want to test if this high resistance could be explained by introductions.

```{r, fig.width=7, fig.height=5}
ui <- fluidPage({
  titlePanel("Correlation between past use and the measure of resistance")
  sidebarLayout(
    sidebarPanel(
      sliderInput("nwkseff",
                  paste0("Number of weeks of past use to take into account (beta):"),
                  min=1,
                  max=25,
                  value=3),
      sliderInput("lag_amu_init",
                  "Number of first weeks of the cycle with permanent effect of use (delta):",
                  min=1,
                  max=15,
                  value=3),
      sliderInput("thresh_res",
                  "Threshold value for the measure of resistance:",
                  min=0,
                  max=1,
                  value=0.5),
      selectInput("tempo_amu",
                  "Temporal effect of AMU:",
                  c("exp_decay", "linear", "step")),
      selectInput("expo",
                  "AMU is all antibiotics (allab) or only colistin (col):",
                  c("allab", "col")),
      selectInput("quantuse",
                  "Select if the quantitative AMU should be used:",
                  c(T, F)),
      selectInput("ignore.nul",
                  "Select if the nul values of use should be ignored:",
                  c(T, F))
    ),
    
    mainPanel(
      plotOutput("plotdisp")
    )
  )
})

server <- function(input, output){
  # define the output, the output id must be the same with the ID in the UI object
  # because the output change as the input change - it's dynamic so we need a special function renderPlot() to make the output react with our choice
  
  output$plotdisp <- renderPlot({
    # filter the data by the input and save into new object
    # the input id must be the same as the UI object
    # plot
    resamu(nwkseff=input$nwkseff, lag_amu_init=input$lag_amu_init, expo=input$expo, quantuse=input$quantuse, tempo_amu=input$tempo_amu, thresh_res=input$thresh_res, ignore.nul=input$ignore.nul)
  })
}

run_shiny = F
if(run_shiny){
  shinyApp(ui=ui, server=server)
}else{
  resamu(nwkseff = 3, quantuse = F, tempo_amu="exp_decay", thresh_res = 0.5, lag_amu_init = 1, obj="plot", expo="allab")
}
rm(ui, server, run_shiny)
```

# Analysis

## Model and Expectation-Maximization algorithm

The introduction of colistin resistant bacteria from outside the flock is not measured but can potentially play an important role in the resistance of the sample. We consider a latent (unobserved) binary variable to describe if an introduction happened in the farm (yes/no).

We use an Expectation-Maximization (E-M) algorithm with 2 steps:

* Expectation: given a set of parameters, determine the most likely status for introduction (yes/no) for each observation.
* Maximization: given an assumed introduction status, determine the set of parameters maximizing the likelihood of the model.

The algorithm is initiated with a random set of parameters, and is terminated when a convergence of likelihood is reached.

For all observations in the 2nd or 3rd round of samplings, we define a model used in the Maximization step. Several models are tested, all being simplified versions of this full model:

$$R_t = \sum_{k \in \{ t_1 ; t_2 \} }(\lambda_k.R_k) + \eta.f(t) + \alpha.AMU_t + \theta.AMU_{init} + \mu$$
where:

* $R$ is the measure of sample's resistance
* $t$ is the week the sample was collected
* $R_{t_1}$ is the measure of resistance in the initial sample of the same flock
* $R_{t_2}$ is the measure of resistance in the previous sample if the observation belongs to the 3rd round of sampling (autocorrelation)
* $f(t)=1$ if an introduction occurred before the sampling (between $t-\xi$ and $t-1$, where $\xi$ is fixed at 4 weeks), and $f(t)=0$ otherwise
* $\mu$ is the average resistance when all other variables are null.

The AMU metric $AMU_t$ is the use that just precedes the sampling date, and is defined as:

$$AMU_t=\sum_{i=1}^{t-1}(U(i).exp[-\frac{ln(1/\epsilon)}{\beta}.(t-i-1)])$$

where $U(i)$ is the (quantitative or qualitative) antimicrobial use in the flock on week i. For now, in the following, we consider only qualitative use (= 0 or 1).

As shown in the figure below, $AMU_t$ is parametrized as an exponential decay: after $\beta$ weeks, the effect is as low as $\epsilon$, that we fix at 5%.

$AMU_{init}$ is the use that occurs at the begining of the cycle (first $\delta$ weeks). It is uncertain if it has effect on the resistance during the whole production cycle. We test this hypothesis. It is defined as:

$$AMU_{init}=\sum_{i=1}^{\delta}U(i)$$

```{r echo=FALSE, fig.width=7, fig.height=5}
metramu = data.frame(x = seq(0,10,0.01), exp = 0.95*exp(-log(20)/(10-6))^seq(10,0,-0.01), step = c(rep(0.3, 451), rep(0, 550)))
p = ggplot(data=metramu, aes(x=x))
p = p + ggtitle("Metric for the past use of antibiotics (AMU(t) and AMU(init))")
p = p + xlab("Weeks i") + theme(axis.title.y=element_blank())
p = p + geom_ribbon(aes(ymin=0, ymax=pmax(exp, step)), alpha=0.3)
p = p + geom_line(aes(y=exp, col="1"), size=1)
p = p + geom_step(aes(y=step, col="2"), size=1)
p = p + geom_line(aes(y=0.05), linetype="dashed")
p = p + geom_line(aes(y=0.95), linetype="dashed")
p = p + geom_point(aes(x=6, y=0.05), size=4, col="blue")
p = p + geom_text(aes(x=4.5, y=0.38, label = "delta"), parse = T, size=5, col="red")
p = p + geom_text(aes(x=0, y=0.38, label = "0"), parse = T, size=5, col="red")
p = p + geom_text(aes(x=-0.2, y=0.3, label = "theta"), parse = T, size=5, col="black")
p = p + geom_text(aes(x=6, y=0.15, label = "t-beta"), parse = T, size=5, col="blue")
p = p + geom_text(aes(x=10, y=1, label = "t"), parse = T, size=5, col="black")
p = p + geom_text(aes(x=-0.2, y=0.05, label = "epsilon"), parse = T, size=5, col="black")
p = p + geom_text(aes(x=-0.2, y=0.95, label = "alpha"), parse = T, size=5, col="black")
p = p + scale_color_manual(name="", values=c("1"="blue", "2"="red"), labels=c("1"="AMU(t)", "2"="AMU(init)"))
p

rm(metramu, p)
```


## Variations of the model

The full model described above is model 24. Models 1 to 23 are simplifications of model 24.

|Models|Previous resistance   |AMU             |Introductions|
|:----:|:--------------------:|:--------------:|:-----------:|
|**1** |None                  |None            |None         |
|**2** |Initial               |None            |None         |
|**3** |Initial + Previous    |None            |None         |
|**4** |None                  |Initial         |None         |
|**5** |Initial               |Initial         |None         |
|**6** |Initial + Previous    |Initial         |None         |
|**7** |None                  |Recent          |None         |
|**8** |Initial               |Recent          |None         |
|**9** |Initial + Previous    |Recent          |None         |
|**10**|None                  |Initial + Recent|None         |
|**11**|Initial               |Initial + Recent|None         |
|**12**|Initial + Previous    |Initial + Recent|None         |
|**13**|None                  |None            |Possible     |
|**14**|Initial               |None            |Possible     |
|**15**|Initial + Previous    |None            |Possible     |
|**16**|None                  |Initial         |Possible     |
|**17**|Initial               |Initial         |Possible     |
|**18**|Initial + Previous    |Initial         |Possible     |
|**19**|None                  |Recent          |Possible     |
|**20**|Initial               |Recent          |Possible     |
|**21**|Initial + Previous    |Recent          |Possible     |
|**22**|None                  |Initial + Recent|Possible     |
|**23**|Initial               |Initial + Recent|Possible     |
|**24**|Initial + Previous    |Initial + Recent|Possible     |


## Useful functions

### Function returning the minus loglikelihood

This function enters the raw (transformed) values of the model's parameters and the probability of introduction for each observation, and returns either the minus loglikelihood (`aim="est"`), or the predicted and observed values of resistance and AMU, the minus loglikelihood and the true (not transformed) values of parameters (`aim="pred"`).

Observations for which the resistance measure (model outcome) is absent are deleted. However, if an observation has NA values for measure of resistance in previous samples (`obsdat$init` and `obsdat$prev`), the value is set at the mean of the other values.

We also define `run2_intr` which is a wraper: its output is the function `run2` with the value of `intr` (input of `run2`) specified.

```{r}
run2 = function(aim="est", intr=introd, distrib_res="normal", mu, lambda1, lambda2, alpha, beta, teta, delta, eta, Sd){

  mu = sigmoid(mu)
  lambda1 = lambda1
  lambda2 = lambda2
  alpha = alpha
  beta = 10 * sigmoid(beta) +1
  teta = teta
  delta = 5 * sigmoid(delta) +1
  eta = sigmoid(eta)
  Sd = 3 * sigmoid(Sd)

  obsdat = resamu(nwkseff=beta, quantuse=F, tempo_amu="exp_decay", thresh_res=0.4, ignore.nul=F, lag_amu_init=delta, obj="est")
  obsdat = obsdat[! is.na(obsdat$obsres),]
  obsdat$init[is.na(obsdat$init)] = mean(obsdat$init, na.rm=T) # 0
  obsdat$prev[is.na(obsdat$prev)] = mean(obsdat$prev, na.rm=T) # 0
  obsdat$introduc = intr

  obsdat$predmod_intro = mu + lambda1 * obsdat$init + (lambda2 * obsdat$prev)*as.numeric(obsdat$end) + alpha * obsdat$amu_t + teta * obsdat$amu_init + eta
  
  obsdat$predmod_nointro = mu + lambda1 * obsdat$init + (lambda2 * obsdat$prev)*as.numeric(obsdat$end) + alpha * obsdat$amu_t + teta * obsdat$amu_init
  
  obsdat$predmod = obsdat$introduc * obsdat$predmod_intro + (1-obsdat$introduc) * obsdat$predmod_nointro

  if(aim == "est"){
    if(distrib_res == "normal"){
      lik_intro = dnorm(x=obsdat$obsres, mean=obsdat$predmod_intro, sd=Sd)
      lik_nointro = dnorm(x=obsdat$obsres, mean=obsdat$predmod_nointro, sd=Sd)
      
      total_loglik = log(obsdat$introduc * lik_intro + (1-obsdat$introduc) * lik_nointro)
      total_loglik[is.infinite(total_loglik)] = -710

      return(-sum(total_loglik))
      
    }else{
      print("Residuals distribution not defined.")
    }
  }else if (aim == "pred"){
    return(list(obsdat = obsdat[,c("obsres", "amu_init", "amu_t", "introduc", "predmod")], minll = -sum(dnorm(x=obsdat$obsres, mean=obsdat$predmod, sd=Sd, log=T)), true_param = c(mu, lambda1, lambda2, alpha, beta, teta, delta, eta, Sd)))
  }
}

# Factory/Wrapper function (returns a function with one of the parameters ("intr") fixed)
run2_intr <- function(x) {
 function(aim="est", distrib_res="normal", mu, lambda1, lambda2, alpha, beta, teta, delta, eta, Sd)
   run2(aim, intr=x, distrib_res, mu, lambda1, lambda2, alpha, beta, teta, delta, eta, Sd)
}

```

### Function plotting a model's fit

This function takes as input the fitted model, and returns a plot with one dot per observation: the observed resistance on x-axis, the predicted resistance on y-axis.

```{r}
fitplot = function(fitted_model, intro){
  fit_coef = coef(fitted_model)
  
  obs_vs_pred = run2(aim="pred", intr=intro, mu=fit_coef["mu"], lambda1=fit_coef["lambda1"], lambda2=fit_coef["lambda2"], alpha=fit_coef["alpha"], beta=fit_coef["beta"], teta=fit_coef["teta"], delta=fit_coef["delta"], eta=fit_coef["eta"], Sd=fit_coef["Sd"])
  
  minll = obs_vs_pred[[2]]
  print(paste("Minus LogLikelihood:", minll))
  
  true_fit_coef = obs_vs_pred[[3]]
  print("True values of parameters:")
  print(true_fit_coef)
  
  p = ggplot(obs_vs_pred[[1]], aes(x = obsres, y = predmod, fill = true_fit_coef["alpha"]*amu_t + true_fit_coef["teta"]*amu_init, col = introduc))
  p = p + xlab("Observed resistance") + ylab("Predicted resistance")
  p = p + ggtitle(paste0("Observed VS Predicted resistance (minus LogLikelihood =", round(minll, 2), ")"))
  p = p + geom_abline(intercept = 0, slope = 1, linetype = "dashed")
  p = p + geom_point(shape = 21, size = 3)
  p = p + labs(col = "Probability of introduction (outline):", fill = "Global AMU metric (interior color):")
  p = p + scale_colour_gradient(low = "green", high = "red")
  p = p + scale_fill_gradient(low = "white", high = "blue")
  p
}
```

### Example of estimation of the full model for one given introduction status

Here, we test the full model's estimation (M-step on model 24), with one given introduction status for each observation. We use "bbmle" package for point estimates.

```{r, label="Test full model estimation", fig.width=6, fig.height=4}
introd = c(rep(0, 4), 1, 0, 1, 1, 1, rep(0, 6), 1, 0, 1, rep(0, 3), 1, 0, 0, 1, rep(0, 7), 1, 0, 0, 1, 1, 1, 0, 1, rep(0, 5), 1, rep(0, 3))

fit_mle2 = mle2(minuslogl = run2_intr(introd), method = "Nelder-Mead", control = list(maxit = 5000), start = list(mu = logit(runif(1)), lambda1 = runif(1), lambda2 = runif(1), alpha = runif(1), beta = logit(runif(1)), teta = runif(1), delta = logit(runif(1)), eta = logit(runif(1)), Sd = logit(runif(1))))

fitplot(fitted_model = fit_mle2, intro = introd)
```

```{r}
rm(introd)
```

### Function repeating the fit several times

This function repeats the model's fit process several times, to make sure the maximum of likelihood is not only local. Among repetitions, the model with the highest likelihood is the one chosen by the function.

The function takes as inputs the model's parameters that should be fixed (not estimated), and those that should be estimated. This is to adapt to the different variations of the model (see above). Some parameters are transformed to be estimated in an interval.

It is made such that the random initialization of parameters is different from one repetition to another.

```{r}
rep_est = function(nrep, minusloglrep, methodrep, controlrep, fixedvar, startvar){
  modrep = list()
  for(repet in 1:nrep){
    cat("\r", paste0("Fit repetition ", repet, "/", nrep))
    
    valpar = list(mu=NULL, lambda1=NULL, lambda2=NULL, alpha=NULL, beta=NULL, teta=NULL, delta=NULL, eta=NULL, Sd=NULL)
    valpar[fixedvar] = 0
    valpar[startvar] = runif(length(startvar))
    valpar[c("mu", "beta", "delta", "eta", "Sd")] = lapply(X = valpar[c("mu", "beta", "delta", "eta", "Sd")], FUN = logit)
    
    modrep[[repet]] = mle2(minuslogl=minusloglrep, method=methodrep, control=controlrep, fixed=valpar[fixedvar], start=valpar[startvar])
  }
  loglikrep = lapply(X=modrep, FUN=logLik)
  
  cat("\n")
  
  modrep[[which.max(loglikrep)]]
}

```

### Implementation of the full E-M algorithm

#### Function for the E step

With one given set of parameters for the model, we determine what is the most likely probability for the occurrence of an introduction in the flock, for each observation.

We first calculate the residuals $r_{intro}^i$ (`r_intro`) and $r_{nointro}^i$ (`r_nointro`) of observations $R_i$ (observed resistance) under the 2 models: $M_{intro}$ ("Only introductions") and $M_{nointro}$ ("No introduction").

The idea is to compute, for each observation, the probability that it is assigned to each of the 2 models:

$$p(R_i \in M_{intro}) = \frac{p(r_{intro}^i/R_i \in M_{intro})}{p(r_{intro}^i/R_i \in M_{intro}) + p(r_{nointro}^i/R_i \in M_{nointro})}$$

And:

$$p(R_i \in M_{nointro}) = 1-p(R_i \in M_{intro})$$

As the residuals follow a normal distribution (with mean=0 and standard deviation $\sigma$ (`sigma`), estimated in the M-step):

$$p(R_i \in M_{intro}) = \frac{exp(\frac{-(r_{intro}^i)^2}{2.\sigma^2})}{exp(\frac{-(r_{intro}^i)^2}{2.\sigma^2}) + exp(\frac{-(r_{nointro}^i)^2}{2.\sigma^2})}$$

(Sources: http://www.cs.huji.ac.il/~yweiss/emTutorial.pdf ; http://www.di.fc.ul.pt/~jpn/r/EM/EM.html#eg-em-with-mix-of-two-linear-models)

```{r}
E_step <- function(observed, params) {

  sigma = 3 * sigmoid(params[["Sd"]]) # because the parameter is modified when estimated in function "run2"
  
  lin_intro = run2(aim="pred", intr=1, mu=params[["mu"]], lambda1=params[["lambda1"]], lambda2=params[["lambda2"]], alpha=params[["alpha"]], beta=params[["beta"]], teta=params[["teta"]], delta=params[["delta"]], eta=params[["eta"]], Sd=params[["Sd"]])[["obsdat"]]$predmod
  
  lin_nointro = run2(aim="pred", intr=0, mu=params[["mu"]], lambda1=params[["lambda1"]], lambda2=params[["lambda2"]], alpha=params[["alpha"]], beta=params[["beta"]], teta=params[["teta"]], delta=params[["delta"]], eta=params[["eta"]], Sd=params[["Sd"]])[["obsdat"]]$predmod

  # Residuals of observations under models "Intro" and "No intro"
  r_intro = abs(lin_intro - observed)
  r_nointro = abs(lin_nointro - observed)

  # Residuals follow a normal distribution
  exp_intro = exp(- r_intro^2/(2*sigma^2))
  exp_nointro = exp(- r_nointro^2/(2*sigma^2))

  exp_intro = pmin(0.9999, exp_intro)
  exp_intro = pmax(0.0001, exp_intro)
  exp_nointro = pmin(0.9999, exp_nointro)
  exp_nointro = pmax(0.0001, exp_nointro)

  # Probability of "Intro":
  prob_intro = exp_intro / (exp_intro + exp_nointro)

  prob_intro
}
```

#### Function for the M step

Given a probability of introduction for each observation, we fit the model. Depending on the model, some parameters can be fixed in the `mle2` function. Several repetitions of the fit are computed, with function `rep_est`.

```{r}
M_step <- function(introd, modnum) {

  prop_intro = mean(introd)
  
  #####
  if(modnum == 13){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda1",  "lambda2",  "alpha",  "beta",  "teta",  "delta"), startvar = c("mu", "eta", "Sd"))
  }else if(modnum == 14){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2",  "alpha",  "beta",  "teta",  "delta"), startvar = c("mu",  "lambda1", "eta", "Sd"))
  }else if(modnum == 15){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "alpha",  "beta",  "teta",  "delta"), startvar = c("mu",  "lambda1",  "lambda2", "eta", "Sd"))
  }else if(modnum == 16){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda1",  "lambda2",  "alpha",  "beta"), startvar = c("mu",  "teta",  "delta", "eta", "Sd"))
  }else if(modnum == 17){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2",  "alpha",  "beta"), startvar = c("mu",  "lambda1",  "teta",  "delta", "eta", "Sd"))
  }else if(modnum == 18){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "alpha",  "beta"), startvar = c("mu",  "lambda1",  "lambda2",  "teta",  "delta", "eta", "Sd"))
  }else if(modnum == 19){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda1",  "lambda2",  "teta",  "delta"), startvar = c("mu",  "alpha",  "beta", "eta", "Sd"))
  }else if(modnum == 20){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2",  "teta",  "delta"), startvar = c("mu",  "lambda1",  "alpha",  "beta", "eta", "Sd"))
  }else if(modnum == 21){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "teta",  "delta"), startvar = c("mu",  "lambda1",  "lambda2",  "alpha",  "beta", "eta", "Sd"))
  }else if(modnum == 22){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda1",  "lambda2"), startvar = c("mu",  "alpha",  "beta",  "teta",  "delta", "eta", "Sd"))
  }else if(modnum == 23){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2"), startvar = c("mu",  "lambda1",  "alpha",  "beta",  "teta",  "delta", "eta", "Sd"))
  }else if(modnum == 24){
    modstep = rep_est(nrep=5, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c(), startvar = c("mu",  "lambda1",  "lambda2",  "alpha",  "beta",  "teta",  "delta", "eta", "Sd"))
  }
  #####
  
  raw_coef = coef(modstep)
  ob_pred = run2(aim="pred", intr=introd, mu=raw_coef["mu"], lambda1=raw_coef["lambda1"], lambda2=raw_coef["lambda2"], alpha=raw_coef["alpha"], beta=raw_coef["beta"], teta=raw_coef["teta"], delta=raw_coef["delta"], eta=raw_coef["eta"], Sd=raw_coef["Sd"])
  
  raw_coef = c(raw_coef, p_intro = prop_intro)
  llmod = ob_pred[[2]]
  true_coef = ob_pred[[3]]
  
  list(raw_coef, llmod, modstep, true_coef)
}
```

#### Function to launch the whole algorithm

We repeat the algorithm several times to make sure the whole space has been explored.

Convergence is defined as: the distance between the likelihood and the former likelihood is less than 1, 3 times in a row.

```{r}
EM_algo <- function(modnum, numb_rep_algo=5, tol=1, max.step=100) {

  repetalgo = list()
  repetalgo_minloglik = c()
  
  for(algorep_ite in 1:numb_rep_algo){
    print(paste0("##### Model ", modnum, " (repetition ", algorep_ite, "/", numb_rep_algo, " of the algorithm) #####")); cat("\n")
    
    step = 0
    minloglik = 10^6
    
    raw_params = list(mu = logit(runif(1)), lambda1 = runif(1), lambda2 = runif(1), alpha = runif(1), beta = logit(runif(1)), teta = runif(1), delta = logit(runif(1)), eta = logit(runif(1)), Sd = logit(runif(1)), p_intro = runif(1))
    
    obs_res = resamu(obj="est", nwkseff=1, thresh_res=0.5, lag_amu_init=1)$obsres
    obs_res = obs_res[! is.na(obs_res)]

    counting_for_convergence = 0
    while((counting_for_convergence <3) & (step <= max.step+1)){
      step = step +1
      if(step == max.step+1){
        print(paste0("Convergence of the likelihood was not reached after ", max.step, " steps."))
        break
      }
      
      print(paste0("Step: ", step))
      introductions = E_step(observed=obs_res, params=raw_params)
      output_mstep = M_step(introd=introductions, modnum)
      raw_params = output_mstep[[1]]
      old.minloglik = minloglik
      minloglik = output_mstep[[2]]
      print(paste0("Minus Log Likelihood: ", minloglik))
      modstep = output_mstep[[3]]
      param_true_format = output_mstep[[4]]
  
      if (abs(minloglik - old.minloglik) < tol){
        counting_for_convergence = counting_for_convergence +1
      }else{
        counting_for_convergence = 0
      }
    }
    write.table(c(), file=paste0("C:/Users/Jonathan/Desktop/Model ", modnum, " completed.txt"))
    print(paste0("Final step: ", step)); cat("\n")
    
    repetalgo[[algorep_ite]] = list(modstep, raw_params, introductions, minloglik, param_true_format, number_obs=length(obs_res))
    repetalgo_minloglik[algorep_ite] = minloglik
  }
  
repetalgo[[which.min(repetalgo_minloglik)]]
}
```

### Test of the E-M algorithm on simulated data

From the real AMU data, we want to create fake data of resistance showing a clear relationship between use and resistance. We also want to add fake introductions above this. This is to test the ability of the algorithm to detect an effect of the recent AMU on resistance, along with introductions.

First, we save the true resistance data:

```{r}
res_data_saved = res_data
```

We create fake resistance data such that the relationship with AMU is linear, with $\beta_{fake} = 8$ (`beta_fake`) and $\alpha_{fake} = 0.4$ (`alpha_fake`). Note that we add some noise.

```{r, fig.width=7, fig.height=5}
beta_fake = 8
alpha_fake = 0.4

fake_res = run2(aim="pred", intr=0, mu=logit(0.05), lambda1=0, lambda2=0, alpha=alpha_fake, beta=logit((beta_fake-1)/10), teta=0, delta=1, eta=0, Sd=0.1)[["obsdat"]]$predmod
fake_res = fake_res + rnorm(length(fake_res), 0, 0.05)

res_data = as.data.frame(cbind(res_data$S, matrix(c(fake_res[1:40], NA, fake_res[41:49]), nrow=length(res_data$S), ncol=2, byrow=T)))

resamu(nwkseff = 8, quantuse=F, thresh_res = 0.5, lag_amu_init = 1)
```

Now we add some introductions in observations with low AMU, to simulate the fact observations with low AMU have sometimes a high resistance:

```{r, fig.width=7, fig.height=5}
introd_fake = sample(x = which(fake_res < 0.2), size = round(0.5 * length(which(fake_res < 0.2))))
fake_res[introd_fake] = fake_res[introd_fake] + rnorm(length(introd_fake), 0.5, 0.05)

res_data = res_data_saved
res_data = as.data.frame(cbind(res_data$S, matrix(c(fake_res[1:40], NA, fake_res[41:49]), nrow=length(res_data$S), ncol=2, byrow=T)))

resamu(nwkseff = 8, quantuse=F, thresh_res = 0.5, lag_amu_init = 1)
```

Therefore, we are in a situation where the fake resistance data was simulated by model 19 (introductions + recent AMU). We want to test now if the algorithm can detect the relationship between AMU and fake resistance data. We also want to know if model 19 is selected against model 13 (introductions only).

We apply the algorithm on model 13:

```{r, label="E-M algorithm on fake resistance data (model 13)", fig.width=6, fig.height=4}
results_mod13 = EM_algo(modnum=13, numb_rep_algo=1)
fitplot(fitted_model = results_mod13[[1]], intro = results_mod13[[3]])
```

And we apply the algorithm on model 19:

```{r, label="E-M algorithm on fake resistance data (model 19)", fig.width=6, fig.height=4}
results_mod19 = EM_algo(modnum=19, numb_rep_algo=1)
fitplot(fitted_model = results_mod19[[1]], intro = results_mod19[[3]])
```

The fit seems good. Let us compare both models' AIC:

```{r}
print("AIC of model 13:")
AIC(results_mod13[[1]]) + results_mod13[["number_obs"]]

print("AIC of model 19:")
AIC(results_mod19[[1]]) + results_mod19[["number_obs"]]
```

The algorithm detects the relationship between use and fake resistance, and model 19 has a lower AIC than model 13.

We delete the fake resistance data and come back to the true data:

```{r}
res_data = res_data_saved

rm(res_data_saved, results_mod13, results_mod19, beta_fake, alpha_fake, fake_res, introd_fake)
```

## Results

If the computation was already performed, `alread_comp = T`.

```{r}
alread_comp = T
```

### Estimation of models 1 to 12 without introduction

First, we fit models that do not take into account the introductions. For them, we do not need to implement the E-M algorithm. Because the package `bbmle` does not always find the likelihood maximum, we repeat the estimation 10 times with `rep_est`, and take the maximum likelihood among these repetitions. The list `lfit` contains all fitted models.

```{r, label="Estimation of the models without introduction"}

if(alread_comp){
  load("C:/Users/Jonathan/Desktop/Resultats simulations/colistin_resistance/lfit.rdata")
}else{
  introd = 0
  lfit = list()
  
  lfit[[1]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c("lambda1", "lambda2", "alpha", "beta", "teta", "delta", "eta"), startvar = c("mu", "Sd"))
  
  lfit[[2]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2",  "alpha",  "beta",  "teta",  "delta", "eta"), startvar = c("mu",  "lambda1", "Sd"))
  
  lfit[[3]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "alpha",  "beta",  "teta",  "delta", "eta"), startvar = c("mu",  "lambda1",  "lambda2", "Sd"))
  
  lfit[[4]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c("lambda1", "lambda2", "alpha", "beta", "eta"), startvar = c("mu", "teta", "delta", "Sd"))
  
  lfit[[5]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2",  "alpha",  "beta", "eta"), startvar = c("mu",  "lambda1",  "teta",  "delta", "Sd"))
  
  lfit[[6]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "alpha",  "beta", "eta"), startvar = c("mu",  "lambda1",  "lambda2",  "teta",  "delta", "Sd"))
  
  lfit[[7]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda1",  "lambda2",  "teta",  "delta", "eta"), startvar = c("mu",  "alpha",  "beta", "Sd"))
  
  lfit[[8]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2",  "teta",  "delta", "eta"), startvar = c("mu",  "lambda1",  "alpha",  "beta", "Sd"))
  
  lfit[[9]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "teta",  "delta", "eta"), startvar = c("mu",  "lambda1",  "lambda2",  "alpha",  "beta", "Sd"))
  
  lfit[[10]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda1",  "lambda2", "eta"), startvar = c("mu",  "alpha",  "beta",  "teta",  "delta", "Sd"))
  
  lfit[[11]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c( "lambda2", "eta"), startvar = c("mu",  "lambda1",  "alpha",  "beta",  "teta",  "delta", "Sd"))
  
  lfit[[12]] = rep_est(nrep=10, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), fixedvar = c("eta"), startvar = c("mu",  "lambda1",  "lambda2",  "alpha",  "beta",  "teta",  "delta", "Sd"))
}

rm(introd)

```

### Parallel computation of the E-M algorithm for models 13 to 24

We repeat the algorithm 5 times (with different seeds) per model, and each maximization step (model fit) is repeated 5 times (with different seeds too) to make sure the likelihood maximum is reached.

```{r, label="E-M algorithm on all models"}

if(alread_comp){
  load("C:/Users/Jonathan/Desktop/Resultats simulations/colistin_resistance/resulpar.rdata")
}else{
  cl<-makeCluster(7)
  clusterEvalQ(cl, list(library(pracma), library(bbmle)))
  clusterExport(cl, c("E_step","M_step","rep_est","run2","run2_intr","resamu","n_cyc","res_data","weeks_samp","col_expo","col_expo_quanti","allab_expo","allab_expo_quanti"))
  t = as.numeric(Sys.time())
  resulpar = parLapply(cl, X=13:24, fun=EM_algo, numb_rep_algo=5)
  stopCluster(cl)
  as.numeric(Sys.time()) - t
  
  save(resulpar, file = "C:/Users/Jonathan/Desktop/resulpar.rdata")
  
  for(m in 1:12){lfit[[m+12]] = resulpar[[m]][[1]]}
  save(lfit, file = "C:/Users/Jonathan/Desktop/lfit.rdata")
  rm(m)
}

rm(alread_comp)
```

### Comparison of the models

The list `lfit` contains the 24 fitted models. We compare and plot their loglikelihood and AIC weights (= probability that each model is the best).

```{r, fig.width=8, fig.height=6}
l_loglik = unlist(lapply(X = lfit, FUN = logLik))
l_aic = unlist(lapply(X = lfit, FUN = AIC))
l_aic[13:24] = l_aic[13:24] + resulpar[[1]][["number_obs"]]

l_aic = exp(-(l_aic - min(l_aic))/2) / sum(exp(-(l_aic - min(l_aic))/2))

plot_aic_ll = data.frame(mod = as.factor(1:24), AIC_weights = l_aic, MinusLogLikelihood = -l_loglik)
plot_aic_ll = melt(plot_aic_ll, id = "mod")

p = ggplot(data = plot_aic_ll, aes(y = mod))
p = p + ggtitle("Comparison of the fitted models") + xlab("Indicator value") + ylab("Models")
p = p + geom_vline(data = subset(plot_aic_ll, variable == "AIC_weights"), aes(xintercept = min(plot_aic_ll$value[plot_aic_ll$variable == "AIC_weights"])), linetype = "dashed")
p = p + geom_vline(data = subset(plot_aic_ll, variable == "MinusLogLikelihood"), aes(xintercept = min(plot_aic_ll$value[plot_aic_ll$variable == "MinusLogLikelihood"])), linetype = "dashed")
p = p + geom_point(aes(x = value), shape=18, size=2)
p = p + facet_wrap(~variable, scales = "free_x")
p = ggplotly(p)
p

rm(p, plot_aic_ll, l_loglik, l_aic)
```

Models 13 and 19 have the highest AIC weights (= lowest AIC). We plot their fit (observed VS predicted):

```{r, fig.width=6, fig.height=4}
fitplot(fitted_model = resulpar[[1]][[1]], intro = resulpar[[1]][[3]])
print(paste0("Probability of introduction in model 13: ", resulpar[[1]][[2]][["p_intro"]]))

fitplot(fitted_model = resulpar[[7]][[1]], intro = resulpar[[7]][[3]])
print(paste0("Probability of introduction in model 19: ", resulpar[[7]][[2]][["p_intro"]]))

```

We can perform a Likelihood Ratio Test between models 13 and 19:

```{r}
bbmle::anova(lfit[[13]], lfit[[19]])

```

In conclusion, model 19 (introductions + recent AMU) seems to explain best the data, followed closely by model 13 (introductions only). With the LRT, it is not significant to add variables to complexify model 13.

The most important factor explaining the level of colistin resistance measured in the farms is therefore the introductions. The AMU (all antibiotics) in the 3.3 (value of Beta) weeks preceding the sampling seems to play a role too, even though the p-value of the LRT is only 0.12 (not significant). On the contrary, the AMU at the beginning of the production cycle does not appear to play an important role, nor the resistance in the previous samples in the same flock (autocorrelation).

### Alternative analysis

An other way to perform the analysis could be to:

1) Compare models without introduction (models 1 to 12)
2) Among the best of them (highest AIC weight), compute the E-M algorithm to check if adding introductions improves the fit
3) Compare the models that are left

Among models 1-12, we compare AIC weights:

```{r, fig.width=8, fig.height=6}
l_aic = unlist(lapply(X = lfit[1:12], FUN = AIC))

l_aic = exp(-(l_aic - min(l_aic))/2) / sum(exp(-(l_aic - min(l_aic))/2))

print(l_aic)
rm(l_aic)
```

we select models 7 (AIC weight = 44.3%) and 8 (AIC weight = 23.2%), and perform the E-M algorithm with them only. In the end, we therefore compare models 7, 8, 19 and 20:


```{r, fig.width=8, fig.height=6}
l_aic = unlist(lapply(X = lfit[c(7,8,19,20)], FUN = AIC))

l_aic = exp(-(l_aic - min(l_aic))/2) / sum(exp(-(l_aic - min(l_aic))/2))

print(l_aic)
rm(l_aic)
```

In this case, model 19 (AIC weight = 73%) is the final model chosen.
