---
title: "Relative contributions of antimicrobial use and external contaminations on colistin resistance in Mekong delta chicken farms"
csl: the-american-naturalist.csl
# runtime: shiny
output:
  html_document:
    theme: cerulean
    toc: yes
    toc_float: true
  pdf_document:
    toc: yes
<!-- bibliography: references.bib -->
---

<!--
IMAGES:
Insert them with: ![alt text](image.png)
You can also resize them if needed: convert image.png -resize 50% image.png
If you want to center the image, go through HTML code:
<div style="text-align:center"><img src ="image.png"/></div>

REFERENCES:
For references: Put all the bibTeX references in the file "references.bib"
in the current folder and cite the references as @key or [@key] in the text.
Uncomment the bibliography field in the above header and put a "References"
title wherever you want to display the reference list.
-->

<style type="text/css">
.main-container {
  max-width: 1370px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r general_options, include = FALSE}
knitr::knit_hooks$set(
  margin = function(before, options, envir) {
    if (before) par(mgp = c(1.5, .5, 0), bty = "n", plt = c(.105, .97, .13, .97))
    else NULL
  },
  prompt = function(before, options, envir) {
    options(prompt = if (options$engine %in% c("sh", "bash")) "$ " else "> ")
  })

knitr::opts_chunk$set(margin = TRUE, prompt = TRUE, comment = "",
                      collapse = TRUE, cache = FALSE, autodep = TRUE,
                      dev.args = list(pointsize = 11), fig.height = 3.5,
                      fig.width = 4.24725, fig.retina = 2, fig.align = "center")

options(width = 137)
```

The objective is to understand the relative contributions of different factors to the colistin resistance detected in samples from ViParc chicken farms:

* antimicrobial use (AMU)
* the carriage of colistin resistance in day-old chicks
* unobserved contamination events

Available data is:

* Data on ViParc farms, including AMU and sampling dates: [ViParc data overview](https://rpubs.com/choisy/viparc_data).
* During each production cycle, 3 (or less) pooled chicken faeces samples are collected: at the beginning, in the middle and at the end of the cycle. Around 30 E.coli colonies are collected from each sample and pooled. The Optical Density of each of these pooled samples is measured twice to obtain their growth curve in presence of different colistin concentrations. For each sample, the MIC can be determined from these growth curves.

We first clean work environment and load needed packages.

```{r}
rm(list=ls(all=TRUE))

library(deSolve)
library(bbmle)
library(ggplot2)
library(ggrepel)
library(grid)
library(reshape2)
library(readxl)
library(pracma)
library(agrmt)
library(shiny)
library(parallel)
library(plotly)
library(sp)
library(ape)
library(rgdal)
library(ggsn)
library(ggforce)
library(dendextend)
library(mixtools)
library(ggpubr)

```

# Data

We define options for the analysis (explained later):

```{r}
run_shiny = F
definition_amu = "allab"
amu_model_is_quanti = F
type_mod = "logis" #"linear"
effect_recent_amu = "step"
alread_comp = F
test_method_comp = F
almost_one = 0.999
conc_used = 1

```

## Load data

We load data:

* `viparc_data_quali` and `viparc_data_quanti`: Includes production cycles, AMU (resp. qualitative and quantitative), sampling dates in ViParc farms.
* `gcur`: Optical density (growth curves) measured in the samples at different times, at different colistin concentrations
* `mic_data`: MIC for each sample

```{r}
# viparc_data_quali = read.csv("https://raw.githubusercontent.com/viparc/colistin_resistance/master/data/viparc_qualitative.csv")
# viparc_data_quanti = read.csv("https://raw.githubusercontent.com/viparc/colistin_resistance/master/data/viparc_quantitative.csv")

viparc_data_quali = read.csv("C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/viparc_qualitative.csv")
viparc_data_quanti = read.csv("C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/viparc_quantitative.csv")

mic_data = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/ColR_MIC.xlsx", sheet="MIC data"))
mic_data$MIC = as.numeric(mic_data$MIC)
mic_data = dcast(mic_data, ID + FarmID + Flockseq ~ SamplingPoint, value.var="MIC")
mic_data = mic_data[, c("ID", "FarmID", "Flockseq", "Start", "Mid", "End")]

gcur = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/ColR_MIC.xlsx", sheet="OD"))

id_matrix = mic_data[,c("ID", "FarmID", "Flockseq")]
```

## Choose type of resistance data: growth curves or MIC

Here, we decide which resistance data we analyze: "growth_curves" or "mic". The final object for resistance data will be `res_data`, with values ranging between 0 (lowest observed resistance) and 1 (highest observed resistance).

```{r}
type_res_data = "growth_curves"
```

```{r}
if(type_res_data == "mic"){
  res_data = mic_data[,c("Start", "Mid", "End")]
  rownames(res_data) = mic_data$ID
  colnames(res_data) = c("S", "M", "E")
  res_data = (res_data - min(res_data, na.rm=T))/(max(res_data, na.rm=T) - min(res_data, na.rm=T))
}

```

## Resistance data: determine which colistin concentration to use

This part is computed only in the case we chose growth curves as a measure of resistance in samples. We use `gcur`.

Here, the indicator of colistin resistance is the area between the growth curve under 0 mg/L of colistin (reference) and the growth curve under a non-null concentration of colistin, that we have to determine.

We first remove concentrations 3 and 6, as too few OD were measured for these concentrations.

We plot the OD of all samples for each concentration (2 repetitions). We then plot, for each concentration and all samples (2 repetitions), OD[conc]-OD[conc=0], where OD[conc=0] is the reference OD at concentration 0.

```{r, fig.width=6, fig.height=4}
if(type_res_data == "growth_curves"){
  
  gcur = melt(gcur, id.vars = c("SampleID", "Conc.", "FlockID", "SamplPoint", "Repeat"), variable.name = "Time")
  gcur$Time = as.numeric(gcur$Time)
  
  # Unique ID for each observed curve:
  gcur$obsID = paste0(gcur$SampleID, gcur$Repeat)
  
  # Remove concentrations 3 and 6:
  gcur = gcur[which(!gcur$Conc. %in% c(3, 6)),]
  
  # If we do the diff with concentration 0:
  gcur = gcur[which(gcur$FlockID != "0464"),]
  gcur_before_transfo = gcur
  for (concentr in c(0.5, 1, 2, 4, 8, 12, 16)){
    # Check that the same samples are compared: this sum must be 0
    if(sum(rep(gcur$SampleID[gcur$Conc. == 0], each=2) != gcur$SampleID[gcur$Conc. == concentr]) != 0){print("CAUTION: Different samples are substracted"); break}
    
    gcur$value[gcur$Conc. == concentr] = gcur$value[gcur$Conc. == concentr] - rep(gcur$value[gcur$Conc. == 0], each=2)
    
  }
  gcur = gcur[which(gcur$Conc. != 0),]
  
  p1 = ggplot(data = gcur_before_transfo, aes(x=Time, group=obsID))
  p1 = p1 + ggtitle("Optical Density in all samples for each concentration (in mg/L)")
  p1 = p1 + xlab("Time (hours)") + ylab("Optical Density")
  p1 = p1 + geom_line(aes(y=value), col="grey")
  p1 = p1 + facet_wrap(~Conc.)
  p1 = p1 + theme_bw()
  plot(p1)
  
  p2 = ggplot(data = gcur, aes(x=Time, group=obsID))
  p2 = p2 + ggtitle("OD[conc] - OD[conc=0] for all samples")
  p2 = p2 + xlab("Time (hours)") + ylab("Optical Density difference")
  p2 = p2 + geom_line(aes(y=value), col="grey")
  p2 = p2 + facet_wrap(~Conc.)
  p2 = p2 + theme_bw()
  plot(p2)
  
  gcur = dcast(gcur, SampleID + Conc. + FlockID + SamplPoint + Time ~ Repeat, value.var="value")
  gcur$od = (gcur$`1` + gcur$`2`)/2
  gcur = gcur[,c("SampleID", "Conc.", "FlockID", "SamplPoint", "Time", "od")]
  
  gcur = gcur[! gcur$FlockID %in% c("0351", "0381", "0391", "0765", "0771"),]
  
  rm(p1, p2, concentr)
}
```

Our objective is to use a concentration that offers a distribution of the resistance indicator as spread out as possible. We see that, in concentrations 0 and 0.5, no curve is totally flat, i.e. all samples show some growth. On the contrary, in concentrations 8, 12 and 16, most of the samples show no growth. Therefore, we need to choose between concentrations 1, 2 and 4, that all present a spread range of curves, and thus a spread range of resistance indicator value.

For these concentrations (1, 2 and 4), we compute the area between the OD under this concentration and the OD under 0 mg/L of colistin (reference), for each sample.

To measure how much the distributions of this indicator (area) are spread out for each concentration, we use the standard deviation, that we want as large as possible. We also use the ["van der Eijk's A" statistic](https://en.wikipedia.org/wiki/Multimodal_distribution#van_der_Eijk's_A).  It ranges between -1 (perfect bimodality of the distribution) to +1 (perfect unimodality). A value of 0 corresponds to a uniform distribution. We select the concentration for which this statistic is closest to 0, because we aim to have both a spread distribution, and intermediary values of indicator. We use package `agrmt`.

Moreover, for each concentration, as the distribution of the indicator is bimodal, we use a GMM with 2 components to determine the threshold for which the probability to be in any of the 2 components is 0.5. This separates samples classified as resistant or sensitive.

```{r, label="Calculating van der Eijk's A", fig.width=11.5, fig.height=3.5}
if(type_res_data == "growth_curves"){
  
  VdE_A_score = function(conc, t_start, t_end){
    
    all_val = c()
    for(ts in unique(gcur$SampleID)){
      if(length(seq(t_start,t_end)) == length(gcur$od[which((gcur$Time %in% seq(t_start,t_end)) & (gcur$Conc. == conc) & (gcur$SampleID == ts))])){
        all_val = c(all_val, trapz(x = seq(t_start,t_end), y = gcur$od[which((gcur$Time %in% seq(t_start,t_end)) & (gcur$Conc. == conc) & (gcur$SampleID == ts))]))
      }else{
        print(paste0("Problematic number of repetitions for SampleID=", ts, " (concentration ", conc, ")."))
      }
    }
    
    list(agreement(table(factor(round(all_val), levels = (min(round(all_val)): max(round(all_val)))))), all_val, sd(all_val))
  }
  
  dnorm_lambda = function(x, par_mu, par_sigma, par_lambda){
    par_lambda * dnorm(x, par_mu, par_sigma)
  }
  
  distr_ind = function(c){
    VdE_A = VdE_A_score(conc=c, t_start=1, t_end=79)
    print(paste("van der Eijk's A for concentration", c, ":"))
    print(VdE_A[[1]])
    print(paste("Standard deviation for concentration", c, ":"))
    print(VdE_A[[3]])
    
    # Compute value of threshold
    vect_val = VdE_A[[2]]
    set.seed(78)
    gmm = normalmixEM(vect_val, k = 2)
    dif_distrib = function(x){
      log(dnorm_lambda(x, gmm$mu[1], gmm$sigma[1], gmm$lambda[1]) / dnorm_lambda(x, gmm$mu[2], gmm$sigma[2], gmm$lambda[2]))
    }
    thresh_res_this_conc = uniroot(f = dif_distrib, lower=-40, upper=-30)$root

    # Plot distributions of resistance indicator:
    p = ggplot(data = data.frame(x = VdE_A[[2]]), aes(x=x))
    p = p + geom_histogram(aes(y = ..density..), bins = 12, fill = "royalblue4", alpha = 0.5)
    p = p + stat_function(geom = "line", fun = dnorm_lambda,
                          args = list(par_mu=gmm$mu[1], par_sigma=gmm$sigma[1], par_lambda=gmm$lambda[1]),
                          colour = "#d95f02", lwd = 0.75)
    p = p + stat_function(geom = "line", fun = dnorm_lambda,
                          args = list(par_mu=gmm$mu[2], par_sigma=gmm$sigma[2], par_lambda=gmm$lambda[2]),
                          colour = "#1b9e77", lwd = 0.75)
    set.seed(78)
    p = p + geom_jitter(aes(y = 0.015, col = (x <= thresh_res_this_conc)), size = 1, width = 0, height = 0.015)
    p = p + geom_vline(xintercept = thresh_res_this_conc, col = "black", linetype = "dashed")
    p = p + geom_text(aes(x = (min(x) + thresh_res_this_conc)/2, y = 0.062, label = "Sensitive"), col = "#d95f02", size = 4)
    p = p + geom_text(aes(x = (max(x) + thresh_res_this_conc)/2, y = 0.062, label = "Resistant"), col = "#1b9e77", size = 4)
    p = p + scale_color_manual(name = "", values = c("#1b9e77", "#d95f02"), labels = c("Sensitive", "Resistant"))
    p = p + ggtitle(paste0(c, " mg/L"))
    p = p + xlab("Resistance metric") + ylab("Density")
    p = p + theme_bw()
    
    list(p, thresh_res_this_conc)

  }
  
  p1 = distr_ind(1)[[1]]
  p2 = distr_ind(2)[[1]]
  p3 = distr_ind(4)[[1]]
  plot(ggarrange(p1, p2, p3, ncol = 3, common.legend=T, legend="none"))
  
  thresh_res = distr_ind(conc_used)[[2]]
  
  rm(VdE_A_score, dnorm_lambda, distr_ind, p1, p2, p3)
}
```

We select the concentration for which the standard deviation is the highest, or for which the "van der Eijk's A" statistic is closest to 0, therefore concentration 1 in this case. We average the 2 repetitions of a same sample, and reshape the data to obtain `area_ob`.

We standardize the indicator values to range between 0 (lowest value) and 1 (highest value).

```{r}

if(type_res_data == "growth_curves"){
  
  gcur = gcur[which(gcur$Conc. == conc_used),]
  
  area_ob = as.data.frame(matrix(NA, nrow = length(unique(gcur$FlockID)), ncol = length(unique(gcur$SamplPoint))))
  dimnames(area_ob) = list(unique(gcur$FlockID), c("S", "M", "E"))
  for(flo in unique(gcur$FlockID)){
    for(sp in unique(gcur$SamplPoint)){
      if(length(1:79) == length(gcur$od[which((gcur$Time %in% (1:79)) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))])){
        
        # times_with_data = gcur$Time[which(is.finite(gcur$od) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))]
        
        area_ob[flo, sp] = trapz(x = (1:79), y = gcur$od[which((gcur$Time %in% (1:79)) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))])
      }else{
        print(paste0("Impossible to compute area under curve for flock ", flo, ", sampling point ", sp))
      }
    }
  }
  
  thresh_res = (thresh_res - min(area_ob, na.rm=T))/(max(area_ob, na.rm=T) - min(area_ob, na.rm=T))
  area_ob = (area_ob - min(area_ob, na.rm=T))/(max(area_ob, na.rm=T) - min(area_ob, na.rm=T))
  
  # area_ob = area_ob[which(! rownames(area_ob) %in% c("0351", "0381", "0391", "0765", "0771")),]
  # mic_data = mic_data[which(! mic_data$ID %in% c("0351", "0381", "0391", "0765")),]
  
  res_data = area_ob

  rm(flo, sp, area_ob)
}
```

No matters if the resistance metric we chose is "growth_curves" or "mic", the measure is between 0 (lowest resistance) and 1 (highest resistance). Moreover, if we chose to use a logistic model in the analysis, we turn the quantitative values of resistance to qualitative (Resistant=1 or Sensitive=0) using the threshold found above. Note that we removed incomplete observations (i.e. cycles for which at least one of the 3 samples is missing).

```{r}
if(type_mod == "logis"){
  res_data = as.data.frame(res_data > thresh_res)
}
```

We print the first rows of the resistance dataset `res_data`:

```{r}
print(head(res_data))
rm(type_res_data, mic_data)
```

## Antimicrobial use and sampling times

AMU is observed on a weekly basis. The molecules used are recorded either in a qualitative (use / no use) or quantitave (mg used /kg of chicken on farm) format. We create matrixes containing AMU for each week (columns) of the production cycle and for each cycle (rows):

* `col_expo`: qualitative (yes/no) colistin use
* `col_expo_quanti`: quantitative (mg/kg) colistin use
* `allab_expo`: qualitative (yes/no) use of any antibiotic (including colistin)
* `allab_expo_quanti`: quantitative (mg/kg) use of all antibiotics (including colistin). We sum the quantity used for each antibiotic. An other option would be to take the maximum quantity used.

We specify that we include all antibiotics in the `all_ab` category:

```{r}
all_ab = colnames(viparc_data_quali)[10:54]
```

We define `n_cyc`, number of cycles included in the analysis, and n_samp, number of sampling times (3 in the study):

```{r}
n_cyc = nrow(res_data)
n_samp = 3
```

We also define `weeks_samp`, matrix `n_cyc`*`n_samp`, that indicates the week of samplings for each cycle. `n_weeks` is the maximum number of production weeks among cycles included in the study. `init_flock_size` is the initial flock size for each cycle included (not used for now).

```{r}
col_expo = allab_expo = matrix(NA, n_cyc, max(viparc_data_quali$week))
col_expo_quanti = allab_expo_quanti = matrix(NA, n_cyc, max(viparc_data_quanti$week))
weeks_samp = matrix(NA, n_cyc, n_samp)
init_flock_size = rep(NA, n_cyc)
rownames(col_expo) = rownames(col_expo_quanti) = rownames(allab_expo) = rownames(allab_expo_quanti) = rownames(weeks_samp) = names(init_flock_size) = rownames(res_data)

for(i in 1:n_cyc){
  farm_i = id_matrix$FarmID[id_matrix$ID == rownames(res_data)[i]]
  flockseq_i = id_matrix$Flockseq[id_matrix$ID == rownames(res_data)[i]]
  
  if(rownames(res_data)[i] != paste0(substr(farm_i, nchar(farm_i)-2, nchar(farm_i)), substr(flockseq_i, nchar(flockseq_i), nchar(flockseq_i)))){
    print(paste("CAUTION: Wrong farm or flock ID is used for cycle number", i))
    break
  }
  
  if(!(any(viparc_data_quali$completed[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i))]) == T)){
    print(paste0("For farm ",farm_i,", flock sequence ",flockseq_i,", the production cycle was not completed"))
  }
  
  # Colistin use (qualitative and quantitative):
  
  weeks_col_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,"colistin_use"] == T))]
  
  weeks_col_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,"colistin_use"] == F))]
  
  col_expo[i, weeks_col_use] = 1
  col_expo[i, weeks_col_no_use] = col_expo_quanti[i, weeks_col_no_use] = 0
  
  # Quantitative colistin use
  for(wk in weeks_col_use){
    col_expo_quanti[i, wk] = 1000 * viparc_data_quanti[which((viparc_data_quanti$farm == farm_i)&(viparc_data_quanti$flock == flockseq_i)&(viparc_data_quanti$week == wk)), which(colnames(viparc_data_quali) == "colistin_use")]
  }
  
  # All antibiotics use (qualitative and quantitative):
  
  weeks_allab_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(rowSums(viparc_data_quali[,all_ab], na.rm=T) != 0))]
  
  weeks_allab_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(rowSums(viparc_data_quali[,all_ab], na.rm=T) == 0))]
    
  allab_expo[i, weeks_allab_use] = 1
  allab_expo[i, weeks_allab_no_use] = allab_expo_quanti[i, weeks_allab_no_use] = 0
  
  # Quantitative [all antibiotics] use
  for(wk in weeks_allab_use){
    allab_expo_quanti[i, wk] = 1000 * sum(viparc_data_quanti[which((viparc_data_quanti$farm == farm_i)&(viparc_data_quanti$flock == flockseq_i)&(viparc_data_quanti$week == wk)), which(colnames(viparc_data_quali) %in% all_ab)])
  }
  
  # Weeks of sampling:
  
  weeks_samp[i,] = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali$sampling == T))]
  
  # Initial flock size:
  
  init_flock_size[i] = viparc_data_quali$nb_chicken[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali$week == 1))]
}

weeks_to_keep = which((colSums(!is.na(col_expo)) != 0) & (colSums(!is.na(allab_expo)) != 0))

col_expo = col_expo[,weeks_to_keep]
col_expo_quanti = col_expo_quanti[,weeks_to_keep]
allab_expo = allab_expo[,weeks_to_keep]
allab_expo_quanti = allab_expo_quanti[,weeks_to_keep]

n_weeks = max(weeks_to_keep)

if(any(col_expo != (col_expo_quanti !=0), na.rm=T) | any(allab_expo != (allab_expo_quanti !=0), na.rm=T)){print("ERROR: Qualitative and quantitative AMU data are not consistent.")}

rm(i, wk, weeks_col_use, weeks_col_no_use, weeks_allab_use, weeks_allab_no_use, farm_i, flockseq_i, weeks_to_keep, all_ab, viparc_data_quali, viparc_data_quanti, id_matrix, n_samp, init_flock_size)

```

# Visualization

## Production cycles, AMU and sampling dates

We plot the AMU and sampling dates for the `n_cyc` cycles. We also plot an example of the growth curves available for each sample, and the area between two curves (indicator of resistance).

```{r, fig.width=8, fig.height=6, fig.retina=2}
plot(x=c(0,0.1), y=c(0,0), xlim=c(0,ncol(col_expo)+8), ylim=c(0,n_cyc+10), type="l", xlab="Weeks", ylab="", axes=F)
axis(side = 1)
title(ylab="Cycles", mgp=c(0,1,0))

for (i in 1:n_cyc){
  for (j in 1:max(which(! is.na(col_expo[i,])))){
    
    rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "bisque", border="grey")
    
    if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 1) & (allab_expo[i,j] == 1)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "coral3", border="grey")
    }else if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 0) & (allab_expo[i,j] == 1)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "deepskyblue4", border="grey")
    }

    if (any(weeks_samp[i,] == j, na.rm=T)){
      if (is.finite(res_data[i, which(weeks_samp[i,] == j)])){
        lines(x = c(j, j), y = c(i-0.5, i+0.5), col = "black", lwd = 2)
      }
    }
  }
}

legend(x=0, y=n_cyc+7, inset=0.5, legend=c("Antimicrobials used, including colistin", "Non-colistin antimicrobials used", "No antimicrobials used"), fill=c("coral3", "deepskyblue4", "bisque"), cex = 0.8, bty = "n")

    
# Only for figure:
for_fig_panel = gcur_before_transfo[(gcur_before_transfo$SampleID == "0625E" & gcur_before_transfo$Conc. %in% c(0,1)),]
for_fig_panel = dcast(for_fig_panel, SampleID + Conc. + Time ~ Repeat, value.var="value")
for_fig_panel$od = rowMeans(for_fig_panel[,c("1","2")], na.rm = T)
for_fig_panel = for_fig_panel[,c("SampleID", "Conc.", "Time", "od")]
for_fig_panel$lab = NA
for_fig_panel$lab[for_fig_panel$Conc. == 0] = "0 mg/L (reference)"
for_fig_panel$lab[for_fig_panel$Conc. == 1] = "1 mg/L"

# 18th line starting from above
p2 = ggplot(data = for_fig_panel, aes(x=Time, y=od, group=Conc., label=lab))
p2 = p2 + xlab("Time (hours)") + ylab("OD")
p2 = p2 + geom_ribbon(data = for_fig_panel[for_fig_panel$Conc. == 1,], aes(x = Time, ymin = od, ymax=for_fig_panel$od[for_fig_panel$Conc. == 0]), alpha=0.2, fill="chartreuse4")
p2 = p2 + geom_line(col="black", size=1)
p2 = p2 + geom_label_repel(data = for_fig_panel[for_fig_panel$Time == 70,], xlim = c(10, 11), ylim = c(0.65, 1.4), direction = "y", hjust = 0)
p2 = p2 + theme_classic() + theme(legend.position="none",
                             plot.background=element_rect(colour ="black"),
                             plot.margin = unit(c(0.2, 0.2, 0.1, 0.1), "cm"))

print(p2, vp=viewport(width = 0.3, height = 0.3, x = 0.98, y = 0.38, just = c("right", "bottom")))

lines(x=c(weeks_samp["0625",3], 23), y=c(16, 16.5), lwd=1)

rm(i, j, p2, gcur, for_fig_panel)
```

## Resistance and AMU over time

We plot the quantitative observed resistance (red dots) for each cycle.

AMU (only colistin and all antibotics) is represented as colored bars. We can plot the qualitative use (yes/no each week) (`amu_quanti=F`), or the quantitative use (amount per kg of chicken each week) (`amu_quanti=T`). In the latter case, we fix a ceiling, i.e. a maximum of the quantity plotted: this is to avoid one large AMU measure preventing to observe lower quantities on the graph. This ceiling is determined by `ratio_resmet_quanti_amu` which is the ratio between the maximum value of the resistance metric, and the maximum value of the AMU **on the plot** (no consequence on the values in the analysis).

```{r}
amu_quanti = F
ratio_resmet_quanti_amu = 50

```

```{r, warning=F, fig.width=8, fig.height=6}
obs_plot = rbind(data.frame(cyc=rep(NA, n_cyc*n_weeks), week=rep(NA, n_cyc*n_weeks), obser=rep(NA, n_cyc*n_weeks), exp_ab=rep(NA, n_cyc*n_weeks), type_ab=rep("Colistin", n_cyc*n_weeks)),
                 data.frame(cyc=rep(NA, n_cyc*n_weeks), week=rep(NA, n_cyc*n_weeks), obser=rep(NA, n_cyc*n_weeks), exp_ab=rep(NA, n_cyc*n_weeks), type_ab=rep("All antibiotics", n_cyc*n_weeks)))

obs_plot$cyc = as.vector(matrix(rep(rownames(res_data),n_weeks), n_weeks, n_cyc, byrow=T))
obs_plot$week = rep(seq(1,n_weeks),n_cyc)

for (f in rownames(res_data)){
  n_weeks_cyc = sum(!(is.na(col_expo[f,])))
  samp_cyc = which(!(is.na(res_data[f,])))
  
  if(length(samp_cyc) < 2){print(paste0("CAUTION: For cycle ", f,", the number of samples collected is strictly less than 2"))}

  obs_plot$obser[which((obs_plot$cyc==f) & (obs_plot$week %in% weeks_samp[f,samp_cyc]))] = as.numeric(res_data[f,samp_cyc])
  
  if(amu_quanti){
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "Colistin"))] = col_expo_quanti[f,]
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "All antibiotics"))] = allab_expo_quanti[f,]

  }else{
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "Colistin"))] = col_expo[f,]
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "All antibiotics"))] = allab_expo[f,]
  }
}

if(amu_quanti){
  obs_plot$exp_ab[which(obs_plot$exp_ab > (max(res_data, na.rm=T) * ratio_resmet_quanti_amu))] = max(res_data, na.rm=T) * ratio_resmet_quanti_amu
}

obs_plot$cyc = factor(obs_plot$cyc, levels=rownames(res_data))

# Plot:

p = ggplot(data=obs_plot[which(obs_plot$cyc %in% rownames(res_data)),], aes(x=week))
p = p + xlab("Time (weeks)")
p = p + ylab("Resistance metric")

if(amu_quanti){
  
  p = p + ggtitle("Quantitative AMU and colistin resistance in each cycle")
  p = p + geom_bar(aes(y=exp_ab/ratio_resmet_quanti_amu, fill=type_ab), stat="identity", position="dodge2", col=NA)
  
  p = p + scale_y_continuous(sec.axis = sec_axis(~.*ratio_resmet_quanti_amu, name = paste0("AMU (mg/kg) (ceiling ", round(max(res_data, na.rm=T) * ratio_resmet_quanti_amu), " mg/kg for each type)")))
  p = p + theme(axis.text.y.right = element_text(color="blue"), axis.title.y.right = element_text(color="blue"))

}else{
  
  p = p + ggtitle("Qualitative AMU and colistin resistance in each cycle")
  p = p + geom_bar(aes(y=exp_ab/2, fill=type_ab), stat="identity", position="stack", col=NA)
}

p = p +  scale_fill_discrete(name = "Antibiotics used:")
p = p + geom_point(aes(y = obser), size = 3, shape = 21,  fill = "red", color = "black")
p = p + facet_wrap(~ cyc)
p

rm(f, n_weeks_cyc, samp_cyc, far, iter_list, p, pl, ratio_resmet_quanti_amu, obs_plot, amu_quanti, n_weeks, gcur_before_transfo)
```

## Resistance plotted versus different AMU metrics in Shiny

This function is useful to:

* Show an interactive Shiny plot (`obj="plot"`) (not shown in the regular html)
* The further estimation of our model (`obj="est"`), as it returns the observed resistance and AMU, depending on the value of some parameters

```{r}

resamu = function(nwkseff, quantuse, tempo_amu, ignore.nul=F, lag_amu_init, obj="plot", expo, var_to_plot = "amu_t"){
  obtab = data.frame(cycle=NA, amu_init=NA, amu_t=NA, resquanti=NA, age=NA, init=NA, prev=NA, end=NA)
  for(cyc_i in 1:n_cyc){
    for(time_i in 2:3){
      
      first_amu_week_consid = as.numeric(weeks_samp[cyc_i, time_i] - nwkseff - 1)
      last_amu_week_consid = as.numeric(weeks_samp[cyc_i, time_i] - 1)
      
      if(first_amu_week_consid <= 0){
        save(first_amu_week_consid, file = paste0("C:/Users/Jonathan/Desktop/CAUTION - In function resamu first_amu_week_consid=", first_amu_week_consid, ".txt"))
      }
      
      if(expo == "allab"){
        use_quali = allab_expo
        use_quanti = allab_expo_quanti
      }else if(expo == "col"){
        use_quali = col_expo
        use_quanti = col_expo_quanti
      }
      
      if(tempo_amu %in% c("step", "linear")){
        inc_wks = ceil(first_amu_week_consid):last_amu_week_consid
      }else if(tempo_amu == "exp_decay"){
        inc_wks = 1:last_amu_week_consid
      }
      
      amu_t = rep(0, last_amu_week_consid)
      if(quantuse){
        amu_t[inc_wks] = use_quanti[cyc_i, inc_wks]
      }else{
        amu_t[inc_wks] = use_quali[cyc_i, inc_wks]
      }

      if(tempo_amu == "linear"){
        time_eff = seq(0, 1, length.out = length(inc_wks)+1)[-1]
        amu_t[inc_wks] = amu_t[inc_wks] * time_eff
      }else if(tempo_amu == "exp_decay"){
        time_eff = (exp(log(0.01)/(nwkseff+1)))^(rev(inc_wks)-1)
        amu_t = amu_t * time_eff
      }
      
      amu_t[inc_wks[1]] = amu_t[inc_wks[1]] * (inc_wks[1] - first_amu_week_consid)
      
      amu_t = sum(amu_t)
      
      # if(lag_amu_init > last_amu_week_consid){
      #  lag_amu_init = last_amu_week_consid
      # }
      
      if(quantuse){
        amu_init = c(use_quanti[cyc_i, (1:floor(lag_amu_init))], as.numeric((lag_amu_init - floor(lag_amu_init)) * use_quanti[cyc_i, ceiling(lag_amu_init)]))
      }else{
        amu_init = c(use_quali[cyc_i, (1:floor(lag_amu_init))], as.numeric((lag_amu_init - floor(lag_amu_init)) * use_quali[cyc_i, ceiling(lag_amu_init)]))
      }
      
      amu_init = sum(amu_init)
      
      obtab = rbind(obtab, c(cyc_i, amu_init, amu_t, res_data[cyc_i, time_i], weeks_samp[cyc_i, time_i], res_data[cyc_i, 1], res_data[cyc_i, 2], time_i==3))
    }
  }
  obtab = obtab[-1,]
  
  if(ignore.nul){
    obtab = obtab[which(obtab[,var_to_plot] != 0),]
  }
  obtab$cycle = as.factor(obtab$cycle)
  
  obtab$resquali = (obtab$resquanti >= 0.5)

  if(obj == "plot"){
    p = ggplot(data=obtab, aes(x=obtab[,var_to_plot], y=resquanti, col=init))
    p = p + ylab("Measure of resistance")
    p = p + xlab(var_to_plot)
    p = p + ggtitle(paste0("Pearson correlation between recent AMU and the measure of resistance: ", round(cor(obtab$amu_t, obtab$resquanti, use="complete.obs"), 3)))
    set.seed(78)
    p = p + geom_jitter(height = 0.25, width = 0)
    p = p + geom_hline(yintercept = 0.5, linetype="dashed", col="grey")
    p = p + scale_color_gradient(low = "red", high = "blue")
    # p = ggplotly(p)
    return(p)
  }else if(obj == "est"){
    return(obtab)
  }
}

```

Here, either we run an interactive Shiny app, or we plot a simple graph showing on the y-axis the measured resistance, and on the x-axis the AMU. The color is the value of resistance in the initial sample of the production cycle (week 1). This is to observe the graphical association between resistance and AMU, and the group of observations for which resistance is high but AMU is low. For the last ones, we want to test if this high resistance could be explained by events of contamination.

```{r, fig.width=7, fig.height=5}
ui <- fluidPage({
  titlePanel("Correlation between past use and the measure of resistance")
  sidebarLayout(
    sidebarPanel(
      selectInput("var_to_plot",
                  "Category of AMU to take into account:",
                  c("amu_t", "amu_init")),
      sliderInput("nwkseff",
                  paste0("Number of weeks of past use to take into account on top of the last one (beta):"),
                  min=0,
                  max=4,
                  value=1),
      sliderInput("lag_amu_init",
                  "Number of first weeks of the cycle with permanent effect of use (delta):",
                  min=1,
                  max=5,
                  value=3),
      selectInput("tempo_amu",
                  "Temporal effect of AMU:",
                  c("exp_decay", "linear", "step")),
      selectInput("expo",
                  "AMU is all antibiotics (allab) or only colistin (col):",
                  c("allab", "col")),
      selectInput("quantuse",
                  "Select if the quantitative AMU should be used:",
                  c(T, F)),
      selectInput("ignore.nul",
                  "Select if the nul values of use should be ignored:",
                  c(T, F))
    ),
    
    mainPanel(
      plotOutput("plotdisp")
    )
  )
})

server <- function(input, output){
  # define the output, the output id must be the same with the ID in the UI object
  # because the output change as the input change - it's dynamic so we need a special function renderPlot() to make the output react with our choice
  
  output$plotdisp <- renderPlot({
    # filter the data by the input and save into new object
    # the input id must be the same as the UI object
    # plot
    resamu(nwkseff=input$nwkseff, lag_amu_init=input$lag_amu_init, expo=input$expo, quantuse=input$quantuse, tempo_amu=input$tempo_amu, ignore.nul=input$ignore.nul, var_to_plot=input$var_to_plot)
  })
}

if(run_shiny){
  shinyApp(ui=ui, server=server)
}else{
  resamu(nwkseff = 1.00001, quantuse = F, tempo_amu="exp_decay", lag_amu_init = 1, obj="plot", expo="col")
}
rm(ui, server, run_shiny)
```

# Analysis

## Model and Expectation-Maximization algorithm

Some events of contamination with colistin resistant bacteria may be not measured but can potentially play an important role in the resistance of the sample. We consider a latent (unobserved) binary variable to describe if a contamination event happened in the farm (yes/no).

We use an Expectation-Maximization (E-M) algorithm with 2 steps:

* Expectation: given a set of parameters, determine the probability of occurrence of a contamination event for each observation.
* Maximization: given this contamination probability, determine the set of parameters maximizing the likelihood of the model.

The algorithm is initiated with a given probability of contamination, and is terminated when a convergence of likelihood is reached.

For all observations in the 2nd or 3rd round of samplings, we define a model used in the Maximization step. Several models are tested, all being simplified versions of this full model:

$$R_i(t) = \sum_{k \in \{ t_1 ; t_2 \} }(\lambda_k.R_i(k)) + \eta.f_i(t) + \alpha.A_{recent,i}(t) + \theta.A_{init,i}(t) + \mu$$
where:

* $R_i(t)$ is the measure of resistance in observation (sample) i
* $t$ is the week the sample was collected
* $R_i(t_1)$ is the measure of resistance in the initial sample of the same flock
* $R_i(t_2)$ is the measure of resistance in the previous sample if the observation belongs to the 3rd round of sampling (autocorrelation)
* $f_i(t)=1$ if an introduction occurred before the sampling (between $t-\xi$ and $t-1$, where $\xi$ is fixed at 4 weeks), and $f(t)=0$ otherwise
* $\mu$ is the average resistance when all other variables are null.

The AMU metric $A_{recent,i}(t)$ is the use that just precedes the sampling date, and is defined as:

$$A_{recent,i}(t)=\sum_{j=1}^{t-1}(U(j).exp[\frac{ln(\epsilon)}{\beta}.(t-j-1)])$$

where $U(j)$ is the (quantitative or qualitative) antimicrobial use in the flock on week j. For now, in the following, we consider only qualitative use (= 0 or 1).

As shown in the figure below, $A_{recent,i}(t)$ is parametrized as an exponential decay: after β+1 weeks, the effect of AMU is $\epsilon$ (<< 1) times its effect after one week. We fix $\epsilon$ at 1%.

$A_{init,i}(t)$ is the use that occurs at the begining of the cycle (first $\delta$ weeks). It is uncertain if it has effect on the resistance during the whole production cycle. We test this hypothesis. It is defined as:

$$A_{init,i}(t)=\sum_{j=1}^{\delta}U(j)$$

```{r echo=FALSE, fig.width=5.5, fig.height=5.5}
metramu = data.frame(x = seq(0,10,0.01), exp = 0.95*exp(-log(20)/(10-6))^seq(10,0,-0.01), step = c(rep(0.3, 451), rep(0, 550)))
p = ggplot(data=metramu, aes(x=x))
p = p + xlab("Time since beginning of production cycle") + ylab("Effect of antimicrobial use")
p = p + geom_ribbon(aes(ymin=0, ymax=exp), fill="dodgerblue4", alpha=0.2)
p = p + geom_ribbon(aes(ymin=0, ymax=step), fill="brown", alpha=0.2)
p = p + geom_line(aes(y=exp, col="1"), size=1)
p = p + geom_step(aes(y=step, col="2"), size=1)
p = p + geom_line(aes(y=0.05), linetype="dashed")
p = p + geom_line(aes(y=0.95), linetype="dashed")
p = p + geom_point(aes(x=6.1, y=0.05), size=4, col="dodgerblue4")
p = p + geom_point(aes(x=10, y=0.95), size=4, col="dodgerblue4")
p = p + geom_text(aes(x=4.5, y=-0.03, label = "delta"), parse = T, size=5, col="brown")
p = p + geom_text(aes(x=-0.2, y=0.3, label = "theta"), parse = T, size=5, col="black")
p = p + geom_text(aes(x=6.1, y=0.12, label = as.character(expression(paste("t"["ij"]," -",~beta," -1")))), parse = T, size=5, col="dodgerblue4")
p = p + geom_text(aes(x=10, y=1, label = as.character(expression(paste("t"["ij"]," -1")))), parse = T, size=5, col="dodgerblue4")
p = p + geom_text(aes(x=-0.4, y=0.05, label = as.character(expression(~epsilon~alpha))), parse = T, size=5, col="black")
p = p + geom_text(aes(x=-0.2, y=-0.03, label = "0"), parse = T, size=5, col="black")
p = p + geom_text(aes(x=-0.2, y=0.95, label = "alpha"), parse = T, size=5, col="black")
p = p + scale_color_manual(name="", values=c("1"="dodgerblue4", "2"="brown"), labels=c("1"=expression(paste("L"["ij"]," (recent use)")), "2"=expression(paste("S"["i"]," (initial use)"))))
p = p + theme_bw()
p = p + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = "top", legend.direction = "vertical")
p

rm(metramu, p)
```

If `definition_amu = "col"`, the use is defined as the qualitative colistin use. If `definition_amu = "allab"`, the use is defined as the qualitative use of any antibiotic.

## Variations of the model

The full model described above is model 24. Models 1 to 23 are simplifications of model 24.

|Models|Previous resistance   |AMU             |Introductions|
|:----:|:--------------------:|:--------------:|:-----------:|
|**1** |None                  |None            |None         |
|**2** |Initial               |None            |None         |
|**3** |Previous              |None            |None         |
|**4** |None                  |Initial         |None         |
|**5** |Initial               |Initial         |None         |
|**6** |Previous              |Initial         |None         |
|**7** |None                  |Recent          |None         |
|**8** |Initial               |Recent          |None         |
|**9** |Previous              |Recent          |None         |
|**10**|None                  |Initial + Recent|None         |
|**11**|Initial               |Initial + Recent|None         |
|**12**|Previous              |Initial + Recent|None         |
|**13**|None                  |None            |Possible     |
|**14**|Initial               |None            |Possible     |
|**15**|Previous              |None            |Possible     |
|**16**|None                  |Initial         |Possible     |
|**17**|Initial               |Initial         |Possible     |
|**18**|Previous              |Initial         |Possible     |
|**19**|None                  |Recent          |Possible     |
|**20**|Initial               |Recent          |Possible     |
|**21**|Previous              |Recent          |Possible     |
|**22**|None                  |Initial + Recent|Possible     |
|**23**|Initial               |Initial + Recent|Possible     |
|**24**|Previous              |Initial + Recent|Possible     |


## Useful functions

### Function returning the minus loglikelihood

This function enters the raw (transformed) values of the model's parameters and the probability of introduction for each observation, and returns either the minus loglikelihood (`aim="est"`), or the predicted and observed values of resistance and AMU, the minus loglikelihood and the true (not transformed) values of parameters (`aim="pred"`).

Observations for which the resistance measure (model outcome) is absent are deleted. However, if an observation has NA values for measure of resistance in previous samples (`obsdat$init` and `obsdat$prev`), the value is set at the mean of the other values.

We also define `run2_intr` which is a wraper: its output is the function `run2` with the value of `intr` (input of `run2`) specified.

```{r}
run2 = function(aim="est", intr=introd, mu, lambda1, lambda2, alpha, beta, teta, delta, eta){

  beta = 5 * sigmoid(beta)
  delta = 5 * sigmoid(delta)
  eta = almost_one

  obsdat = resamu(nwkseff=beta, quantuse=amu_model_is_quanti, tempo_amu=effect_recent_amu, ignore.nul=F, lag_amu_init=delta, obj="est", expo=definition_amu)
  # obsdat = obsdat[! is.na(obsdat$resquanti),]
  # obsdat$init[is.na(obsdat$init)] = mean(obsdat$init, na.rm=T) # 0
  # obsdat$prev[is.na(obsdat$prev)] = mean(obsdat$prev, na.rm=T) # 0
  obsdat$introduc = intr

  obsdat$glm_predmod_intro = eta
  # obsdat$glm_predmod_intro = pmax(pmin(obsdat$glm_predmod_intro, almost_one), 1-almost_one)
  
  obsdat$glm_predmod_nointro = mu + lambda1 * obsdat$init + (lambda2 * obsdat$prev)*as.numeric(obsdat$end) + alpha * obsdat$amu_t + teta * obsdat$amu_init
  obsdat$glm_predmod_nointro = pracma::sigmoid(obsdat$glm_predmod_nointro)
  # obsdat$glm_predmod_nointro = pmax(pmin(obsdat$glm_predmod_nointro, almost_one), 1-almost_one)
  
  obsdat$predmod = obsdat$introduc * obsdat$glm_predmod_intro + (1-obsdat$introduc) * obsdat$glm_predmod_nointro
  
  total_loglik = (1-obsdat$introduc) * log(obsdat$glm_predmod_nointro ^ obsdat$resquali * ((1-obsdat$glm_predmod_nointro) ^ (1-obsdat$resquali))) + obsdat$introduc * log(obsdat$glm_predmod_intro ^ obsdat$resquali * ((1-obsdat$glm_predmod_intro) ^ (1-obsdat$resquali)))
  
total_loglik[is.infinite(total_loglik)] = -710

  if(aim == "est"){
    
    return(-sum(total_loglik))
    
  }else if (aim == "pred"){

    return(list(obsdat = obsdat[,c("resquali", "amu_init", "amu_t", "introduc", "predmod", "cycle", "resquanti", "init", "prev")], minll = -sum(total_loglik), true_param = c(mu, lambda1, lambda2, alpha, beta, teta, delta, eta=eta)))
  }
}

# Factory/Wrapper function (returns a function with one of the parameters ("intr") fixed)
run2_intr <- function(x) {
 function(aim="est", mu, lambda1, lambda2, alpha, beta, teta, delta, eta)
   run2(aim, intr=x, mu, lambda1, lambda2, alpha, beta, teta, delta, eta)
}

```

### Function plotting a model's fit

This function takes as input the fitted model, and returns a plot with one dot per observation: the observed resistance on x-axis, the predicted resistance on y-axis.

```{r}
fitplot = function(fitted_model, intro, var_to_plot = "amu_t"){
  fit_coef = coef(fitted_model)
  
  obs_vs_pred = run2(aim="pred", intr=intro, mu=fit_coef["mu"], lambda1=fit_coef["lambda1"], lambda2=fit_coef["lambda2"], alpha=fit_coef["alpha"], beta=fit_coef["beta"], teta=fit_coef["teta"], delta=fit_coef["delta"], eta=fit_coef["eta"])
  
  minll = obs_vs_pred[[2]]
  print(paste("Minus LogLikelihood:", minll))
  
  true_fit_coef = obs_vs_pred[[3]]
  print("True values of parameters:")
  print(true_fit_coef)
  
  p = ggplot(obs_vs_pred[[1]], aes(x = resquanti, y = predmod, fill = true_fit_coef["alpha"]*amu_t + true_fit_coef["teta"]*amu_init, col = introduc))
  p = p + geom_vline(xintercept = 0.5, linetype = "dashed")
  p = p + geom_hline(yintercept = 0.5, linetype = "dashed")
  p = p + xlab("Observed resistance") + ylab("Predicted resistance")
  p = p + ggtitle(paste0("Observed VS Predicted resistance (minus LogLikelihood =", round(minll, 2), ")"))
  p = p + geom_abline(intercept = 0, slope = 1, linetype = "dashed")
  set.seed(78)
  p = p + geom_jitter(shape = 21, size = 3)
  p = p + labs(col = "Probability of introduction (outline):", fill = "Global AMU metric (interior color):")
  p = p + scale_colour_gradient(low = "green", high = "red")
  p = p + scale_fill_gradient(low = "white", high = "blue")
  plot(p)
  
  if(type_mod == "logis"){
    for_curve = data.frame(amu = seq(0, max(obs_vs_pred[[1]]$amu_t), 0.01), amr = sigmoid(true_fit_coef["mu"] + true_fit_coef["alpha"] * seq(0, max(obs_vs_pred[[1]]$amu_t), 0.01)))
    
    p2 = ggplot()
    p2 = p2 + xlab("Recent AMU") + ylab("Observed resistance (quanti)")
    p2 = p2 + ggtitle("Resistance = f(recent amu)")
    p2 = p2 + geom_line(data=for_curve, aes(x = amu, y = true_fit_coef["eta"]), col = "red")
    p2 = p2 + geom_line(data=for_curve, aes(x = amu, y = amr), col = "green")
    set.seed(78)
    p2 = p2 + geom_jitter(data = obs_vs_pred[[1]], aes(x = amu_t, y = resquanti, col = introduc, fill = predmod, shape = (predmod>=0.5)), size = 3, height = 0.1, width = 0)
    p2 = p2 + scale_color_gradient(name = "Probability of introduction", low = "green", high = "red")
    p2 = p2 + scale_fill_gradient(name = "Predicted resistance", low = "white", high = "blue")
    p2 = p2 + scale_shape_manual(name = "Predicted resistance (quali)", values = c(21, 22))
    plot(p2)

    print(paste0("Sensitivity: ", sum((obs_vs_pred[[1]]$predmod >= 0.5) & (obs_vs_pred[[1]]$resquali))/sum(obs_vs_pred[[1]]$resquali)))
    print(paste0("Specificity: ", sum((obs_vs_pred[[1]]$predmod >= 0.5) & (obs_vs_pred[[1]]$resquali))/sum(obs_vs_pred[[1]]$predmod >= 0.5)))
    
    as.numeric(obs_vs_pred[[1]]$resquali)
  }
}

fitplot_for_EMalgo = function(fitted_model, intro){
  fit_coef = coef(fitted_model)
  
  obs_vs_pred = run2(aim="pred", intr=intro, mu=fit_coef["mu"], lambda1=fit_coef["lambda1"], lambda2=fit_coef["lambda2"], alpha=fit_coef["alpha"], beta=fit_coef["beta"], teta=fit_coef["teta"], delta=fit_coef["delta"], eta=fit_coef["eta"])
  
  minll = obs_vs_pred[[2]]

  true_fit_coef = obs_vs_pred[[3]]
  
  if(type_mod == "logis"){
    for_curve = data.frame(amu = seq(0, max(obs_vs_pred[[1]]$amu_t), 0.01), amr = sigmoid(true_fit_coef["mu"] + true_fit_coef["alpha"] * seq(0, max(obs_vs_pred[[1]]$amu_t), 0.01)))
    
    p2 = ggplot()
    p2 = p2 + xlab("Recent AMU") + ylab("Observed resistance (quanti)")
    p2 = p2 + ggtitle("Resistance = f(recent amu)")
    p2 = p2 + geom_line(data=for_curve, aes(x = amu, y = true_fit_coef["eta"]), col = "red")
    p2 = p2 + geom_line(data=for_curve, aes(x = amu, y = amr), col = "green")
    set.seed(78)
    p2 = p2 + geom_jitter(data = obs_vs_pred[[1]], aes(x = amu_t, y = resquanti, col = introduc, fill = predmod, shape = (predmod>=0.5)), size = 3, height = 0.1, width = 0)
    p2 = p2 + scale_color_gradient(name = "Probability of introduction", low = "green", high = "red")
    p2 = p2 + scale_fill_gradient(name = "Predicted resistance", low = "white", high = "blue")
    p2 = p2 + scale_shape_manual(name = "Predicted resistance (quali)", values = c(21, 22))
    plot(p2)
  }
}

```

### Function repeating the fit several times

This function repeats the model's fit process several times, to make sure the maximum of likelihood is not only local. Among repetitions, the model with the highest likelihood is the one chosen by the function.

The function takes as inputs the model's parameters that should be fixed (not estimated), and those that should be estimated. This is to adapt to the different variations of the model (see above). Some parameters are transformed to be estimated in an interval.

It is made such that the random initialization of parameters is different from one repetition to another.

```{r}
rep_est = function(nrep, minusloglrep, methodrep, controlrep, prev_est, fixedvar, startvar){
  modrep = list()
  if(prev_est[1] == "none"){nrep = 20}
  
  for(repet in 1:nrep){
    cat("\r", paste0("Fit repetition ", repet, "/", nrep))
    
    valpar = list(mu=NULL, lambda1=NULL, lambda2=NULL, alpha=NULL, beta=NULL, teta=NULL, delta=NULL, eta=NULL)
    valpar[fixedvar] = 0
    
    if(prev_est[1] == "none"){
      valpar[startvar] = runif(length(startvar), min = -5, max = 5)
      valpar[c("beta", "delta")] = logit(runif(2))
    }else{
      valpar[startvar] = prev_est[startvar]
    }
    # valpar["mu"] = -10

    modrep[[repet]] = mle2(minuslogl=minusloglrep, method=methodrep, control=controlrep, fixed=valpar[fixedvar], start=valpar[startvar])
  }
  loglikrep = lapply(X=modrep, FUN=logLik)
  
  cat("\n")
  
  modrep[[which.max(loglikrep)]]
}

```

### Implementation of the full E-M algorithm

$$Q(\theta,\theta^{(s)}) = E[ln \, P(Y,Z|\theta) \quad |Y,\theta^{(s)}]$$

$$\begin{align*}
Q(\theta,\theta^{(s)})  &= \sum_{m} E[ln \, P(Y_m,Z_m|\theta) \quad |Y_m,\theta^{(s)}] \\
                    &= \sum_{m} [P(Z_m=0|Y_m,\theta^{(s)}).ln \, P(Z_m=0,Y_m|\theta) \: + \: P(Z_m=1|Y_m,\theta^{(s)}).ln \, P(Z_m=1,Y_m|\theta)]
\end{align*}$$

And:

$$\theta^{(s+1)} = \underset{\theta}{argmax} (Q(\theta,\theta^{(s)}))$$


Therefore:
$$\begin{align*}
P(Z_m=1|Y_m,\theta^{(s+1)}) &= \frac{P(Z_m=1,Y_m|\theta^{(s+1)})}{P(Y_m|\theta^{(s+1)})}\\
                      &= \frac{P(Z_m=1,Y_m|\theta^{(s+1)})}{P(Z_m=0,Y_m|\theta^{(s+1)}) \: + \: P(Z_m=1,Y_m|\theta^{(s+1)})}\\
                      &= \frac{\eta^{R_m}.(1-\eta)^{1-R_m}}{M_{m,s+1}^{R_m}.(1-M_{m,s+1})^{1-R_m} \: + \: \eta^{R_m}.(1-\eta)^{1-R_m}}
\end{align*}$$ 
      
And:
                
$$P(Z_m=0|Y_m,\theta^{(s+1)}) = 1-P(Z_m=1|Y_m,\theta^{(s+1)})$$

And:

$$\begin{align*}
  Q(\theta,\theta^{(s)}) &= \sum_{m} [P(Z_m=0|Y_m,\theta^{(s)}).ln \, P(Z_m=0,Y_m|\theta) \: + \: P(Z_m=1|Y_m,\theta^{(s)}).ln \, P(Z_m=1,Y_m|\theta)]\\
                     &= \sum_{m} [P(Z_m=0|Y_m,\theta^{(s)}).ln \, [P(Z_m=0|Y_m,\theta).P(Y_m|\theta)] \: + \: P(Z_m=1|Y_m,\theta^{(s)}).ln \, [P(Z_m=1|Y_m,\theta).P(Y_m|\theta)]]\\
                     &= \sum_{m} [[ln \, P(Y_m|\theta)].[P(Z_m=0|Y_m,\theta^{(s)})+P(Z_m=1|Y_m,\theta^{(s)})]\\
                     &\: + \: P(Z_m=0|Y_m,\theta^{(s)}).ln \, P(Z_m=0|Y_m,\theta) \: + \: P(Z_m=1|Y_m,\theta^{(s)}).ln \, P(Z_m=1|Y_m,\theta)]\\
                     &= \sum_{m} [ln \, P(Y_m|\theta)] \: + \: H(\theta,\theta^{(s)})\\
                     &= L_{obs}(\theta) \: + \: H(\theta,\theta^{(s)})\\
\end{align*}$$

And :

$$H(\theta,\theta^{(s)})= \sum_{m} [P(Z_m=0|Y_m,\theta^{(s)}).ln \, P(Z_m=0|Y_m,\theta) \: + \: P(Z_m=1|Y_m,\theta^{(s)}).ln \, P(Z_m=1|Y_m,\theta)]$$

And:

$$Q(\theta^*,\theta^*)= L_{obs}(\theta^*) \: + \: H(\theta^*,\theta^*)$$

$$IC_{H,Q}=-2Q(\theta^*,\theta^*)+2H(\theta^*,\theta^*)+C^*(\theta^*)$$

$$H(\theta^*,\theta^*)= \sum_{m} [P^{cont}_{m}.ln(P^{cont}_{m}) + (1-P^{cont}_{m}).ln(1-P^{cont}_{m})]$$

#### Function for the E step

With one given set of parameters for the model, we determine what is the most likely probability for the occurrence of a contamination event in the flock, for each sample.

We first calculate the residuals $r_{intro}^i$ (`r_intro`) and $r_{nointro}^i$ (`r_nointro`) of observations $R_i$ (observed resistance) under the 2 models: $M_{intro}$ ("Only introductions") and $M_{nointro}$ ("No introduction").

The idea is to compute, for each observation, the probability that it is assigned to each of the 2 models:

$$p(R_i \in M_{intro}) = \frac{p(r_{intro}^i|R_i \in M_{intro})}{p(r_{intro}^i|R_i \in M_{intro}) + p(r_{nointro}^i|R_i \in M_{nointro})}$$

And:

$$p(R_i \in M_{nointro}) = 1-p(R_i \in M_{intro})$$

As the residuals follow a normal distribution (with mean=0 and standard deviation $\sigma$ (`sigma`), estimated in the M-step):

$$p(R_i \in M_{intro}) = \frac{exp(\frac{-(r_{intro}^i)^2}{2.\sigma^2})}{exp(\frac{-(r_{intro}^i)^2}{2.\sigma^2}) + exp(\frac{-(r_{nointro}^i)^2}{2.\sigma^2})}$$

(Sources: http://www.cs.huji.ac.il/~yweiss/emTutorial.pdf ; http://www.di.fc.ul.pt/~jpn/r/EM/EM.html#eg-em-with-mix-of-two-linear-models)

```{r}

E_step <- function(observed, params, prev_p_intro) {
  
  pred_intro = run2(aim="pred", intr=1, mu=params[["mu"]], lambda1=params[["lambda1"]], lambda2=params[["lambda2"]], alpha=params[["alpha"]], beta=params[["beta"]], teta=params[["teta"]], delta=params[["delta"]], eta=params[["eta"]])[["obsdat"]]$predmod

  pred_nointro = run2(aim="pred", intr=0, mu=params[["mu"]], lambda1=params[["lambda1"]], lambda2=params[["lambda2"]], alpha=params[["alpha"]], beta=params[["beta"]], teta=params[["teta"]], delta=params[["delta"]], eta=params[["eta"]])[["obsdat"]]$predmod
  
  # Probability of "Intro":
  pred_intro = pmax(pmin(pred_intro, almost_one), 1-almost_one)
  pred_nointro = pmax(pmin(pred_nointro, almost_one), 1-almost_one)
  prob_intro = ((pred_intro ^ observed)*((1-pred_intro) ^ (1-observed)))/((pred_nointro ^ observed)*((1-pred_nointro) ^ (1-observed)) + (pred_intro ^ observed)*((1-pred_intro) ^ (1-observed)))

  prob_intro
}

```

#### Function for the M step

Given a probability of introduction for each observation, we fit the model. Depending on the model, some parameters can be fixed in the `mle2` function. Several repetitions of the fit are computed, with function `rep_est`.

The likelihood is defined as:

$$L(\Theta|R_1,...,R_i,...) = \prod_{i}(p(R_i \in M_{intro}).L(\Theta|R_i,R_i \in M_{intro}) + p(R_i \in M_{nointro}).L(\Theta|R_i,R_i \in M_{nointro}))$$

```{r}
M_step <- function(introd, modnum, prevfit) {

  prop_intro = mean(introd)
  
  #####
  if(modnum == 1){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda1",  "lambda2",  "alpha",  "beta",  "teta",  "delta", "eta"), startvar = c("mu"))
  }else if(modnum == 2){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda2",  "alpha",  "beta",  "teta",  "delta", "eta"), startvar = c("mu",  "lambda1"))
  }else if(modnum == 3){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("alpha",  "beta",  "teta",  "delta", "eta"), startvar = c("mu", "lambda1",  "lambda2"))
  }else if(modnum == 4){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda1",  "lambda2",  "alpha",  "beta", "eta"), startvar = c("mu",  "teta",  "delta"))
  }else if(modnum == 5){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda2",  "alpha",  "beta", "eta"), startvar = c("mu",  "lambda1",  "teta",  "delta"))
  }else if(modnum == 6){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("alpha",  "beta", "eta"), startvar = c("mu",  "lambda1",  "lambda2",  "teta",  "delta"))
  }else if(modnum == 7){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda1",  "lambda2",  "teta",  "delta", "eta"), startvar = c("mu",  "alpha",  "beta"))
  }else if(modnum == 8){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda2",  "teta",  "delta", "eta"), startvar = c("mu",  "lambda1",  "alpha",  "beta"))
  }else if(modnum == 9){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("teta",  "delta", "eta"), startvar = c("mu",  "lambda1",  "lambda2",  "alpha",  "beta"))
  }else if(modnum == 10){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda1",  "lambda2", "eta"), startvar = c("mu",  "alpha",  "beta",  "teta",  "delta"))
  }else if(modnum == 11){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("lambda2", "eta"), startvar = c("mu",  "lambda1",  "alpha",  "beta",  "teta",  "delta"))
  }else if(modnum == 12){
    modstep = rep_est(nrep=3, minusloglrep = run2_intr(introd), methodrep = "Nelder-Mead", controlrep = list(maxit = 5000), prev_est = prevfit, fixedvar = c("eta"), startvar = c("mu",  "lambda1",  "lambda2",  "alpha",  "beta",  "teta",  "delta"))
  }
  #####
  
  raw_coef = coef(modstep)
  ob_pred = run2(aim="pred", intr=introd, mu=raw_coef["mu"], lambda1=raw_coef["lambda1"], lambda2=raw_coef["lambda2"], alpha=raw_coef["alpha"], beta=raw_coef["beta"], teta=raw_coef["teta"], delta=raw_coef["delta"], eta=raw_coef["eta"])
  
  raw_coef = c(raw_coef, p_intro = prop_intro)
  llmod = ob_pred[[2]]
  true_coef = ob_pred[[3]]
  number_cycle = ob_pred[[1]]$cycle
  
  print(true_coef)
  list(raw_coef, llmod, modstep, true_coef, number_cycle)
}
```

#### Function to launch the whole algorithm

We repeat the algorithm several times to make sure the whole space has been explored.

Convergence is defined as: the distance between the likelihood and the former likelihood is less than 1, 3 times in a row.

```{r}
EM_algo <- function(modnum, numb_rep_algo=5, tol=0.01, max.step=100000000000, plot_step=T, init_p = 0.01) {

  repetalgo = list()
  repetalgo_minloglik = c()
  
  for(algorep_ite in 1:numb_rep_algo){
    print(paste0("##### Model ", modnum, " (repetition ", algorep_ite, "/", numb_rep_algo, " of the algorithm) #####")); cat("\n")
    
    step = 0
    minloglik = 10^6
    
    raw_params = "none"
    
    obs_res = resamu(obj="est", nwkseff=1, lag_amu_init=1, expo=definition_amu, quantuse=amu_model_is_quanti, tempo_amu=effect_recent_amu)$resquanti
    obs_res = obs_res[! is.na(obs_res)]
    
    introductions_next_step = init_p
    # introductions_next_step = runif(length(obs_res))

    counting_for_convergence = 0
    while((step < 4) | ((counting_for_convergence <5) & (step <= max.step+1))){
      step = step +1
      if(step == max.step+1){
        print(paste0("Convergence of the likelihood was not reached after ", max.step, " steps."))
        break
      }
      
      print(paste0("Step: ", step))
      introductions = introductions_next_step
      output_mstep = M_step(introd=introductions, modnum, prevfit=raw_params)
      raw_params = output_mstep[[1]]
      # If we determine a prior value for "eta", because it could not be estimated in the first step (no introduction)
      # if(step == 1){
      #   raw_params["eta"] = 5
      # }
      old.minloglik = minloglik
      minloglik = output_mstep[[2]]
      modstep = output_mstep[[3]]
      param_true_format = output_mstep[[4]]
      cycle_id = output_mstep[[5]]
      introductions_next_step = E_step(observed=obs_res, params=raw_params, prev_p_intro = introductions)
      if(plot_step){
        fitplot_for_EMalgo(modstep, introductions)
      }
      Q_val = -minloglik
      print(paste("Complete data LogLikelihood (Q function):", Q_val))
      H_val = sum(introductions * log(introductions) + (1-introductions) * log(1-introductions))
      print(paste("H function:", H_val))
      obs_ll = Q_val - H_val
      print(paste("Observed data LogLikelihood (Q-H):", obs_ll))

      if (abs(minloglik - old.minloglik) < tol){
        counting_for_convergence = counting_for_convergence +1
      }else{
        counting_for_convergence = 0
      }
    }
    print(paste0("Final step: ", step)); cat("\n")
    
    if(step == max.step+1){write.table(c(), file=paste0("C:/Users/Jonathan/Desktop/Model ", modnum, ", iteration ", algorep_ite, " out of ", numb_rep_algo, " - no convergence of the likelihood"))}

    repetalgo[[algorep_ite]] = list(modstep, raw_params, introductions, minloglik, param_true_format, number_obs=length(obs_res), cycle_id)
    repetalgo_minloglik[algorep_ite] = minloglik
  }
  
  save(repetalgo, file = paste0("C:/Users/Jonathan/Desktop/resul_mod", modnum, "_", definition_amu, "_isquanti", amu_model_is_quanti, "_", effect_recent_amu, "_c", conc_used, ".rdata"))

  repetalgo[[which.min(repetalgo_minloglik)]]
}
```

### Validation of the method on simulated data

From the real AMU data, we use simulated (fake) data of resistance to test our method on 4 scenarios:

- No relationship between recent use and resistance, and no introductions (scenario 1)
- No relationship between recent use and resistance, and introductions (scenario 2)
- Relationship between recent use and resistance, and no introductions (scenario 3)
- Relationship between recent use and resistance, and introductions (scenario 4)

We fit models 1, 7, 13 and 19 in these 4 scenarios. The objective is to test the ability of our method to detect such situations.

First, we save the true resistance data:

```{r}
res_data_saved = res_data
rm(res_data)

```

#### Scenario 1

We create fake resistance data randomly: there is no relationship between the resistance and explanatory variables.

```{r, fig.width=7, fig.height=5}
if(test_method_comp){
  load("C:/Users/Jonathan/Desktop/Resultats simulations/colistin_resistance/Method validation/test_scenario_1.rdata")
  res_data = test_scenario_1[[1]]
}else{
  set.seed(1)
  res_data = data.frame(S = res_data_saved$S,
                        M = base::sample(x = c(T,F), size = n_cyc, replace = T, prob = c(0.5,0.5)),
                        E = base::sample(x = c(T,F), size = n_cyc, replace = T, prob = c(0.5,0.5)))
  rownames(res_data) = rownames(res_data_saved)
}

hist(as.numeric(as.matrix(res_data)), nclass = 15, col = "royalblue4", main = "Histogram of all resistance data", xlab = "Resistance values")
resamu(nwkseff = 2.5, quantuse=F, lag_amu_init = 1, expo="allab", tempo_amu="step", var_to_plot="amu_t")

```

We use the EM algorithm for all models:

```{r, fig.width=6, fig.height=4}

if(test_method_comp){
  resul_scenar_1 = test_scenario_1[[2]]
}else{

  cl<-makeCluster(7)
  clusterEvalQ(cl, list(library(pracma), library(bbmle)))
  clusterExport(cl, c("effect_recent_amu","almost_one","type_mod","definition_amu","amu_model_is_quanti","E_step","M_step","rep_est","run2","run2_intr","resamu","n_cyc","res_data","weeks_samp","col_expo","col_expo_quanti","allab_expo","allab_expo_quanti"))
  resul_scenar_1 = parLapply(cl, X=1:12, fun=EM_algo, numb_rep_algo=4, plot_step=F)
  stopCluster(cl)

  test_scenario_1 = list(res_data, resul_scenar_1)
  save(test_scenario_1, file = "C:/Users/Jonathan/Desktop/test_scenario_1.rdata")
}

fitplot(fitted_model = resul_scenar_1[[1]][[1]], intro = resul_scenar_1[[1]][[3]])
fitplot(fitted_model = resul_scenar_1[[4]][[1]], intro = resul_scenar_1[[4]][[3]])
fitplot(fitted_model = resul_scenar_1[[7]][[1]], intro = resul_scenar_1[[7]][[3]])

rm(res_data)

```

Let us compare models' $IC_{H,Q}$. Minimum $IC_{H,Q}$ should be model 1.

```{r}

l_Q_fct = l_H_fct = l_numb_par = c()

for(m in 1:12){
  l_Q_fct[m] = -as.numeric(resul_scenar_1[[m]][[4]])
  
  l_numb_par[m] = length(resul_scenar_1[[m]][[1]]@coef)
  
  vec_p = resul_scenar_1[[m]][[3]]
  vec_p[vec_p == 0] = 1e-7
  vec_p[vec_p == 1] = 1-(1e-7)
  
  l_H_fct[m] = sum(vec_p * log(vec_p) + (1-vec_p) * log(1-vec_p))
}

l_IC_HQ = -2*l_Q_fct + 2*l_H_fct + 2*l_numb_par

print(data.frame(model = 1:12, IC_HQ = l_IC_HQ))
print(paste0("Best model: ", which.min(l_IC_HQ)))

rm(l_Q_fct, l_H_fct, l_numb_par, vec_p, l_IC_HQ)

```

#### Scenario 2

We create fake resistance data such that there is a relationship between the resistance and recent AMU (2.5 last days):

```{r, fig.width=7, fig.height=5}
if(test_method_comp){
  load("C:/Users/Jonathan/Desktop/Resultats simulations/colistin_resistance/Method validation/test_scenario_2.rdata")
  res_data = test_scenario_2[[1]]
}else{

  mu_fake = -15
  beta_fake = 2.5
  alpha_fake = 20
  
  res_data = res_data_saved
  fake_obsdat = run2(aim="pred", intr=0, mu=mu_fake, lambda1=0, lambda2=0, alpha=alpha_fake, beta=logit((beta_fake)/5), teta=0, delta=1, eta=0)[["obsdat"]]
  rm(res_data)
  fake_res = fake_obsdat$predmod >=0.5
  fake_amu = fake_obsdat$amu_t
  
  set.seed(1)
  fake_res[fake_res] = base::sample(x = c(T, F), size = sum(fake_res), replace = T, prob = c(0.85,0.15))
  fake_res[! fake_res] = base::sample(x = c(T, F), size = sum(! fake_res), replace = T, prob = c(0.15,0.85))
  
  na_res_val = which(is.na(c(rbind(res_data_saved$M, res_data_saved$E))))
  print(paste("CAUTION: The number of NAs in the true resistance data on 2nd and 3rd samplings is:", length(na_res_val)))
  if(length(na_res_val) == 1){
    fake_res = c(fake_res[1:(na_res_val-1)], NA, fake_res[na_res_val:length(fake_res)])
  }
  
  res_data = as.data.frame(cbind(res_data_saved$S, matrix(fake_res, nrow=n_cyc, ncol=2, byrow=T)))

  rm(beta_fake, alpha_fake, fake_res, na_res_val, fake_obsdat)
  
}

hist(as.numeric(as.matrix(res_data)), nclass = 15, col = "royalblue4", main = "Histogram of all resistance data", xlab = "Resistance values")
resamu(nwkseff = 2.5, quantuse=F, lag_amu_init = 1, expo="allab", tempo_amu = "step", var_to_plot="amu_t")

```

We use the EM algorithm for all models:

```{r, fig.width=6, fig.height=4}

if(test_method_comp){
  resul_scenar_2 = test_scenario_2[[2]]
}else{

  cl<-makeCluster(7)
  clusterEvalQ(cl, list(library(pracma), library(bbmle)))
  clusterExport(cl, c("effect_recent_amu","almost_one","type_mod","definition_amu","amu_model_is_quanti","E_step","M_step","rep_est","run2","run2_intr","resamu","n_cyc","res_data","weeks_samp","col_expo","col_expo_quanti","allab_expo","allab_expo_quanti"))
  resul_scenar_2 = parLapply(cl, X=1:12, fun=EM_algo, numb_rep_algo=4, plot_step=F)
  stopCluster(cl)

  test_scenario_2 = list(res_data, resul_scenar_2)
  save(test_scenario_2, file = "C:/Users/Jonathan/Desktop/test_scenario_2.rdata")
}

fitplot(fitted_model = resul_scenar_2[[1]][[1]], intro = resul_scenar_2[[1]][[3]])
fitplot(fitted_model = resul_scenar_2[[4]][[1]], intro = resul_scenar_2[[4]][[3]])
fitplot(fitted_model = resul_scenar_2[[7]][[1]], intro = resul_scenar_2[[7]][[3]])

rm(res_data)

```

Let us compare models' $IC_{H,Q}$. Minimum $IC_{H,Q}$ should be model 7.

```{r}

l_Q_fct = l_H_fct = l_numb_par = c()

for(m in 1:12){
  l_Q_fct[m] = -as.numeric(resul_scenar_2[[m]][[4]])
  
  l_numb_par[m] = length(resul_scenar_2[[m]][[1]]@coef)
  
  vec_p = resul_scenar_2[[m]][[3]]
  vec_p[vec_p == 0] = 1e-7
  vec_p[vec_p == 1] = 1-(1e-7)
  
  l_H_fct[m] = sum(vec_p * log(vec_p) + (1-vec_p) * log(1-vec_p))
}

l_IC_HQ = -2*l_Q_fct + 2*l_H_fct + 2*l_numb_par

print(data.frame(model = 1:12, IC_HQ = l_IC_HQ))
print(paste0("Best model: ", which.min(l_IC_HQ)))

rm(l_Q_fct, l_H_fct, l_numb_par, vec_p, l_IC_HQ)

```

#### Scenario 3

We create fake resistance data such that there is a relationship between the resistance and initial AMU (2.5 first days):

```{r, fig.width=7, fig.height=5}
if(test_method_comp){
  load("C:/Users/Jonathan/Desktop/Resultats simulations/colistin_resistance/Method validation/test_scenario_3.rdata")
  res_data = test_scenario_3[[1]]
}else{

  mu_fake = -15
  delta_fake = 2.5
  teta_fake = 20
  
  res_data = res_data_saved
  fake_obsdat = run2(aim="pred", intr=0, mu=mu_fake, lambda1=0, lambda2=0, alpha=0, beta=1, teta=teta_fake, delta=logit((delta_fake)/5), eta=0)[["obsdat"]]
  rm(res_data)
  fake_res = fake_obsdat$predmod >=0.5
  fake_amu = fake_obsdat$amu_t
  
  set.seed(1)
  fake_res[fake_res] = base::sample(x = c(T, F), size = sum(fake_res), replace = T, prob = c(0.85,0.15))
  fake_res[! fake_res] = base::sample(x = c(T, F), size = sum(! fake_res), replace = T, prob = c(0.15,0.85))
  
  na_res_val = which(is.na(c(rbind(res_data_saved$M, res_data_saved$E))))
  print(paste("CAUTION: The number of NAs in the true resistance data on 2nd and 3rd samplings is:", length(na_res_val)))
  if(length(na_res_val) == 1){
    fake_res = c(fake_res[1:(na_res_val-1)], NA, fake_res[na_res_val:length(fake_res)])
  }
  
  res_data = as.data.frame(cbind(res_data_saved$S, matrix(fake_res, nrow=n_cyc, ncol=2, byrow=T)))
  rm(delta_fake, teta_fake, fake_res, na_res_val, fake_obsdat)
}

hist(as.numeric(as.matrix(res_data)), nclass = 15, col = "royalblue4", main = "Histogram of all resistance data", xlab = "Resistance values")
resamu(nwkseff = 1, quantuse=F, lag_amu_init = 2.5, expo="allab", tempo_amu="step", var_to_plot="amu_init")

```

We use the EM algorithm for all models:

```{r, fig.width=6, fig.height=4}

if(test_method_comp){
  resul_scenar_3 = test_scenario_3[[2]]
}else{

  cl<-makeCluster(7)
  clusterEvalQ(cl, list(library(pracma), library(bbmle)))
  clusterExport(cl, c("effect_recent_amu","almost_one","type_mod","definition_amu","amu_model_is_quanti","E_step","M_step","rep_est","run2","run2_intr","resamu","n_cyc","res_data","weeks_samp","col_expo","col_expo_quanti","allab_expo","allab_expo_quanti"))
  resul_scenar_3 = parLapply(cl, X=1:12, fun=EM_algo, numb_rep_algo=4, plot_step=F)
  stopCluster(cl)

  test_scenario_3 = list(res_data, resul_scenar_3)
  save(test_scenario_3, file = "C:/Users/Jonathan/Desktop/test_scenario_3.rdata")
}

fitplot(fitted_model = resul_scenar_3[[1]][[1]], intro = resul_scenar_3[[1]][[3]])
fitplot(fitted_model = resul_scenar_3[[4]][[1]], intro = resul_scenar_3[[4]][[3]])
fitplot(fitted_model = resul_scenar_3[[7]][[1]], intro = resul_scenar_3[[7]][[3]])

rm(res_data)

```

Let us compare models' $IC_{H,Q}$. Minimum $IC_{H,Q}$ should be model 4.

```{r}

l_Q_fct = l_H_fct = l_numb_par = c()

for(m in 1:12){
  l_Q_fct[m] = -as.numeric(resul_scenar_3[[m]][[4]])
  
  l_numb_par[m] = length(resul_scenar_3[[m]][[1]]@coef)
  
  vec_p = resul_scenar_3[[m]][[3]]
  vec_p[vec_p == 0] = 1e-7
  vec_p[vec_p == 1] = 1-(1e-7)
  
  l_H_fct[m] = sum(vec_p * log(vec_p) + (1-vec_p) * log(1-vec_p))
}

l_IC_HQ = -2*l_Q_fct + 2*l_H_fct + 2*l_numb_par

print(data.frame(model = 1:12, IC_HQ = l_IC_HQ))
print(paste0("Best model: ", which.min(l_IC_HQ)))

rm(l_Q_fct, l_H_fct, l_numb_par, vec_p, l_IC_HQ)

```


After these tests, we delete the fake resistance data and come back to the true data:

```{r}
res_data = res_data_saved
rm(res_data_saved, test_scenario_1, resul_scenar_1, test_scenario_2, resul_scenar_2, test_scenario_3, resul_scenar_3)
```

## Results

If the computation was already performed, `alread_comp = T`.

### Parallel computation of the E-M algorithm for models 1 to 12

We repeat the algorithm 5 times (with different seeds) per model, and each maximization step (model fit) is repeated 5 times (with different seeds too) to make sure the likelihood maximum is reached.

```{r, label="E-M algorithm on all models"}

if(alread_comp){
  
  load(paste0("C:/Users/Jonathan/Desktop/Resultats simulations/colistin_resistance/", definition_amu, " - ", ifelse(amu_model_is_quanti, "Quantitative", "Qualitative"), " - ", effect_recent_amu, "/c=", conc_used, "/resulpar_", definition_amu, "_isquanti", amu_model_is_quanti, "_", effect_recent_amu, "_c", conc_used, "_all.rdata"))
  
}else{
  t = as.numeric(Sys.time())

  cl<-makeCluster(7)
  clusterEvalQ(cl, list(library(pracma), library(bbmle)))
  clusterExport(cl, c("conc_used","effect_recent_amu","almost_one","type_mod","definition_amu","amu_model_is_quanti","E_step","M_step","rep_est","run2","run2_intr","resamu","n_cyc","res_data","weeks_samp","col_expo","col_expo_quanti","allab_expo","allab_expo_quanti"))
  resulpar = parLapply(cl, X=1:12, fun=EM_algo, numb_rep_algo=4, plot_step=F)
  stopCluster(cl)

  save(resulpar, file = paste0("C:/Users/Jonathan/Desktop/resulpar_all_", definition_amu, "_isquanti", amu_model_is_quanti, "_", effect_recent_amu, "_c", conc_used, ".rdata"))

  as.numeric(Sys.time()) - t
  
  rm(cl)
}

rm(alread_comp)
```

### Comparison of the models

The list `lfit` contains the 24 fitted models. We compare and plot their loglikelihood and AIC weights (= probability that each model is the best).

The AIC weight $W_k$ of a model $k$ is defined as:

$$ W_k = \frac{e^{-\frac{1}{2}.(AIC_k-min(AIC))}}{\sum_{j=1}^{24}(e^{-\frac{1}{2}.(AIC_j-min(AIC))})}$$
where $N$ is the total number of models, and $AIC_k$ is the AIC of model $k$.

```{r, fig.width=8, fig.height=6}
l_Q_fct = l_H_fct = l_numb_par = c()

for(m in 1:12){
  l_Q_fct[m] = -as.numeric(resulpar[[m]][[4]])
  
  l_numb_par[m] = length(resulpar[[m]][[1]]@coef)
  
  vec_p = resulpar[[m]][[3]]
  vec_p[vec_p == 0] = 1e-7
  vec_p[vec_p == 1] = 1-(1e-7)
  
  l_H_fct[m] = sum(vec_p * log(vec_p) + (1-vec_p) * log(1-vec_p))
}

l_IC_HQ = -2*l_Q_fct + 2*l_H_fct + 2*l_numb_par
l_LL_obs = l_Q_fct - l_H_fct

tab_comp_mod = data.frame(mod = as.factor(1:12), Q_fct = l_Q_fct, H_fct = l_H_fct, LL_obs = l_LL_obs, numb_par = l_numb_par, IC_HQ = l_IC_HQ)
plot_comp_mod = melt(tab_comp_mod, id = "mod")

p = ggplot(data = plot_comp_mod, aes(y = mod))
p = p + ggtitle("Comparison of the fitted models") + xlab("Indicator value") + ylab("Models")

p = p + geom_vline(data = subset(plot_comp_mod, variable == "Q_fct"), aes(xintercept = max(plot_comp_mod$value[plot_comp_mod$variable == "Q_fct"])), linetype = "dashed")

p = p + geom_vline(data = subset(plot_comp_mod, variable == "LL_obs"), aes(xintercept = max(plot_comp_mod$value[plot_comp_mod$variable == "LL_obs"])), linetype = "dashed")

p = p + geom_vline(data = subset(plot_comp_mod, variable == "IC_HQ"), aes(xintercept = min(plot_comp_mod$value[plot_comp_mod$variable == "IC_HQ"])), linetype = "dashed")

p = p + geom_point(aes(x = value), shape=18, size=2)
p = p + facet_wrap(~variable, scales = "free_x")
p = ggplotly(p)
p

rm(p, plot_comp_mod, l_Q_fct, l_H_fct, l_numb_par)
```

According to the AIC, the best model is:

```{r}
best_model = which.min(tab_comp_mod$IC_HQ)
print(paste0("Model ", best_model))

```

We plot its fit (observed VS predicted):

```{r, fig.width=6, fig.height=4}

fitplot(fitted_model = resulpar[[best_model]][[1]], intro = resulpar[[best_model]][[3]])

```

We plot a figure showing the models in the order of decreasing AIC weight, and the cumulative AIC weight tending to 100%. The variables included for each model are plotted above:

```{r}
val_ICHQ = data.frame(mod = 1:12, IC_HQ = l_IC_HQ, rec_amu = NA, init_amu = NA, first_samp = NA, prev_samp = NA)

gap_minmax = max(l_IC_HQ) - min(l_IC_HQ)
val_ICHQ[7:12, "rec_amu"] = min(l_IC_HQ) - gap_minmax/20
val_ICHQ[c(4:6, 10:12), "init_amu"] = min(l_IC_HQ) - 2 * gap_minmax/20
val_ICHQ[seq(2,11,3), "first_samp"] = min(l_IC_HQ) - 3 * gap_minmax/20
val_ICHQ[seq(3,12,3), "prev_samp"] = min(l_IC_HQ) - 4 * gap_minmax/20

val_ICHQ = val_ICHQ[order(val_ICHQ$IC_HQ, decreasing = F),]
val_ICHQ$mod = factor(val_ICHQ$mod, levels = val_ICHQ$mod)

val_ICHQ = melt(val_ICHQ, id.vars=c("mod", "IC_HQ"), variable.name="var", value.name="position")

p = ggplot(data = val_ICHQ, aes(x = mod, y = IC_HQ, group = 1, na.rm = TRUE))
p = p + xlab("Models") + ylab(bquote(IC['H,Q']))
p = p + theme_bw()
p = p + geom_line() + geom_point(size = 2)
p = p + geom_segment(aes(x = as.numeric(mod)-0.5, xend = as.numeric(mod)+0.5, y = position, yend = position, col = var), size = 3)
p = p + scale_colour_brewer(type="qual", palette=1,
                           name = "Variables included:",
                           labels = c("Recent AMU", "Initial AMU", "First sample", "Previous sample"))
p

```

### Spatial analysis of the probability of introduction

Now we computed the probability of introduction (PoI) for each sample (2 per farm), we want to check if:

- the PoI is spatially autocorrelated
- some areas have a higher or lower PoI.

We load the GPS data (`gps_farms`):

```{r}
gps_farms = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/GPS-300818.xlsx", sheet="Cycle")[,c(12,13,14)])
colnames(gps_farms) = c("Farm", "Lat", "Long")
gps_farms$Farm = substr(gps_farms$Farm, 4, 6)
gps_farms = gps_farms[! is.na(gps_farms$Lat),]
```

We gather in a same data frame the farm IDs and introduction probabilities (`prob_intro_farms`):

```{r}
max_AICweight_intro_model = best_model
prob_intro_farms = data.frame(Farm = resulpar[[max_AICweight_intro_model]][[7]], Prob = resulpar[[max_AICweight_intro_model]][[3]])
prob_intro_farms$Farm = rownames(weeks_samp)[prob_intro_farms$Farm]
prob_intro_farms$Farm = substr(prob_intro_farms$Farm, 1, 3)
prob_intro_farms$Obs_res = as.character(fitplot(fitted_model = resulpar[[best_model]][[1]], intro = resulpar[[best_model]][[3]]))

if(any(prob_intro_farms$Prob > 0.75 & prob_intro_farms$Obs_res == 0)){
  print("CAUTION: Some data point is susceptible but has a high probability of introduction")
}

```

We add GPS information to `prob_intro_farms`.

```{r, fig.width=7, fig.height=6}
prob_intro_farms = merge(x = prob_intro_farms, y = gps_farms, by = "Farm", all.x = T)

```

We load the data on commune borders in the districts of ViParc study:

```{r, label="Load commune borders data"}
sf_file = readOGR("C:/Users/Jonathan/Desktop/Programmes/amr_vietnam/Shapefiles/VN National Commune_new.shp")
sf_file_df = fortify(sf_file[sf_file$D_CODE %in% c(866, 872, 873),])

```

#### Assessing the global spatial autocorrelation of the probability of introduction

We compute the Moran's I statistic, defined as:

$$I = \frac{N_{samp}\sum_{i,j}W_{i,j}(p_i-\bar{p})(p_j-\bar{p})}{\sum_{i,j}W_{i,j}\sum_{i}(p_i-\bar{p})^2}$$

where $p_i$ is the PoI computed for sample $i$, $\bar{p}$ is the mean of probabilities, and $N_{samp}$ the number of samples. $W$ is the weight matrix: the closer are samples $i$ and $j$, the higher is $W_{i,j}$. We define it as follows:

$$W_{i,j} = g(k, D(i,j))$$
where:

* $D(i,j)$ is the geographical distance between farms where samples $i$ and $j$ were sampled.
* $g$ is a decreasing function of $D(i,j)$, and $k$ a tuning parameter of $g$. We explore several possibilities for $g$ definition:

  + An exponential decay: $W_{i,j} = e^{-k.D(i,j)}$
  + A threshold function: $W_{i,j} = 1$ if $D(i,j) \le k$ and $W_{i,j} = 0$ if $D(i,j) > k$
  + A "Closest neighbor" process where $k$ is the number of closest neighbor that are considered. $W_{i,j} = 1$ if $D(i,j) \le D(i,j_k)$ and $W_{i,j} = 0$ if $D(i,j) > D(i,j_k)$, where $j_k$ is the k-th closest neighbor of $i$.

Therefore, the more distant $i$ and $j$ are, the smaller $W_{i,j}$ (modulated by $k$), and the less the couple $(i,j)$ will be considered for the calculation of the Moran's I statistic.

Using one of the possible functions $g$, we compute the Moran's I for a range of values for $k$. Each time, we use package `ape` to test whether the "observed" value of Moran's I is different than its expected value under the hypothesis of no spatial autocorrelation. We plot the p-value of this test depending on the value of $k$.

We also plot a map with the PoI for each observation. (It's possible to shift the dots to plot both observations of a same farm side by side.)

In green is the value of the weight $W$ around each farm, with a given value of $k$. The more transparent, the lower the weight $W$, and therefore the less farms at this distance are taken into account into the calculation of the Moran's I statistic.

It is possible to plot the green area around one farm only (as an example).

Function to compute the Moran's I statistic:

```{r}
dist_geo = spDists(x = as.matrix(x = prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long", "Lat")]), longlat = T)

coef_moran = function(k, decay_fun){
  
  if(decay_fun == "exp"){
    weight_matrix = exp(-k * dist_geo)
  }else if(decay_fun == "power"){
    weight_matrix = 1/(1+(dist_geo)^k)
  }else if(decay_fun == "linear"){
    weight_matrix = apply(X = dist_geo, MARGIN = c(1,2), FUN = function(x) max(0, 1-(x/k)))
  }else if(decay_fun == "next_neighbor"){
    weight_matrix = dist_geo
    for(i in 1:nrow(weight_matrix)){
      weight_matrix[i,] = 0
      distance_k_closest_neighbor = sort(dist_geo[i,], partial=k)[k]
      weight_matrix[i, which(dist_geo[i,] <= distance_k_closest_neighbor)] = 1
    }
  }else if(decay_fun == "thresh"){
    weight_matrix = (dist_geo <= k)
  }
  diag(weight_matrix) = 0
  
  res = Moran.I(x = prob_intro_farms$Prob[! is.na(prob_intro_farms$Prob)], weight = weight_matrix, alternative = "greater")

  c(obs = res$observed, expect = res$expected, sd = res$sd, p_val = res$p.value)
}

```

Function to plot the figures described above:

```{r}
res_exp = sapply(X = seq(0,24,0.1)[-1], FUN = coef_moran, decay_fun = "exp")
res_power = sapply(X = seq(0,24,0.1)[-1], FUN = coef_moran, decay_fun = "power")
res_thresh = sapply(X = seq(0,24,0.1)[-1], FUN = coef_moran, decay_fun = "thresh")
res_linear = sapply(X = seq(0,24,0.1)[-1], FUN = coef_moran, decay_fun = "linear")
res_next_neighbor = sapply(X = seq(0,24,1)[-1], FUN = coef_moran, decay_fun = "next_neighbor")

spatial_moran = function(decay_param, decay_fct = "exp", pval = 0.05, plot_only_one_area = T){
  
  pval = as.numeric(pval)
  pval_this_param = coef_moran(k = decay_param, decay_fun = decay_fct)["p_val"]
  
  if(decay_fct == "exp"){
    val_exp = seq(0,24,0.1)[-1]
    res_ref = res_exp
  }else if(decay_fct == "power"){
    val_exp = seq(0,24,0.1)[-1]
    res_ref = res_power
  }else if(decay_fct == "thresh"){
    val_exp = seq(0,24,0.1)[-1]
    res_ref = res_thresh
  }else if(decay_fct == "linear"){
    val_exp = seq(0,24,0.1)[-1]
    res_ref = res_linear
  }else if(decay_fct == "next_neighbor"){
    val_exp = seq(0,24,1)[-1]
    res_ref = res_next_neighbor
  }
  
  if(any(res_ref["p_val",] < pval)){
    printed_text = paste0("Using this function for the weight matrix in the calculation of Moran's I, some values of the decay parameter make the test of spatial autocorrelation significant (p<", pval, "). For this specific value of the parameter, the p-value is: ", round(pval_this_param, 3), ".")
  }else{
    printed_text = paste0("Using this function for the weight matrix in the calculation of Moran's I, no value of the decay parameter make the test of spatial autocorrelation significant (p>", pval, "). For this specific value of the parameter, the p-value is: ", round(pval_this_param, 3), ".")
  }
  
  p1 = ggplot()
  p1 = p1 + ggtitle("p-value of Moran's I test")
  p1 = p1 + xlab("Decay parameter") + ylab("p-value")
  p1 = p1 + geom_line(aes(x = val_exp, y = res_ref["p_val",]))
  p1 = p1 + geom_line(aes(x = c(0, max(val_exp)), y = c(pval, pval)), col = "red", linetype = "dashed")
  p1 = p1 + geom_line(aes(x = c(decay_param, decay_param), y = c(0, max(res_ref["p_val",]))), col = "dodgerblue2", size = 1)

  heat_map_lat = seq(min(prob_intro_farms$Lat)-0.035, max(prob_intro_farms$Lat)+0.035, 0.005)
  heat_map_long = seq(min(prob_intro_farms$Long)-0.035, max(prob_intro_farms$Long)+0.035, 0.005)
  heat_map_grid = expand.grid(heat_map_long, heat_map_lat)
  colnames(heat_map_grid) = c("Long", "Lat")
  heat_map_grid$decay = 0
  
  if(decay_fct != "next_neighbor"){
    if(plot_only_one_area == T){
      ref_point = 21
      dist_this_ref = spDistsN1(pts = as.matrix(heat_map_grid[,c("Long", "Lat")]), pt = as.numeric(prob_intro_farms[ref_point, c("Long", "Lat")]), longlat = T)
      
      if(decay_fct == "exp"){
        heat_map_grid$decay = exp(-decay_param * dist_this_ref)
      }else if(decay_fct == "power"){
        heat_map_grid$decay = 1/(1+dist_this_ref^decay_param)
      }else if(decay_fct == "thresh"){
        heat_map_grid$decay = as.numeric(dist_this_ref <= decay_param)
      }else if(decay_fct == "linear"){
        heat_map_grid$decay = -decay_param*dist_this_ref
        heat_map_grid$decay = pmax(0, 1-(dist_this_ref/decay_param))
      }
    }else{
      for(ref_point in 1:nrow(prob_intro_farms)){
        dist_this_ref = spDistsN1(pts = as.matrix(heat_map_grid[,c("Long", "Lat")]), pt = as.numeric(prob_intro_farms[ref_point, c("Long", "Lat")]), longlat = T)
        
        if(decay_fct == "exp"){
          dec_this_ref = exp(-decay_param * dist_this_ref)
        }else if(decay_fct == "power"){
          dec_this_ref = 1/(1+dist_this_ref^decay_param)
        }else if(decay_fct == "thresh"){
          dec_this_ref = as.numeric(dist_this_ref <= decay_param)
        }else if(decay_fct == "linear"){
          dec_this_ref = pmax(0, 1-(dist_this_ref/decay_param))
        }
        heat_map_grid$decay = pmax(heat_map_grid$decay, dec_this_ref)
      }
    }
  }

  p2 = ggplot()
  p2 = p2 + ggtitle("Probability of introduction in farms and area around them considered for the calculation of Moran's I")
  p2 = p2 + xlab("Longitude") + ylab("Latitude")
  
  p2 = p2  + geom_polygon(data = sf_file_df, aes(x = long, y = lat, group = id), col="black", size=0.5, fill = "beige")
  
  if(decay_fct != "next_neighbor"){
    p2 = p2 + geom_tile(data = heat_map_grid, aes(x = Long, y = Lat, alpha = decay), fill = "forestgreen")
  }
  p2 = p2 + geom_point(data = gps_farms, aes(x = Long, y = Lat), col = "black", size = 0.75)
  # p2 = p2 + geom_point(data = prob_intro_farms, aes(x = Long, y = Lat, fill = Prob), col = "black", shape = 21, size = 2)
  set.seed(78)
  p2 = p2 + geom_jitter(data = prob_intro_farms, aes(x = Long, y = Lat, fill = Prob), col = "black", shape = 21, size = 2, width = 0.003, height = 0.003)
  
  p2 = p2 + scale_fill_gradient2(name = "Probability of introduction:", low = "dodgerblue", mid = "white", high = "red", midpoint = 0.5, limits = c(0, 1))
  p2 = p2 + scale_alpha_continuous(name = "Value in the weight matrix:", range = c(0,1))
  p2 = p2 + ggsn::scalebar(dist = 5, location = "topright", dist_unit = "km", transform = TRUE, model = "WGS84", x.min = min(sf_file_df$long), x.max = max(sf_file_df$long), y.min = min(sf_file_df$lat), y.max = max(sf_file_df$lat))
  p2 = p2 + blank()

  list(printed_text = printed_text, p1 = p1, p2 = p2)
}

```

Shiny app for visualization:

```{r}
ui <- fluidPage(titlePanel("Spatial analysis with Moran's I"),{
  sidebarLayout(
    sidebarPanel(
      sliderInput("decay_param",
                  "Decay parameter:",
                  min = 0.1,
                  max = 24,
                  value=1),
      radioButtons("decay_fct",
                  "Decay function:",
                  choices = list("Exponential" = "exp", "Power" = "power", "Threshold" = "thresh", "Next neighbor" = "next_neighbor", "Linear" = "linear"),
                  selected = "exp"),
      radioButtons("pval",
                  "p-value of the test:",
                  choices = list("0.01" = 0.01, "0.05" = 0.05, "0.1" = 0.1),
                  selected = 0.05),
      radioButtons("plot_only_one_area",
                  "Do we plot the area around one point only ?",
                  choices = list("Yes" = T, "No" = F),
                  selected = T)
    ),
    
    mainPanel(
      textOutput("textdisp"),
      plotOutput("plotdisp1"),
      plotOutput("plotdisp2")
    )
  )
})

server <- function(input, output){

  output$textdisp <- renderText(
    spatial_moran(decay_param = input$decay_param, decay_fct = input$decay_fct, pval = input$pval, plot_only_one_area = input$plot_only_one_area)[["printed_text"]]
  )
  output$plotdisp1 <- renderPlot({
    spatial_moran(decay_param = input$decay_param, decay_fct = input$decay_fct, pval = input$pval, plot_only_one_area = input$plot_only_one_area)[["p1"]]
  })
  output$plotdisp2 <- renderPlot({
    spatial_moran(decay_param = input$decay_param, decay_fct = input$decay_fct, pval = input$pval, plot_only_one_area = input$plot_only_one_area)[["p2"]]
  })
}

shinyApp(ui=ui, server=server)

```

#### Local Moran's I

```{r}
library(spdep)

# prob_intro_farms = stats::aggregate(prob_intro_farms, by = list(prob_intro_farms$Farm), FUN = mean)
# prob_intro_farms$Farm = prob_intro_farms$Group.1

dist_geo = spDists(x = as.matrix(x = prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long", "Lat")]), longlat = T) # Must stay outside the function "local_moran"

local_moran = function(decay_param, decay_fun, p_val, style_weight, vis_spat_weight=T){
  
  set.ZeroPolicyOption(TRUE)
  prob_intro_farms$Lat = prob_intro_farms$Lat + runif(length(prob_intro_farms$Lat), 0, 0.00001)
  vec_scaled_prob = as.numeric(scale(prob_intro_farms$Prob[! is.na(prob_intro_farms$Prob)]))
  prob_intro_farms$scaled_prob = NA
  prob_intro_farms$scaled_prob[! is.na(prob_intro_farms$Prob)] = vec_scaled_prob
  
  if(decay_fun == "exp"){
    
    neighbors = dnearneigh(x = as.matrix(prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long","Lat")]), d1 = 0, d2 = 1000, longlat = T)
    weight_matrix = exp(-decay_param * dist_geo)
    list_weight_matrix = split(weight_matrix, rep(1:ncol(weight_matrix), each = 1))
    for(i in 1:length(list_weight_matrix)){list_weight_matrix[[i]] = list_weight_matrix[[i]][-i]}
    listw_for_test = nb2listw(neighbors, glist = list_weight_matrix, style = style_weight)
    
  }else if(decay_fun == "power"){
    
    neighbors = dnearneigh(x = as.matrix(prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long","Lat")]), d1 = 0, d2 = 1000, longlat = T)
    weight_matrix = 1/(1+(dist_geo)^decay_param)
    list_weight_matrix = split(weight_matrix, rep(1:ncol(weight_matrix), each = 1))
    for(i in 1:length(list_weight_matrix)){list_weight_matrix[[i]] = list_weight_matrix[[i]][-i]}
    listw_for_test = nb2listw(neighbors, glist = list_weight_matrix, style = style_weight)

  }else if(decay_fun == "linear"){
    
    neighbors = dnearneigh(x = as.matrix(prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long","Lat")]), d1 = 0, d2 = 1000, longlat = T)
    weight_matrix = apply(X = dist_geo, MARGIN = c(1,2), FUN = function(x) max(0, 1-(x/decay_param)))
    list_weight_matrix = split(weight_matrix, rep(1:ncol(weight_matrix), each = 1))
    for(i in 1:length(list_weight_matrix)){list_weight_matrix[[i]] = list_weight_matrix[[i]][-i]}
    listw_for_test = nb2listw(neighbors, glist = list_weight_matrix, style = style_weight)

  }else if(decay_fun == "next_neighbor"){
    
    neighbors = knn2nb(knearneigh(x = as.matrix(prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long","Lat")]), k = decay_param, longlat = T))
    listw_for_test = nb2listw(neighbors, style = style_weight)
    
  }else if(decay_fun == "thresh"){
    
    neighbors = dnearneigh(x = as.matrix(prob_intro_farms[! is.na(prob_intro_farms$Prob), c("Long","Lat")]), d1 = 0, d2 = decay_param, longlat = T)
    listw_for_test = nb2listw(neighbors, style = style_weight)
  }

  p.val.moran = moran.test(x = vec_scaled_prob, listw = listw_for_test)$p.value
  res.local.moran = localmoran(x = vec_scaled_prob, listw = listw_for_test)
  prob_intro_farms$lag = NA
  prob_intro_farms$lag[! is.na(prob_intro_farms$Prob)] = lag.listw(listw_for_test, vec_scaled_prob)
  prob_intro_farms$pval = NA
  prob_intro_farms$pval[! is.na(prob_intro_farms$Prob)] = res.local.moran[,5]
  
  prob_intro_farms$pval_cat[prob_intro_farms$pval >= 0.1] = ">0.1"
  prob_intro_farms$pval_cat[prob_intro_farms$pval < 0.1] = "<0.1"
  prob_intro_farms$pval_cat[prob_intro_farms$pval < 0.05] = "<0.05"
  
  prob_intro_farms$cluster[which((prob_intro_farms$scaled_prob < 0) & (prob_intro_farms$lag < 0))] = "Low-low"
  prob_intro_farms$cluster[which((prob_intro_farms$scaled_prob >= 0) & (prob_intro_farms$lag >= 0))] = "High-high"
  prob_intro_farms$cluster = paste0(prob_intro_farms$cluster, " (p", prob_intro_farms$pval_cat, ")")
  prob_intro_farms$cluster[which((prob_intro_farms$scaled_prob >= 0) & (prob_intro_farms$lag < 0))] = "High-low"
  prob_intro_farms$cluster[which((prob_intro_farms$scaled_prob < 0) & (prob_intro_farms$lag >= 0))] = "Low-high"
  prob_intro_farms$cluster[prob_intro_farms$pval_cat != "<0.05"] = "Not significant"
  # prob_intro_farms$cluster[prob_intro_farms$pval_cat != ">0.1"] = "p>0.1"
  
  # To visualize spatial weighting:
  if(vis_spat_weight){
    heat_map_lat = seq(min(prob_intro_farms$Lat)-0.035, max(prob_intro_farms$Lat)+0.035, 0.005)
    heat_map_long = seq(min(prob_intro_farms$Long)-0.035, max(prob_intro_farms$Long)+0.035, 0.005)
    heat_map_grid = expand.grid(heat_map_long, heat_map_lat)
    colnames(heat_map_grid) = c("Long", "Lat")
    heat_map_grid$decay = 0

    ref_point = 21
    dist_this_ref = spDistsN1(pts = as.matrix(heat_map_grid[,c("Long", "Lat")]), pt = as.numeric(prob_intro_farms[ref_point, c("Long", "Lat")]), longlat = T)
    
    if(decay_fun == "exp"){
      heat_map_grid$decay = exp(-decay_param * dist_this_ref)
    }else if(decay_fun == "power"){
      heat_map_grid$decay = 1/(1+dist_this_ref^decay_param)
    }else if(decay_fun == "thresh"){
      heat_map_grid$decay = as.numeric(dist_this_ref <= decay_param)
    }else if(decay_fun == "linear"){
      heat_map_grid$decay = -decay_param*dist_this_ref
      heat_map_grid$decay = pmax(0, 1-(dist_this_ref/decay_param))
    }
  }
  
  p1 = ggplot()
  p1 = p1 + xlab("Longitude") + ylab("Latitude")
  p1 = p1  + geom_polygon(data = sf_file_df, aes(x = long, y = lat, group = id), col="black", size=0.5, fill = "beige")
  if(vis_spat_weight & (decay_fun != "next_neighbor")){
    p1 = p1 + geom_tile(data = heat_map_grid, aes(x = Long, y = Lat, alpha = decay), fill = "forestgreen")
    p1 = p1 + scale_alpha_continuous(name = "Value in the weight matrix:", range = c(0,1))
  }
  p1 = p1 + geom_point(data = gps_farms, aes(x = Long, y = Lat), col = "black", size = 0.75)
  set.seed(78)
  p1 = p1 + geom_jitter(data = prob_intro_farms[! is.na(prob_intro_farms$Prob),], aes(x = Long, y = Lat, fill = cluster), col = "black", shape = 21, size = 3, width = 0.006, height = 0.006)
  p1 = p1 + ggsn::scalebar(dist = 5, location = "bottomleft", dist_unit = "km", transform = TRUE, model = "WGS84", x.min = min(sf_file_df$long), x.max = max(sf_file_df$long), y.min = min(sf_file_df$lat), y.max = max(sf_file_df$lat))
  # p1 = p1 + scale_fill_manual("Cluster:", values = c("High-high (p<0.05)"="red", "Low-low (p<0.05)"="dodgerblue", "High-high (p<0.1)"="lightpink", "Low-low (p<0.1)"="skyblue2", "High-low"="forestgreen", "Low-high"="forestgreen", "p>0.1"="white"))
  p1 = p1 + scale_fill_manual("Cluster:", values = c("High-high (p<0.05)"="red", "Low-low (p<0.05)"="dodgerblue", "High-high (p<0.1)"="lightpink", "Low-low (p<0.1)"="skyblue2", "High-low"="forestgreen", "Low-high"="forestgreen", "Not significant"="white"))
  p1 = p1 + theme(legend.position = "top", legend.direction = "vertical")
  p1 = p1 + blank()

  p2 = ggplot(data = prob_intro_farms, aes(x = scaled_prob, y = lag, fill = cluster))
  set.seed(78)
  p2 = p2 + geom_jitter(col = "black", shape = 21, size = 4, width = 0.05, height = 0.05)
  p2 = p2 + geom_vline(xintercept = 0) + geom_hline(yintercept = 0)
  p2 = p2 + xlab("Scaled probability of introduction") + ylab("Spatial lag")
  p2 = p2 + scale_fill_manual("Cluster:", values = c("High-high (p<0.05)"="red", "Low-low (p<0.05)"="blue", "High-high (p<0.1)"="lightpink", "Low-low (p<0.1)"="skyblue2", "High-low"="forestgreen", "Low-high"="forestgreen", "p>0.1"="white"))

  list(p.val.moran=p.val.moran, p1=p1, p2=p2)
}

```

Shiny app for visualization:

```{r}
ui <- fluidPage(titlePanel("Local Moran's I"),{
  sidebarLayout(
    sidebarPanel(
      sliderInput("decay_param",
                  "Decay parameter:",
                  min = 0.1,
                  max = 24,
                  value=1),
      radioButtons("decay_fun",
                  "Decay function:",
                  choices = list("Exponential" = "exp", "Power" = "power", "Threshold" = "thresh", "Next neighbor" = "next_neighbor", "Linear" = "linear"),
                  selected = "exp"),
      radioButtons("p_val",
                  "p-value of the test:",
                  choices = list("0.01" = 0.01, "0.05" = 0.05, "0.1" = 0.1),
                  selected = 0.05),
      radioButtons("style_weight",
                  "Style of weight:",
                  choices = list("Basic binary" = "B", "Row standardised" = "W", "Globally standardised" = "C", "Globally standardised divided by number of neighbours" = "U"),
                  selected = "W")
    ),
    
    mainPanel(
      textOutput("textdisp"),
      plotOutput("plotdisp1"),
      plotOutput("plotdisp2")
    )
  )
})

server <- function(input, output){

  output$textdisp <- renderText(
    local_moran(decay_param = input$decay_param, decay_fun = input$decay_fun, p_val = input$p_val, style_weight = input$style_weight)[["p.val.moran"]]
  )
  output$plotdisp1 <- renderPlot({
    local_moran(decay_param = input$decay_param, decay_fun = input$decay_fun, p_val = input$p_val, style_weight = input$style_weight)[["p1"]]
  })
  output$plotdisp2 <- renderPlot({
    local_moran(decay_param = input$decay_param, decay_fun = input$decay_fun, p_val = input$p_val, style_weight = input$style_weight)[["p2"]]
  })
}

shinyApp(ui=ui, server=server)

```

We plot on a same figure the probability of contamination for each cycle, and the local clusters using local Moran's I:

```{r}
library(ggpubr)
pA = ggplot()
pA = pA  + geom_polygon(data = sf_file_df, aes(x = long, y = lat, group = id), col="black", size=0.5, fill = "beige")
pA = pA + geom_point(data = gps_farms, aes(x = Long, y = Lat), col = "black", size = 0.75)
set.seed(78)
pA = pA + geom_jitter(data = prob_intro_farms, aes(x = Long, y = Lat, fill = Prob), col = "black", shape = 21, size = 3, width = 0.006, height = 0.006)
pA = pA + scale_fill_gradient2(name = "Probability of contamination:", low = "dodgerblue", mid = "white", high = "red", midpoint = 0.5, limits = c(0, 1))
# pA = pA + scale_alpha_continuous(name = "Value in the weight matrix:", range = c(0,1))
pA = pA + ggsn::scalebar(dist = 5, location = "bottomleft", dist_unit = "km", transform = TRUE, model = "WGS84", x.min = min(sf_file_df$long), x.max = max(sf_file_df$long), y.min = min(sf_file_df$lat), y.max = max(sf_file_df$lat))
pA = pA + theme(legend.position = "top")
pA = pA + blank()

pB = local_moran(decay_param = 2.3, decay_fun = "thresh", p_val = 0.05, style_weight = "W")[[2]]

pC = ggplot()
pC = pC  + geom_polygon(data = sf_file_df, aes(x = long, y = lat, group = id), col="black", size=0.5, fill = "beige")
pC = pC + geom_point(data = gps_farms, aes(x = Long, y = Lat), col = "black", size = 0.75)
set.seed(78)
pC = pC + geom_jitter(data = prob_intro_farms, aes(x = Long, y = Lat, fill = Obs_res), col = "black", shape = 21, size = 3, width = 0.006, height = 0.006)
pC = pC + scale_fill_manual(name = "Observed resistance:", values = c("0" = "dodgerblue", "1" = "red"), labels = c("0" = "Sensitive", "1" = "Resistant"))
pC = pC + ggsn::scalebar(dist = 5, location = "bottomleft", dist_unit = "km", transform = TRUE, model = "WGS84", x.min = min(sf_file_df$long), x.max = max(sf_file_df$long), y.min = min(sf_file_df$lat), y.max = max(sf_file_df$lat))
pC = pC + theme(legend.position = "top", legend.direction = "vertical")
pC = pC + blank()

prob_intro_farms$categ = "Source"
prob_intro_farms$categ[which(prob_intro_farms$Prob < 0.25)] = "None"
prob_intro_farms$categ[which(prob_intro_farms$Prob > 0.75)] = "Sink"

pD = ggplot()
pD = pD  + geom_polygon(data = sf_file_df, aes(x = long, y = lat, group = id), col="black", size=0.5, fill = "beige")
pD = pD + geom_point(data = gps_farms, aes(x = Long, y = Lat), col = "black", size = 0.75)
set.seed(78)
pD = pD + geom_jitter(data = prob_intro_farms, aes(x = Long, y = Lat, fill = categ), col = "black", shape = 21, size = 3, width = 0.006, height = 0.006)
# pD = pD + scale_fill_manual(name = "Observed resistance:", values = c("0" = "dodgerblue", "1" = "red"), labels = c("0" = "Sensitive", "1" = "Resistant"))
pD = pD + ggsn::scalebar(dist = 5, location = "bottomleft", dist_unit = "km", transform = TRUE, model = "WGS84", x.min = min(sf_file_df$long), x.max = max(sf_file_df$long), y.min = min(sf_file_df$lat), y.max = max(sf_file_df$lat))
pD = pD + theme(legend.position = "top", legend.direction = "vertical")
pD = pD + blank()


com_plot = ggarrange(pC, pA, pB, pD, nrow = 2, ncol = 2, labels = c("A", "B", "C"))
plot(com_plot)

```

#### Detecting geographic areas with a higher or lower probability of introduction

In the previous section, we assessed globally the spatial autocorrelation of the PoI. Here, we investigate the existence of geographic areas of high or low PoI.

We separate the data points ("observations") into clusters, only determined by their geographic location (euclidian distance). There are 1 or 2 (even 3) probabilities of introduction in each location-farm: in this case, the distance between them is 0. We perform a hierarchical clustering using the Ward algorithm. We obtain a dendrogram in which each leaf is a PoI.

From this process, we have the possibility to divide the data into a range of numbers of clusters $N_{clust}$: from 2 to the number of "observations".

For each value of $N_{clust}$, and for each of these $N_{clust}$ clusters, we test the null hypothesis that it is equally likely that a value of PoI randomly selected from this cluster will be less than or greater than a value of PoI randomly selected from outside the cluster. 

Regarding the potentially small number of observations in a cluster, we perform a non-parametric Wilcoxon-Mann-Whitney test.

In the following Shiny app, depending on the value of $N_{clust}$, we display on both the dendrogram and on the map, the clusters for which the p-value of the test is below the value we set for this test.

Function performing the test and plotting the figures:

```{r}
spatial_clusters = function(nb_clust, which_signif_clust = 1, adapted_to_shiny = F, p_val = 0.05){
  dist.geo <- as.dist(dist_geo)
  farms.hc = hclust(dist.geo, method = "ward.D2") # Compute hierachical clustering
  
  analysis = prob_intro_farms
  analysis$group = as.factor(cutree(farms.hc, k = nb_clust))
  signif_clust = c()
  numb_indiv_in_clust = c()
  
  for(clust_i in 1:nb_clust){
    analysis$in_group = (analysis$group == clust_i)
    res_test = wilcox.test(x = analysis$Prob[analysis$in_group], y = analysis$Prob[! analysis$in_group]) 
    if(res_test$p.value < p_val){
      signif_clust = c(signif_clust, clust_i)
      numb_indiv_in_clust = c(numb_indiv_in_clust, sum(analysis$in_group))
    }
  }
  
  if(length(signif_clust) == 0){
    analysis$in_group = F
    text_to_print = paste0("When data points are divided in ", nb_clust, " spatial clusters, no cluster has a mean value of introductions statistically different from the mean of all other clusters (p>", p_val, ", Wilcoxon test).")
  }else{
    analysis$in_group = (analysis$group == signif_clust[min(as.numeric(which_signif_clust), length(signif_clust))])
    text_to_print = paste0("When data points are divided in ", nb_clust, " spatial clusters, ", length(signif_clust), " clusters have a mean value of introductions statistically different from the mean of all other clusters (p<", p_val, ", Wilcoxon test). The number of data points included in each of these clusters (in green below) is respectively: ", paste(numb_indiv_in_clust, collapse = ", "), ".")
  }
  if(! adapted_to_shiny){print(text_to_print)}

  p1 = ggplot()
  p1 = p1 + ggtitle("Probability of introduction in farms:")
  p1 = p1 + xlab("Longitude") + ylab("Latitude")
  p1 = p1  + geom_polygon(data = sf_file_df, aes(x = long, y = lat, group = id), col="black", size=0.5, fill = "beige")
  p1 = p1 + geom_point(data = gps_farms, aes(x = Long, y = Lat), col = "black", size = 0.75)
  if(length(signif_clust) != 0){
    p1 = p1 + geom_circle(data = analysis[analysis$in_group,], aes(x0 = (max(Long)+min(Long))/2, y0 = (max(Lat)+min(Lat))/2, r = 0.005 + max((max(Long)-min(Long))/2, (max(Lat)-min(Lat))/2)), col = "green", size = 1)
  }
  set.seed(78)
  p1 = p1 + geom_jitter(data = analysis, aes(x = Long, y = Lat, fill = Prob, shape = in_group), col = "black", size = 3, width = 0.003, height = 0.003)
  # p1 = p1 + geom_point(data = analysis, aes(x = Long, y = Lat, fill = Prob, shape = in_group), col = "black", size = 3)
  p1 = p1 + scale_fill_gradient2(name = "Probability of introduction:", low = "dodgerblue", mid = "white", high = "red", midpoint = 0.5, limits = c(0, 1))
  p1 = p1 + scale_shape_manual(name = "In cluster:", values = c(21, 24))
  p1 = p1 + ggsn::scalebar(dist = 5, location = "topright", dist_unit = "km", transform = TRUE, model = "WGS84", x.min = min(sf_file_df$long), x.max = max(sf_file_df$long), y.min = min(sf_file_df$lat), y.max = max(sf_file_df$lat))
  p1 = p1 + blank() #+ coord_fixed()
  if(! adapted_to_shiny){plot(p1)}

  farms.hc = as.dendrogram(farms.hc)
  p2 <- dendrapply(farms.hc, function(n){
    if(is.leaf(n)){
      row_in_data = attributes(n)$label
      leaf_color = ifelse(analysis$Prob[row_in_data] > 0.95, "red", ifelse(analysis$Prob[row_in_data] < 0.05, "blue", "grey"))
      label_color = ifelse(analysis$in_group[row_in_data], "green", "black")
      attr(n, "nodePar") <- list(pch=20, lab.cex = 1, cex = 1.5, lab.col = label_color, col = leaf_color)
    }
    n
  })
  
  plot(p2, main = "Spatial clustering")
  abline(h = heights_per_k.dendrogram(farms.hc)[nb_clust], col = "darkviolet", lty = "dotted", lwd = 2)
  
  if(adapted_to_shiny){list(text = text_to_print, map = p1, dendogram = p2)}
}

```

Shiny app for visualization:

```{r}
ui <- fluidPage(titlePanel("Spatial analysis by clustering"),{
  sidebarLayout(
    sidebarPanel(
      sliderInput("nb_clust",
                  "Number of spatial clusters:",
                  min = 2,
                  max = 23,
                  value = 11),
      selectInput("which_signif_clust",
                  "Significant cluster to display:",
                  choices = list("Cluster n°1" = 1, "Cluster n°2" = 2, "Cluster n°3" = 3, "Cluster n°4" = 4, "Cluster n°5" = 5),
                  selected = 1),
      radioButtons("p_val",
                  "p-value in Wilcoxon test:",
                  choices = list("0.01" = 0.01, "0.05" = 0.05, "0.1" = 0.1),
                  selected = 0.05)
    ),
    
    mainPanel(
      textOutput("textdisp"),
      plotOutput("plotdisp1"),
      plotOutput("plotdisp2")
    )
  )
})

server <- function(input, output){

  output$textdisp <- renderText(
    spatial_clusters(nb_clust = input$nb_clust, which_signif_clust = input$which_signif_clust, p_val = input$p_val, adapted_to_shiny = T)[["text"]]
  )
  output$plotdisp1 <- renderPlot({
    spatial_clusters(nb_clust = input$nb_clust, which_signif_clust = input$which_signif_clust, p_val = input$p_val, adapted_to_shiny = T)[["dendogram"]]
  })
  output$plotdisp2 <- renderPlot({
    spatial_clusters(nb_clust = input$nb_clust, which_signif_clust = input$which_signif_clust, p_val = input$p_val, adapted_to_shiny = T)[["map"]]
  })
}

shinyApp(ui=ui, server=server)

```

