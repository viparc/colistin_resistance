---
title: "Relative contributions of AMU, initial carriage and introductions on colistin resistance in chicken farms of the Mekong delta"
csl: the-american-naturalist.csl
output:
  html_document:
    theme: cerulean
    toc: yes
  pdf_document:
    toc: yes
<!-- bibliography: references.bib -->
---

<!--
IMAGES:
Insert them with: ![alt text](image.png)
You can also resize them if needed: convert image.png -resize 50% image.png
If you want to center the image, go through HTML code:
<div style="text-align:center"><img src ="image.png"/></div>

REFERENCES:
For references: Put all the bibTeX references in the file "references.bib"
in the current folder and cite the references as @key or [@key] in the text.
Uncomment the bibliography field in the above header and put a "References"
title wherever you want to display the reference list.
-->

<style type="text/css">
.main-container {
  max-width: 1370px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r general_options, include = FALSE}
knitr::knit_hooks$set(
  margin = function(before, options, envir) {
    if (before) par(mgp = c(1.5, .5, 0), bty = "n", plt = c(.105, .97, .13, .97))
    else NULL
  },
  prompt = function(before, options, envir) {
    options(prompt = if (options$engine %in% c("sh", "bash")) "$ " else "> ")
  })

knitr::opts_chunk$set(margin = TRUE, prompt = TRUE, comment = "",
                      collapse = TRUE, cache = FALSE, autodep = TRUE,
                      dev.args = list(pointsize = 11), fig.height = 3.5,
                      fig.width = 4.24725, fig.retina = 2, fig.align = "center")

options(width = 137)
```

The objective is to understand the relative contributions of different factors to the colistin resistance detected in samples from ViParc chicken farms:

* antimicrobial use (AMU), particularly colistin ue
* the carriage of colistin resistance in day-old chicks
* introductions of resistant strains from outside the flock (other flocks, environment, human-poultry transmission)

Available data is:

* Data on ViParc farms, including AMU and sampling dates: [ViParc data overview](https://rpubs.com/choisy/viparc_data).
* During each production cycle, 3 (or less) pooled chicken faeces samples are collected: at the beginning, in the middle and at the end of the cycle. Around 30 E.coli colonies are collected from each sample and pooled. The Optical Density of each of these pooled samples is measured twice to obtain their growth curve in presence of different colistin concentrations. For each sample, the MIC can be determined from these growth curves.

We first clean work environment and load needed packages.

```{r}
rm(list=ls(all=TRUE))

library(deSolve)
library(bbmle)
library(ggplot2)
library(reshape2)
library(readxl)
library(pracma)
library(agrmt)
library(shiny)

```

# Data

## Load data

We load data:

* `viparc_data_quali` and `viparc_data_quanti`: Includes production cycles, AMU (resp. qualitative and quantitative), sampling dates in ViParc farms.
* `gcur`: Optical density (growth curves) measured in the samples at different times
* `mic_data`: MIC for each sample

```{r, cache=T}
# viparc_data_quali = read.csv("https://raw.githubusercontent.com/viparc/colistin_resistance/master/data/viparc_qualitative.csv")
# viparc_data_quanti = read.csv("https://raw.githubusercontent.com/viparc/colistin_resistance/master/data/viparc_quantitative.csv")

viparc_data_quali = read.csv("C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/viparc_qualitative.csv")
viparc_data_quanti = read.csv("C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/viparc_quantitative.csv")

mic_data = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/ColR_MIC.xlsx", sheet="MIC data"))
mic_data$MIC = as.numeric(mic_data$MIC)
mic_data = dcast(mic_data, ID + FarmID + Flockseq ~ SamplingPoint, value.var="MIC")
mic_data = mic_data[, c("ID", "FarmID", "Flockseq", "Start", "Mid", "End")]

gcur = as.data.frame(read_excel(path="C:/Users/Jonathan/Desktop/Programmes/colistin_resistance/ColR_MIC.xlsx", sheet="OD"))

id_matrix = mic_data[,c("ID", "FarmID", "Flockseq")]
```

## Resistance data: determine which Optical Density dataset to use

First, we use `gcur`. Let us plot the growth curves for each concentration. We remove concentrations 3 and 6, as too few OD were measured for these concentrations.

```{r, cache=T}
gcur = melt(gcur, id.vars = c("SampleID", "Conc.", "FlockID", "SamplPoint", "Repeat"), variable.name = "Time")
gcur$Time = as.numeric(gcur$Time)

# Unique ID for each observed curve:
gcur$obsID = paste0(gcur$SampleID, gcur$Repeat)

# Remove concentrations 3 and 6:
gcur = gcur[which(!gcur$Conc. %in% c(3, 6)),]

p = ggplot(data = gcur, aes(x=Time, group=obsID))
p = p + ggtitle("Optical Density in all samples for each concentration")
p = p + xlab("Time (hours)") + ylab("OD (optical density)")
p = p + geom_line(aes(y=value), col="grey")
p = p + facet_wrap(~Conc.)
p

rm(p)
```

Our objective is to use a concentration that offers a distribution of areas under curve (AUC) as spread out as possible. We see that, in concentrations 0 and 0.5, no curve is totally flat, i.e. all samples show some growth. On the contrary, in concentrations 8, 12 and 16, most of the samples show no growth. Therefore, we need to choose between concentrations 1, 2 and 4, that all present a spread range of curves (and thus AUC).

For these concentrations (1, 2 and 4), we compute the area under curve (AUC), for each sample, between t=15 hours and t=50 hours. This corresponds to the moment curves start to grow.

To measure how much the distributions of AUC are spread out for each concentration, we use the ["van der Eijk's A" statistic](https://en.wikipedia.org/wiki/Multimodal_distribution#van_der_Eijk's_A).  It ranges between -1 (perfect bimodality of the distribution) to +1 (perfect unimodality). A value of 0 corresponds to a uniform distribution. We select the concentration for which this statistic is closest to 0, because we aim to have both a spread distribution, and intermediary values of AUC (between high AUC - resistant samples - and low AUC - not resistant samples). We use package `agrmt`.

```{r, cache=T}
VdE_A_score = function(conc, t_start, t_end){
  
  all_auc = c()
  for(ts in unique(gcur$obsID)){
    if(length(seq(t_start,t_end)) == length(gcur$value[which((gcur$Time %in% seq(t_start,t_end)) & (gcur$Conc. == conc) & (gcur$obsID == ts))])){
      all_auc = c(all_auc, trapz(x = seq(t_start,t_end), y = gcur$value[which((gcur$Time %in% seq(t_start,t_end)) & (gcur$Conc. == conc) & (gcur$obsID == ts))]))
    }else{
      print(paste0("Problematic number of repetitions for obsID=", ts, " (concentration ", conc, ")."))
    }
  }
  return(VdE_A = agreement(table(factor(round(all_auc)))))
}

for(c in c(1, 2, 4)){
  print(paste("van der Eijk's A for concentration", c, ":"))
  print(VdE_A_score(conc=c, t_start=15, t_end=50))
}

rm(c, VdE_A_score)
```

We therefore select concentration 1. We average the 2 repetitions of a same sample, and reshape the data to obtain `auc_ob`. We standardize the AUC values to range between 0 (lowest value) and 1 (highest value).

```{r}
gcur = gcur[which(gcur$Conc. == 1),]

gcur = dcast(gcur, SampleID + Conc. + FlockID + SamplPoint + Time ~ Repeat, value.var="value")
gcur$od = (gcur$`1` + gcur$`2`)/2
gcur = gcur[,c("SampleID", "Conc.", "FlockID", "SamplPoint", "Time", "od")]

auc_ob = as.data.frame(matrix(NA, nrow = length(unique(gcur$FlockID)), ncol = length(unique(gcur$SamplPoint))))
dimnames(auc_ob) = list(unique(gcur$FlockID), c("S", "M", "E"))
for(flo in unique(gcur$FlockID)){
  for(sp in unique(gcur$SamplPoint)){
    if(length(15:50) == length(gcur$od[which((gcur$Time %in% (15:50)) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))])){
      auc_ob[flo, sp] = trapz(x = 15:50, y = gcur$od[which((gcur$Time %in% (15:50)) & (gcur$FlockID == flo) & (gcur$SamplPoint == sp))])
    }else{
      print(paste0("Impossible to compute area under curve for flock ", flo, ", sampling point ", sp))
    }
  }
}

auc_ob = (auc_ob - min(auc_ob, na.rm=T))/(max(auc_ob, na.rm=T) - min(auc_ob, na.rm=T))

# auc_ob = auc_ob[which(! rownames(auc_ob) %in% c("0351", "0381", "0391", "0765")),]
# mic_data = mic_data[which(! mic_data$ID %in% c("0351", "0381", "0391", "0765")),]

rm(gcur, flo, sp)
```

## Choose type of resistance data: growth curves or MIC

Here, we decide which resistance data we analyze: "growth_curves" or "mic":

```{r}
type_res_data = "growth_curves"
````

```{r}
if(type_res_data == "growth_curves"){
  res_data = auc_ob
}else if(type_res_data == "mic"){
  res_data = mic_data[,c("Start", "Mid", "End")]
  rownames(res_data) = mic_data$ID
  colnames(res_data) = c("S", "M", "E")
  res_data = (res_data - min(res_data, na.rm=T))/(max(res_data, na.rm=T) - min(res_data, na.rm=T))
}

print(head(res_data))
rm(type_res_data, auc_ob, mic_data)
```

## Antimicrobial use and sampling times

AMU is observed on a weekly basis. The molecules used are recorded either in a qualitative (use / no use) or quantitave (mg used /kg of chicken on farm) format. We create matrixes containing AMU for each week (columns) of the production cycle and for each cycle (rows):

* `col_expo`: qualitative (yes/no) colistin use
* `col_expo_quanti`: quantitative (mg/kg) colistin use
* `allab_expo`: qualitative (yes/no) use of any antibiotic (including colistin)
* `allab_expo_quanti`: quantitative (mg/kg) use of all antibiotics (including colistin). We sum the quantity used for each antibiotic. An other option would be to take the maximum quantity used.

We specify that we include all antibiotics in the `all_ab` category:

```{r}
all_ab = colnames(viparc_data_quali)[10:54] #[! all_ab %in% c("colistin_use", "unknown_use")]
```

We define `n_cyc`, number of cycles included in the analysis, and n_samp, number of sampling times (3 in the study):

```{r}
n_cyc = nrow(res_data)
n_samp = 3
```

We also define `weeks_samp`, matrix `n_cyc`*`n_samp`, that indicates the week of samplings for each cycle. `n_weeks` is the maximum number of production weeks among cycles included in the study. `init_flock_size` is the initial flock size for each cycle included (not used for now).

```{r}
col_expo = allab_expo = matrix(NA, n_cyc, max(viparc_data_quali$week))
col_expo_quanti = allab_expo_quanti = matrix(NA, n_cyc, max(viparc_data_quanti$week))
weeks_samp = matrix(NA, n_cyc, n_samp)
init_flock_size = rep(NA, n_cyc)
rownames(col_expo) = rownames(col_expo_quanti) = rownames(allab_expo) = rownames(allab_expo_quanti) = rownames(weeks_samp) = names(init_flock_size) = rownames(res_data)

for(i in 1:n_cyc){
  farm_i = id_matrix$FarmID[i]
  flockseq_i = id_matrix$Flockseq[i]
  
  if(!(any(viparc_data_quali$completed[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i))]) == T)){
    print(paste0("For farm ",farm_i,", flock sequence ",flockseq_i,", the production cycle was not completed"))
  }
  
  # Colistin use (qualitative and quantitative):
  
  weeks_col_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,"colistin_use"] == T))]
  weeks_col_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,"colistin_use"] == F))]
  
  col_expo[i, weeks_col_use] = 1
  col_expo[i, weeks_col_no_use] = col_expo_quanti[i, weeks_col_no_use] = 0
  
  for(wk in weeks_col_use){
    col_expo_quanti[i, wk] = 1000 * viparc_data_quanti[which((viparc_data_quanti$farm == farm_i)&(viparc_data_quanti$flock == flockseq_i)&(viparc_data_quanti$week == wk)), which(colnames(viparc_data_quali) == "colistin_use")]
  }
  
  # Other AB use (qualitative and quantitative):
  
  if(length(all_ab) > 1){
    
    weeks_othab_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(rowSums(viparc_data_quali[,all_ab], na.rm=T) != 0))]
    weeks_othab_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(rowSums(viparc_data_quali[,all_ab], na.rm=T) == 0))]
    
  }else if(length(all_ab) == 1){
    
    weeks_othab_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,all_ab] == T))]
    weeks_othab_no_use = viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali[,all_ab] == F))]

  }
  
  allab_expo[i, weeks_othab_use] = 1
  allab_expo[i, weeks_othab_no_use] = allab_expo_quanti[i, weeks_othab_no_use] = 0
  
  for(wk in weeks_othab_use){
    allab_expo_quanti[i, wk] = 1000 * sum(viparc_data_quanti[which((viparc_data_quanti$farm == farm_i)&(viparc_data_quanti$flock == flockseq_i)&(viparc_data_quanti$week == wk)), which(colnames(viparc_data_quali) %in% all_ab)])
  }
  
  # Weeks of sampling:
  
  weeks_samp[i,] = c(viparc_data_quali$week[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali$sampling == T))], rep(NA, n_samp))[1:n_samp]
  
  # Initial flock size:
  
  init_flock_size[i] = viparc_data_quali$nb_chicken[which((viparc_data_quali$farm == farm_i)&(viparc_data_quali$flock == flockseq_i)&(viparc_data_quali$week == 1))]
}

weeks_to_keep = which((colSums(!is.na(col_expo)) != 0) & (colSums(!is.na(allab_expo)) != 0))

col_expo = col_expo[,weeks_to_keep]
col_expo_quanti = col_expo_quanti[,weeks_to_keep]
allab_expo = allab_expo[,weeks_to_keep]
allab_expo_quanti = allab_expo_quanti[,weeks_to_keep]

n_weeks = max(weeks_to_keep)

if(any(col_expo != (col_expo_quanti !=0), na.rm=T) | any(allab_expo != (allab_expo_quanti !=0), na.rm=T)){print("ERROR: Qualitative and quantitative AMU data are not consistent.")}

rm(i, wk, weeks_col_use, weeks_col_no_use, weeks_othab_use, weeks_othab_no_use, farm_i, flockseq_i, weeks_to_keep, all_ab, viparc_data_quali, viparc_data_quanti)

```

# Visualization

## Production cycles, AMU and sampling dates

We plot the AMU and sampling dates for the `n_cyc` cycles. This allows to visually assess if AMU occurs shortly before sampling.

```{r}
plot(x=c(0,0.1), y=c(0,0), xlim=c(0,ncol(col_expo)), ylim=c(0,n_cyc+10), type="l", xlab="Weeks", ylab="Cycles")
for (i in 1:n_cyc){
  for (j in 1:ncol(col_expo)){
    
    if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 1) & (allab_expo[i,j] == 1)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "green", border="green")
    }else if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 0) & (allab_expo[i,j] == 1)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "blue", border="blue")
    }else if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 1) & (allab_expo[i,j] == 0)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "red", border="red")
    }else if ((!(is.na(col_expo[i,j]))) & (col_expo[i,j] == 0) & (allab_expo[i,j] == 0)){
      rect(xleft = j-0.5, xright = j+0.5, ybottom = i-0.5, ytop = i+0.5, col = "bisque", border="bisque")
    }
    
    if (any(weeks_samp[i,] == j, na.rm=T)){
      rect(xleft = j-0.1, xright = j+0.1, ybottom = i-0.5, ytop = i+0.5, col = "black", border="black")
    }
  }
  lines(x = c(0, ncol(col_expo)), y = c(i-0.5, i-0.5))
}
rm(i,j)
legend(x=1, y=n_cyc+10, legend=c("Colistin use only", "Any antibiotic excepted colistin", "Colistin and any other antibiotic", "No AMU"), fill=c("red", "blue", "green", "bisque"), cex = 0.8, ncol=3)

```

## Resistance and AMU over time

We plot observations (red dots) and model predictions (black line and CI) for `cyc_plot` cycles.

AMU is represented as blue bars. We can plot the qualitative use (yes/no each week), or the quantitative use (amount per kg of chicken each week).

```{r}
cyc_plot = 1:21
amu_quanti = F

```

```{r warning=F}
obs_plot = rbind(data.frame(cyc=rep(NA, n_cyc*n_weeks), week=rep(NA, n_cyc*n_weeks), obser=rep(NA, n_cyc*n_weeks), exp_ab=rep(NA, n_cyc*n_weeks), type_ab=rep("Colistin", n_cyc*n_weeks)),
                 data.frame(cyc=rep(NA, n_cyc*n_weeks), week=rep(NA, n_cyc*n_weeks), obser=rep(NA, n_cyc*n_weeks), exp_ab=rep(NA, n_cyc*n_weeks), type_ab=rep("All antibiotics", n_cyc*n_weeks)))

obs_plot$cyc = as.vector(matrix(rep(rownames(res_data),n_weeks), n_weeks, n_cyc, byrow=T))
obs_plot$week = rep(seq(1,n_weeks),n_cyc)

for (f in rownames(res_data)){
  n_weeks_cyc = sum(!(is.na(col_expo[f,])))
  samp_cyc = which(!(is.na(res_data[f,])))
  if(length(samp_cyc) < 2){print(paste0("CAUTION: For cycle ", f,", the number of samples collected is strictly less than 2"))}

  obs_plot$obser[which((obs_plot$cyc==f) & (obs_plot$week %in% weeks_samp[f,samp_cyc]))] = as.numeric(res_data[f,samp_cyc])
  
  if(amu_quanti){
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "Colistin"))] = col_expo_quanti[f,]
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "All antibiotics"))] = allab_expo_quanti[f,]

  }else{
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "Colistin"))] = col_expo[f,]
    obs_plot$exp_ab[which((obs_plot$cyc == f) & (obs_plot$type_ab == "All antibiotics"))] = allab_expo[f,]
  }
}

# obs_plot$exp_ab[obs_plot$exp_ab == 0] = NA

ratio_mic_quanti_amu = 50

if(amu_quanti){
  obs_plot$exp_ab[which(obs_plot$exp_ab > (max(res_data, na.rm=T) * ratio_mic_quanti_amu/2))] = max(res_data, na.rm=T) * ratio_mic_quanti_amu/2
}

obs_plot$cyc = factor(obs_plot$cyc, levels=rownames(res_data))

rm(f, n_weeks_cyc, samp_cyc)

# Plot:

p = ggplot(data=obs_plot[which(obs_plot$cyc %in% rownames(res_data)[cyc_plot]),], aes(x=week))
p = p + xlab("Time (weeks)")
p = p + ylab("Resistance metric")
p = p + ggtitle("AMU and colistin resistance")

if(amu_quanti){
  p = p + geom_bar(aes(y=exp_ab/ratio_mic_quanti_amu, fill=type_ab), stat="identity", position="dodge2", col=NA)
  
  p = p + scale_y_continuous(sec.axis = sec_axis(~.*ratio_mic_quanti_amu, name = paste0("AMU (mg/kg) (ceiling ", round(max(res_data, na.rm=T) * ratio_mic_quanti_amu/2), " mg/kg for each type)")))#, limits=c(0, 0.1+max(res_data, na.rm=T)))
  p = p + theme(axis.text.y.right = element_text(color="blue"), axis.title.y.right = element_text(color="blue"))

}else{
  p = p + geom_bar(aes(y=exp_ab/2, fill=type_ab), stat="identity", position="stack", col=NA)
}
p = p +  scale_fill_discrete(name = "Antibiotics used:")

p = p + geom_point(aes(y = obser), size = 3, shape = 21,  fill = "red", color = "black")

p = p + facet_wrap(~ cyc)
p

rm(far, iter_list, p, pl)
```

## Resistance plotted versus different AMU metrics in Shiny

```{r}
# nwkseff=2; quantuse=F; tempo_amu="exp_decay"; thresh=0.4; logtrans=F; ignore.nul=F; decay_init=0.8; teta=1; delta=6; obj="plot"

aucamu = function(nwkseff=2, quantuse=T, tempo_amu="exp_decay", thresh=0.4, logtrans=F, ignore.nul=F, decay_init=0, teta=1, delta=3, obj="plot"){
  obtab = data.frame(cycle = NA, amu = NA, auc = NA, age=NA, init=NA, prev=NA, exp.init=NA)
  for(cyc_i in 1:n_cyc){
    for(time_i in 2:3){
      
      if(weeks_samp[cyc_i, time_i] - nwkseff < 1){
        # print(paste0("For cycle ", cyc_i, ", sampling time ", time_i, ", the number of weeks entered for the historic of use (", nwkseff, ") is more important than the number of past production weeks. The total past use is calculated."))
      }
      
      inc_wks = max(1, (weeks_samp[cyc_i, time_i] - nwkseff)):(weeks_samp[cyc_i, time_i] - 1)
      
      if(quantuse){
        if(logtrans){
          datamu = log(allab_expo_quanti[cyc_i, inc_wks])
          datamu[! is.finite(datamu)] = NA
        }else{
          datamu = allab_expo_quanti[cyc_i, inc_wks]
        }
      }else{
        datamu = allab_expo[cyc_i, inc_wks]
      }
      datamu = datamu * teta^(inc_wks <= delta)
      
      if(tempo_amu == "step"){
        usemetr = sum(datamu, na.rm=T)
      }else if(tempo_amu == "linear"){
        time_eff = seq(0, 1, length.out = nwkseff+1)
        usemetr = sum(datamu*tail(time_eff, length(datamu)), na.rm=T)
      }else if(tempo_amu == "exp_decay"){
        time_eff = (exp(-log(100)/nwkseff))^seq(nwkseff, 0, -1)
        usemetr = sum(datamu*tail(time_eff, length(datamu)), na.rm=T)
      }
      
      exp_init = res_data[cyc_i, 1] * (1 - decay_init)^(weeks_samp[cyc_i, time_i] -1)
      
      obtab = rbind(obtab, c(cyc_i, usemetr, res_data[cyc_i, time_i], weeks_samp[cyc_i, time_i], res_data[cyc_i, 1], res_data[cyc_i, time_i-1], exp_init))
    }
  }
  obtab = obtab[-1,]
  
  if(ignore.nul){
    obtab = obtab[which(obtab$amu != 0),]
  }
  obtab$cycle = as.factor(obtab$cycle)
  
  obtab$res = obtab$auc >= thresh
  
  if(obj == "plot"){
    p = ggplot(data=obtab, aes(x=amu, y=auc, col=exp.init))
    p = p + xlab("All antibiotics use metric") + ylab("Area under curve")
    p = p + ggtitle(paste0("Pearson correlation: ", round(cor(obtab$amu, obtab$auc, use="complete.obs"), 3), " (", nrow(na.omit(obtab)), " complete observations, including ", round(sum(obtab$auc >= thresh, na.rm=T)*100/nrow(na.omit(obtab))), "% above threshold)"))
    p = p + geom_point()
    p = p + geom_hline(yintercept = thresh, linetype="dashed", col="grey")
    p = p + scale_color_gradient(low = "red", high = "blue")
    # p = ggplotly(p)
    return(p)
  }else if(obj == "est"){
    return(obtab)
  }
}

```

Shiny interface:

```{r}
ui <- fluidPage({
  titlePanel("Correlation between past use and AUC")
  sidebarLayout(
    sidebarPanel(
      sliderInput("nwkseff",
                  "Number of weeks of past use to take into account:",
                  min=1,
                  max=25,
                  value=5),
      sliderInput("thresh",
                  "Threshold value for AUC:",
                  min=0,
                  max=1,
                  value=0.5),
      selectInput("tempo_amu",
                  "Temporal effect of AMU:",
                  c("exp_decay", "linear", "step")),
      selectInput("quantuse",
                  "Select if the quantitative AMU should be used:",
                  c(T, F)),
      selectInput("logtrans",
                  "Select if the quantitative AMU should be log transformed:",
                  c(F, T)),
      selectInput("ignore.nul",
                  "Select if the nul values of use should be ignored:",
                  c(T, F)),
      sliderInput("decay_init",
                  "Value for decay from initial value:",
                  min=0,
                  max=1,
                  value=0.5)

    ),
    
    mainPanel(
      plotOutput("plotdisp")
    )
  )
})

server <- function(input, output){
  # define the output, the output id must be the same with the ID in the UI object
  # because the output change as the input change - it's dynamic so we need a special function renderPlot() to make the output react with our choice
  
  output$plotdisp <- renderPlot({
    # filter the data by the input and save into new object
    # the input id must be the same as the UI object
    # plot
    aucamu(nwkseff=input$nwkseff, quantuse=input$quantuse, tempo_amu=input$tempo_amu, thresh=input$thresh, logtrans=input$logtrans, ignore.nul=input$ignore.nul, decay_init=input$decay_init)
  })
}
```

To launch Shiny app:

```{r}
# shinyApp(ui=ui, server=server)

```

# Analysis

## Dynamic model

A dynamic model is defined:

```{r}
eq_mod=function(t, R, param){
  gamma = param[[1]]
  nu = param[[2]]
  alpha = param[[3]]
  A = param[[4]]
  B = param[[5]]
  ab_expo_farm = param[[6]]

  Req = alpha * dnorm(ab_expo_farm[t], mean=A, sd=B)
  # Req = ifelse((ab_expo_farm[t] >= A) & (ab_expo_farm[t] <= B), alpha, 0)
  dR = (gamma * nu^(Req < R)) * (Req - R)

  return(list(dR))
}

```

We first define a function `run1` with model's parameters as inputs, and as ouput the qquared residuals sum, to minimize:

```{r}
run1 = function(param){
  gamma = param[1]
  nu = param[2]
  alpha = param[3]
  A = param[4]
  B = param[5]

  pred_sampl_dates = matrix(NA, n_cyc, n_samp)

  for (farm in 1:n_cyc){
    n_weeks_farm = sum(!(is.na(ab_expo[farm,])))
    samp_farm = which(!(is.na(weeks_samp[farm,])))
    if(length(samp_farm) < 2){print(paste0("CAUTION: For farm ", farm,", the number of samples collected is strictly less than 2"))}

    pred_farm = ode(obs[farm,1], 1:n_weeks_farm, eq_mod, list(gamma, nu, alpha, A, B, ab_expo_quanti[farm,]))[,2]

    pred_sampl_dates[farm, samp_farm] = pred_farm[weeks_samp[farm,samp_farm]]
  }
  rm(farm, pred_farm, n_weeks_farm, samp_farm)

  to_min = sum((pred_sampl_dates-obs)^2, na.rm=T)

  return(to_min)
}
```

We use `optim` function for point estimates:

```{r}
# ab_expo = allab_expo
# ab_expo_quanti = allab_expo_quanti
# obs = res_data

# fit = optim(par=c(1,1,1,1,1), fn=run1)
# print(fit$par)
# fit_coef = c(fit$par, 10^-5)
```

## Expectation-Maximization algorithm

The introduction of colistin resistant bacteria from outside the flock is not measured but can potentially play an important role in the resistance of the sample. We consider a latent (unobserved) binary variable to describe if an introduction happened in the farm (yes/no).

We use an Expectation-Maximization (E-M) algorithm with 2 steps:

* Expectation: given a set of parameters, determine the most likely status for introduction (yes/no) for each observation
* Maximization: given an assumed introduction status, determine the set of parameters maximizing the likelihood of the model (and select a model ?)

The algorithm is initiated with a random set of parameters, and is terminated when a convergence of likelihood is reached.

### Function useful for the algorithm

```{r}
introd = c(rep(0, 4), 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, rep(0, 10), 1, 0, 0, 1, rep(0, 8), 1, rep(0,3))

run2 = function(aim="est", intr=introd, mu, gamma, alpha, teta, delta, beta, eta){ #, lambda){

  mu = sigmoid(mu)
  gamma = gamma
  alpha = alpha
  teta = exp(teta) +1
  delta = 10 * sigmoid(delta) +1
  beta = 10 * sigmoid(beta) +1
  eta = sigmoid(eta)
    # lambda = sigmoid(lambda)

  # if (aim == "pred"){
  #   print(c(mu, gamma, alpha, teta, delta, beta, eta)) #, lambda))
  # }

  obsdat = aucamu(nwkseff=beta, quantuse=T, tempo_amu="exp_decay", thresh=0.4, logtrans=F, ignore.nul=F, decay_init=0, teta=teta, delta=delta, obj="est")
  obsdat$introd = intr

  predmod = mu + alpha * obsdat$amu + eta * obsdat$introd + gamma * obsdat$init #+ lambda^(obsdat$age <= delta)

  if(aim == "est"){
    return(-sum(dnorm(x=obsdat$auc, mean=predmod, sd=Sd, log=T), na.rm=T))
    # return(-sum(dbeta(x=obsdat$auc, shape1=shape, shape2=shape*(1.00001-predmod)/(predmod+0.00001), log=T), na.rm=T))
  }else if (aim == "pred"){
    return(list(obs = obsdat$auc, pred = predmod, minll = -sum(dnorm(x=obsdat$auc, mean=predmod, sd=Sd, log=T), na.rm=T)))
  }
}
```

### Example of Maximization step for one given set of parameters

We use "bbmle" package for point estimates, confidence intervals and likelihood profiles:

```{r}
Sd = 0.15
fit_mle2 = mle2(minuslogl = run2, method = "Nelder-Mead", start = list(mu = logit(runif(1)), gamma = runif(1), alpha = runif(1), teta = log(runif(1)), delta = logit(runif(1)), beta = logit(runif(1)), eta = logit(runif(1)))) #, lambda = logit(runif(1)))

# fit_coef = coef(fit_mle2)
# print(fit_coef)
# prof = profile(fit_mle2)
# plot(prof)
# conf = confint(fit_mle2)
# print(conf)

obs_vs_pred = run2(aim="pred", mu=coef(fit_mle2)[1], gamma=coef(fit_mle2)[2], alpha=coef(fit_mle2)[3], teta=coef(fit_mle2)[4], delta=coef(fit_mle2)[5], beta=coef(fit_mle2)[6], eta=coef(fit_mle2)[7]) #, lambda=coef(fit_mle2)[8])

minll = obs_vs_pred[[3]]
obs_vs_pred = data.frame(obs = obs_vs_pred[[1]], pred = obs_vs_pred[[2]])
p = ggplot(obs_vs_pred, aes(x = obs, y = pred))
p = p + xlab("Obs") + ylab("Pred")
p = p + ggtitle(paste0("Observed VS Predicted AUC (minus LogLikelihood =", round(minll, 2), ")"))
p = p + geom_segment(aes(xend = obs, y = pmax(0, obs_vs_pred[[2]] - 1.96*Sd), yend = pmin(1, obs_vs_pred[[2]] + 1.96*Sd)), col = "grey")
p = p + geom_abline(intercept = 0, slope = 1, linetype = "dashed")
p = p + geom_point(aes(col = as.logical(c(rep(0, 4), 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, rep(0, 10), 1, 0, 0, 1, rep(0, 8), 1, rep(0,3))))) + labs(col = "Introduction")
p

```

### Implementation of the full E-M algorithm

Function for the E step:

```{r}
e.step <- function(dat=dat, params, type_mod, sigma = 0.15) {

  if(type_mod == "logistic"){

    lin_intro = params[["mu"]] + params[["alpha"]] * dat$amu + params[["gamma"]] * dat$init + params[["eta"]]
    lin_nointro = params[["mu"]] + params[["alpha"]] * dat$amu + params[["gamma"]] * dat$init

    lin_intro = pmin(200, lin_intro)
    lin_nointro = pmin(200, lin_nointro)

    p_res_if_nointro = 10^lin_nointro/(10^lin_nointro + 1)
    p_res_if_nointro = pmin(0.9999, p_res_if_nointro)
    p_res_if_nointro = pmax(0.0001, p_res_if_nointro)

    p_res_if_intro = 10^lin_intro/(10^lin_intro + 1)
    p_res_if_intro = pmin(0.9999, p_res_if_intro)
    p_res_if_intro = pmax(0.0001, p_res_if_intro)

    p_nores_if_nointro = 1-p_res_if_nointro
    p_nores_if_intro = 1-p_res_if_intro

    p_intro = params[["p_intro"]]
    p_intro_if_res = p_res_if_intro * p_intro / (p_res_if_intro * p_intro + p_res_if_nointro * (1-p_intro))
    p_intro_if_nores = p_nores_if_intro * p_intro / (p_nores_if_intro * p_intro + p_nores_if_nointro * (1-p_intro))

    prob_intro = dat$res * p_intro_if_res + (1-dat$res) * p_intro_if_nores
    introd = rbinom(n = length(prob_intro), size = 1, prob = prob_intro)
    introd = as.logical(introd)

  }else if(type_mod == "linear"){

    lin_intro = params[["mu"]] + params[["alpha"]] * dat$amu + params[["gamma"]] * dat$init + params[["eta"]]
    lin_nointro = params[["mu"]] + params[["alpha"]] * dat$amu + params[["gamma"]] * dat$init

    r_intro = abs(lin_intro - dat$auc) # residuals
    r_nointro = abs(lin_nointro - dat$auc)

    exp_intro = exp(- r_intro^2/sigma^2)
    exp_nointro = exp(- r_nointro^2/sigma^2)

    exp_intro = pmin(0.9999, exp_intro)
    exp_intro = pmax(0.0001, exp_intro)
    exp_nointro = pmin(0.9999, exp_nointro)
    exp_nointro = pmax(0.0001, exp_nointro)

    prob_intro <- exp_intro / (exp_intro + exp_nointro)

    introd = rbinom(n = length(prob_intro), size = 1, prob = prob_intro)
    introd = as.logical(introd)

  }else if(type_mod == "linear2"){

    lin_intro = run2(aim="pred", intr=1, mu=params[["mu"]], gamma=params[["gamma"]], alpha=params[["alpha"]], teta=params[["teta"]], delta=params[["delta"]], beta=params[["beta"]], eta=params[["eta"]])[["pred"]]
    lin_nointro = run2(aim="pred", intr=0, mu=params[["mu"]], gamma=params[["gamma"]], alpha=params[["alpha"]], teta=params[["teta"]], delta=params[["delta"]], beta=params[["beta"]], eta=logit(0))[["pred"]]

    r_intro = abs(lin_intro - obtab$auc) # residuals
    r_nointro = abs(lin_nointro - obtab$auc)

    exp_intro = exp(- r_intro^2/sigma^2)
    exp_nointro = exp(- r_nointro^2/sigma^2)

    exp_intro = pmin(0.9999, exp_intro)
    exp_intro = pmax(0.0001, exp_intro)
    exp_nointro = pmin(0.9999, exp_nointro)
    exp_nointro = pmax(0.0001, exp_nointro)

    prob_intro <- exp_intro / (exp_intro + exp_nointro)

    introd = rbinom(n = length(prob_intro), size = 1, prob = prob_intro)
    introd = as.logical(introd)
  }

  return(introd)
}
```

Function for the M step:

```{r}
m.step <- function(dat=dat, introd, type_mod) {

  prop_intro = sum(introd)/length(introd)

  if(type_mod == "logistic"){

    dat_intr = dat
    dat_intr$intro = introd

    modstep = glm(data = dat_intr, res ~ amu + init + intro)
    llmod = as.numeric(logLik(modstep))

    est_param = as.list(c(summary(modstep)$coefficients[,"Estimate"]))
    if(length(est_param) < 4){est_param[[4]] = 0}

    est_param = c(est_param, prop_intro)
    names(est_param) = c("mu", "alpha", "gamma", "eta", "p_intro")
    true_coef = est_param

  }else if(type_mod == "linear"){

    dat_intr = dat
    dat_intr$intro = introd

    modstep = lm(data = dat_intr, auc ~ amu + init + intro)
    llmod = as.numeric(logLik(modstep))

    est_param = as.list(c(summary(modstep)$coefficients[,"Estimate"]))
    if(length(est_param) < 4){est_param[[4]] = 0}

    est_param = c(est_param, prop_intro)
    names(est_param) = c("mu", "alpha", "gamma", "eta", "p_intro")
    true_coef = est_param

  }else if(type_mod == "linear2"){

    Sd = 0.15
    run2 = function(aim="est", intr=introd, mu, gamma, alpha, teta, delta, beta, eta){ #, lambda){

      mu = sigmoid(mu)
      gamma = gamma
      alpha = alpha
      teta = exp(teta) +1
      delta = 10 * sigmoid(delta) +1
      beta = 10 * sigmoid(beta) +1
      eta = sigmoid(eta)
      # lambda = sigmoid(lambda)

      obsdat = aucamu(nwkseff=beta, quantuse=T, tempo_amu="exp_decay", thresh=0.4, logtrans=F, ignore.nul=F, decay_init=0, teta=teta, delta=delta, obj="est")
      obsdat$introd = intr

      predmod = mu + alpha * obsdat$amu + eta * obsdat$introd + gamma * obsdat$init #+ lambda^(obsdat$age <= delta)

      if(aim == "est"){
        return(-sum(dnorm(x=obsdat$auc, mean=predmod, sd=Sd, log=T), na.rm=T))
        # return(-sum(dbeta(x=obsdat$auc, shape1=shape, shape2=shape*(1.00001-predmod)/(predmod+0.00001), log=T), na.rm=T))
      }else if (aim == "pred"){
        return(list(obs = obsdat$auc, pred = predmod, minll = -sum(dnorm(x=obsdat$auc, mean=predmod, sd=Sd, log=T), na.rm=T)))
      }
    }

    fit_mle2 = list()
    llmod = list()
    for(rep in 1:3){
      print(paste0("M-step repeat ", rep))
      fit_mle2[[rep]] = mle2(minuslogl = run2, method = "Nelder-Mead", start = list(mu = logit(runif(1)), gamma = runif(1), alpha = runif(1), teta = log(runif(1)), delta = logit(runif(1)), beta = logit(runif(1)), eta = logit(runif(1))))
      true_coef = coef(fit_mle2[[rep]])
      llmod[[rep]] = run2(aim="pred", mu=true_coef[1], gamma=true_coef[2], alpha=true_coef[3], teta=true_coef[4], delta=true_coef[5], beta=true_coef[6], eta=true_coef[7])[[3]]
    }
    fit_mle2 = fit_mle2[[which.min(llmod)]]
    llmod = llmod[[which.min(llmod)]]

    # fit_mle2 = mle2(minuslogl = run2, method = "Nelder-Mead", start = list(mu = logit(runif(1)), gamma = runif(1), alpha = runif(1), teta = log(runif(1)), delta = logit(runif(1)), beta = logit(runif(1)), eta = logit(runif(1))))
    # true_coef = coef(fit_mle2)
    # llmod = run2(aim="pred", mu=true_coef[1], gamma=true_coef[2], alpha=true_coef[3], teta=true_coef[4], delta=true_coef[5], beta=true_coef[6], eta=true_coef[7])[[3]]

    modstep = fit_mle2

    true_coef = coef(fit_mle2)
    true_coef = c(true_coef, p_intro = prop_intro)
    est_param = true_coef

    true_coef[1] = sigmoid(true_coef[1])
    true_coef[4] = exp(true_coef[4]) +1
    true_coef[5] = 10 * sigmoid(true_coef[5]) +1
    true_coef[6] = 10 * sigmoid(true_coef[6]) +1
    true_coef[7] = sigmoid(true_coef[7])

  }

  return(list(est_param, llmod, modstep, true_coef))
}
```

Function to launch the whole algorithm:

```{r}
em.2lines <- function(dat=0, tol=1e-6, max.step=1e3, type_mod, init_params) {
  step = 0
  loglik = 10^6

  params = init_params

  repeat {
    print(paste0("Step: ", step))
    introd = e.step(dat, params, type_mod=type_mod)
    fit_mod = m.step(dat, introd, type_mod=type_mod)
    params = fit_mod[[1]]
    old.loglik = loglik
    loglik = fit_mod[[2]]
    print(paste0("MinusLL: ", loglik))
    modstep = fit_mod[[3]]
    param_true_format = fit_mod[[4]]

    if (abs(loglik - old.loglik) < tol){
      counting_for_convergence = counting_for_convergence +1
    }else{
      counting_for_convergence = 0
    }

    if((counting_for_convergence == 1) | (loglik < -30)){break}

    step = step +1
    if (step > max.step){
      break
    }
  }
  print(paste0("Final step: ", step))

return(list(modstep, params, introd, loglik, param_true_format))
}
```

We launch the algorithm:

```{r}
rand = round(100*runif(n=1))
set.seed(rand)
results = em.2lines(type_mod = "linear2", init_params = list(mu = logit(runif(1)), gamma = runif(1), alpha = runif(1), teta = log(runif(1)), delta = logit(runif(1)), beta = logit(runif(1)), eta = logit(runif(1)), p_intro = runif(1)))
summary(results[[1]])
results[[5]]
print(paste0("Probability of introduction: ", results[[2]][["p_intro"]]))
print(paste0("Loglikelihood:", results[[4]]))
```

We plot the results:

```{r}
obs_vs_pred = run2(aim="pred", intr=results[[3]], mu=results[[2]][1], gamma=results[[2]][2], alpha=results[[2]][3], teta=results[[2]][4], delta=results[[2]][5], beta=results[[2]][6], eta=results[[2]][7]) #, lambda=coef(fit_mle2)[8])
obs_vs_pred = data.frame(obs = obs_vs_pred[[1]], pred = obs_vs_pred[[2]])

p = ggplot(obs_vs_pred, aes(x = obs, y = pred))
p = p + xlab("Obs") + ylab("Pred")
p = p + ggtitle(paste0("Observed VS Predicted AUC (minus LogLikelihood =", round(results[[4]], 2), ")"))
p = p + geom_segment(aes(xend = obs, y = pmax(0, obs_vs_pred[[2]] - 1.96*Sd), yend = pmin(1, obs_vs_pred[[2]] + 1.96*Sd)), col = "grey")
p = p + geom_abline(intercept = 0, slope = 1, linetype = "dashed")
p = p + geom_point(aes(col = as.logical(c(rep(0, 4), 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, rep(0, 10), 1, 0, 0, 1, rep(0, 8), 1, rep(0,3))))) + labs(col = "Introduction")
p

dat_intr = aucamu(obj = "est", nwkseff=results[[5]][6], teta=results[[5]][4], delta=results[[5]][5])
dat_intr$intro = results[[3]]

ggplot(dat_intr, aes(x=amu, y=auc, col=intro, shape=res)) + geom_point()

```





